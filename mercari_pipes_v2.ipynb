{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "e6826013-0719-4f1e-ab4e-3bb475ca2f73",
    "_uuid": "69973338b67e6a1d8ac846f009608ac848de6941"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64f489ba4504f0db5acad4612693269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/mohsin/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import hashlib\n",
    "import string\n",
    "import unicodedata\n",
    "import re\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm.pandas(tqdm_notebook())\n",
    "\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder, QuantileTransformer\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "#from nltk.corpus import stopwords\n",
    "#from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from itertools import chain\n",
    "\n",
    "# from __future__ import print_function\n",
    "np.random.seed(786)  # for reproducibility\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D, ZeroPadding1D, AveragePooling1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier,  KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "9bc40f4e-17d3-438f-96dc-9df02c9b203a",
    "_uuid": "3d180491dde0e6310b4c6c3b825abee24ea3d2a9"
   },
   "outputs": [],
   "source": [
    "#Functions we need - Feature Selector, Fasttext_Estimator, Preprocessing Transformer, Binary_Encoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "from pandas.api.types import is_numeric_dtype, is_string_dtype\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "rmse_sklearn = make_scorer(rmse, greater_is_better=False)    \n",
    "    \n",
    "# the following functions allow for a parallelized batch generator\n",
    "class threadsafe_iter(object):\n",
    "    \"\"\"\n",
    "    Takes an iterator/generator and makes it thread-safe by\n",
    "    serializing call to the `next` method of given iterator/generator.\n",
    "    \"\"\"\n",
    "    def __init__(self, it):\n",
    "        self.it = it\n",
    "        self.lock = threading.Lock()\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        with self.lock:\n",
    "            return next(self.it)\n",
    "\n",
    "def threadsafe_generator(f):\n",
    "    \"\"\"\n",
    "    A decorator that takes a generator function and makes it thread-safe.\n",
    "    \"\"\"\n",
    "    def g(*a, **kw):\n",
    "        return threadsafe_iter(f(*a, **kw))\n",
    "    return g\n",
    "\n",
    "@threadsafe_generator\n",
    "def batch_generator(X_data, y_data, batch_size):\n",
    "    \n",
    "    #index = np.random.permutation(X_data.shape[0])    \n",
    "    #X_data = X_data[index]\n",
    "    #y_data = y_data[index]\n",
    "    \n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    index = np.arange(np.shape(y_data)[0])\n",
    "    #idx = 1\n",
    "    while 1:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:].todense()\n",
    "        y_batch = y_data[index_batch]\n",
    "        counter += 1\n",
    "        yield np.array(X_batch),y_batch\n",
    "        #print(\"\")\n",
    "        #print(X_batch.shape)\n",
    "        #print(\"\")\n",
    "        #print('generator yielded a batch %d' % idx)\n",
    "        #idx += 1\n",
    "        if (counter > number_of_batches):\n",
    "            counter=0\n",
    "            \n",
    "            \n",
    "@threadsafe_generator\n",
    "def batch_generator_x(X_data,batch_size):\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    index = np.arange(np.shape(X_data)[0])\n",
    "    while 1:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:].todense()\n",
    "        counter += 1\n",
    "        yield np.array(X_batch)\n",
    "        if (counter > number_of_batches):\n",
    "            counter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "36c127ae-46e3-47ba-94e0-0eaa6aa888b4",
    "_uuid": "54695716f99aaaf38b03dd8f11987f21e93ce490"
   },
   "outputs": [],
   "source": [
    "num_partitions = 30\n",
    "num_cores = 16\n",
    "from multiprocessing import Pool, cpu_count\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "stop_words = ['ourselves', 'hers', 'between', 'yourself', 'but', 'again', \n",
    "              'there', 'about', 'once', 'during', 'out', 'very', 'having', \n",
    "              'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', \n",
    "              'its', 'yours', 'such', 'into', 'most', 'itself', 'other', \n",
    "              'off', 'is', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', \n",
    "              'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', \n",
    "              'through', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', \n",
    "              'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', \n",
    "              'ours', 'had', 'she', 'all', 'when', 'at', 'any', 'before', 'them',\n",
    "              'same', 'and', 'been', 'have', 'in', 'will', 'does', 'yourselves', \n",
    "              'then', 'that', 'because', 'what', 'over', 'why’, ‘so', 'can', 'did',\n",
    "              'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only',\n",
    "              'myself', 'which', 'those', 'i','after', 'few', 'whom', 'being', 'if', \n",
    "              'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return  unicodedata.normalize('NFKC', s)\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"'\", r\"\", s)\n",
    "    s = re.sub(r\"[.!?':;,]\", r\" \", s)\n",
    "    s = re.sub(r\"-\", r\"\", s)\n",
    "    s = re.sub(r\"[^0-9a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"0\", r\"zero\", s)\n",
    "    s = re.sub(r\"1\", r\"one\", s)\n",
    "    s = re.sub(r\"2\", r\"two\", s)\n",
    "    s = re.sub(r\"3\", r\"three\", s)\n",
    "    s = re.sub(r\"4\", r\"four\", s)\n",
    "    s = re.sub(r\"5\", r\"five\", s)\n",
    "    s = re.sub(r\"6\", r\"six\", s)\n",
    "    s = re.sub(r\"7\", r\"seven\", s)\n",
    "    s = re.sub(r\"8\", r\"eight\", s)\n",
    "    #s = re.sub(r\"/s/s\", r\"/s\", s)\n",
    "    return s\n",
    "\n",
    "def _normalize_and_ngrams(sent, ngrams):\n",
    "    input_list = normalizeString(sent).split()\n",
    "    input_list = [word for word in input_list if word not in stop_words]\n",
    "    s = input_list.copy()\n",
    "    for i in range(2, ngrams+1):\n",
    "        s += [' '.join(input_list[j:j+i]) for j in range(len(input_list)-i + 1)]\n",
    "        #s += list((zip(*[input_list[j:] for j in range(i)])))\n",
    "    return s\n",
    "\n",
    "#tmp = \"I am not a dance'r and i am a 6ixy   c-o:d;er programmer\"\n",
    "#print(normalizeString(tmp))\n",
    "#print(_normalize_and_ngrams(tmp, 3))\n",
    "\n",
    "class Vocab_topwords():\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.index2word = {}\n",
    "        \n",
    "    def fit_data(self, data, col, ngrams=3, max_features=50000):\n",
    "        c = Counter(list(chain.from_iterable(data[col].tolist())))\n",
    "        for i, (w, count) in enumerate(c.most_common(max_features)):\n",
    "            self.word2index[w] = i\n",
    "        return\n",
    "    \n",
    "\n",
    "            \n",
    "            \n",
    "def prepareVocab(name, data, max_features):\n",
    "    vocab = Vocab_topwords(name)\n",
    "    vocab.fit_data(data, name, max_features=max_features)\n",
    "    \n",
    "    print(\"Counted words:\")\n",
    "    print(vocab.name, len(vocab.word2index))\n",
    "    return vocab\n",
    "\n",
    "def indexesFromSentence(vocab, tokens, ngrams, max_len):\n",
    "    num_list = []\n",
    "    for i, item in enumerate(tokens):\n",
    "        if len(num_list) == max_len:\n",
    "            break\n",
    "        elif item in vocab.word2index:\n",
    "            num_list.append(vocab.word2index[item])\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "    if len(num_list) < max_len :\n",
    "        num_list += [0]*(max_len - len(num_list) )\n",
    "        \n",
    "    return num_list\n",
    "\n",
    "def parallelize_dataframe(df, func):\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def get_cat_1(x): return str(x).split('/')[0]\n",
    "def get_cat_2(x): return str(x).split('/')[1] if len(str(x).split('/')) > 1 else -1\n",
    "def get_cat_3(x): return ' '.join(str(x).split('/')[2:]) if len(str(x).split('/')) > 2 else -1\n",
    "\n",
    "def applycat1(df): \n",
    "    df['cat1'] = df['category_name'].progress_apply(get_cat_1)\n",
    "    return df\n",
    "\n",
    "def applycat2(df): \n",
    "    df['cat2'] = df['category_name'].progress_apply(get_cat_2)\n",
    "    return df\n",
    "\n",
    "def applycat3(df): \n",
    "    df['cat3'] = df['category_name'].progress_apply(get_cat_3)\n",
    "    return df\n",
    "\n",
    "def norm3grams(s): return _normalize_and_ngrams(s, 3)\n",
    "\n",
    "def applyname(series): return series.progress_apply(norm3grams)\n",
    "\n",
    "def index2sent1(x, name_vocab): return indexesFromSentence(name_vocab, x, 3, 10)\n",
    "\n",
    "def name2index(series): return series.progress_apply(lambda x: index2sent1(x, name_vocab))\n",
    "\n",
    "def norm2grams(s): return _normalize_and_ngrams(s, 2)\n",
    "\n",
    "def applydesc(series):return series.progress_apply(norm2grams)\n",
    "\n",
    "def index2sent2(x, desc_vocab): return indexesFromSentence(desc_vocab, x, 2, 80)\n",
    "\n",
    "def desc2index(series): return series.progress_apply(lambda x: index2sent2(x, desc_vocab))\n",
    "\n",
    "def read_data(in_path, out_path):\n",
    "    if False and os.path.exists(os.path.join(out_path, 'train_2.pkl')) and os.path.exists(os.path.join(out_path, 'test_2.pkl')):\n",
    "        train_data = pd.read_pickle(os.path.join(out_path, 'train_2.pkl'))\n",
    "        test_data  = pd.read_pickle(os.path.join(out_path, 'test_2.pkl'))\n",
    "        \n",
    "        return train_data, test_data\n",
    "    \n",
    "    else:\n",
    "        train_data = pd.read_table(os.path.join(in_path, 'train.tsv'))\n",
    "        test_data  = pd.read_table(os.path.join(in_path, 'test.tsv'))\n",
    "    \n",
    "        train_rows = len(train_data)\n",
    "        data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "        \n",
    "        data['name'] = data['name'].astype(str)\n",
    "        data['item_description'] = data['item_description'].astype(str)\n",
    "        \n",
    "        #ddata = dd.from_pandas(data, 4)\n",
    "\n",
    "        \n",
    "        data = applycat1(data)\n",
    "        data = applycat2(data)\n",
    "        data = applycat3(data)\n",
    "        data.fillna(-1, inplace=True)\n",
    "        cat_cols = ['category_name', 'brand_name', 'item_condition_id', 'cat1', 'cat2', 'cat3']\n",
    "        print(\"Label enoding categoricals\")\n",
    "        for col in cat_cols:\n",
    "            data[col] = LabelEncoder().fit_transform(data[col].astype(str)).astype(np.int32)\n",
    "            \n",
    "        print(\"Tokenizing text columns\")\n",
    "        data['name'] = parallelize_dataframe(data['name'], applyname)\n",
    "        print(\"Preparing vocabs\")\n",
    "        global name_vocab\n",
    "        name_vocab = prepareVocab('name', data[['name']], 50000)\n",
    "        data['name'] = name2index(data['name'])\n",
    "        del name_vocab\n",
    "        \n",
    "        print(\"Transforming text to sequences\")\n",
    "        data['item_description'] = parallelize_dataframe(data['item_description'], applydesc)\n",
    "        global desc_vocab\n",
    "        desc_vocab = prepareVocab('item_description', data[['item_description']], 50000)\n",
    "        data['item_description'] = desc2index(data['item_description'])\n",
    "        del desc_vocab\n",
    "        \n",
    "        train_data = data.loc[: train_rows - 1, :]\n",
    "        train_data = train_data.loc[(train_data.price >= 3) & (train_data.price <= 2000), :].reset_index(drop=True)\n",
    "        test_data  = data.loc[train_rows: , :].reset_index(drop=True)\n",
    "        \n",
    "        del train_data['test_id']\n",
    "        del test_data['train_id']\n",
    "        del data\n",
    "        gc.collect()\n",
    "        print(\"Writing out new pickles dataframes\")\n",
    "        train_data.to_pickle(os.path.join(out_path, 'train_2.pkl'))\n",
    "        test_data.to_pickle(os.path.join(out_path, 'test_2.pkl'))\n",
    "        \n",
    "        return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "c2e58e31-d868-43b6-bd09-946767c39010",
    "_uuid": "212d78f6a1cd4ba08c63f561508fcd165f1c01fa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2175894/2175894 [00:02<00:00, 813412.97it/s]\n",
      "100%|██████████| 2175894/2175894 [00:03<00:00, 616726.31it/s]\n",
      "100%|██████████| 2175894/2175894 [00:03<00:00, 574410.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label enoding categoricals\n",
      "Tokenizing text columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72530/72530 [00:03<00:00, 23079.55it/s]\n",
      "100%|██████████| 72530/72530 [00:03<00:00, 22854.34it/s]\n",
      "100%|██████████| 72530/72530 [00:03<00:00, 23605.23it/s]\n",
      "100%|██████████| 72530/72530 [00:03<00:00, 22775.35it/s]\n",
      "100%|██████████| 72530/72530 [00:03<00:00, 22318.84it/s]\n",
      "100%|██████████| 72530/72530 [00:03<00:00, 22948.05it/s]\n",
      "100%|██████████| 72530/72530 [00:03<00:00, 22705.52it/s]\n",
      "100%|██████████| 72530/72530 [00:03<00:00, 22278.23it/s]\n",
      "100%|██████████| 72530/72530 [00:03<00:00, 22934.23it/s]\n",
      "100%|██████████| 72530/72530 [00:03<00:00, 21508.33it/s]\n",
      "100%|██████████| 72530/72530 [00:03<00:00, 23358.91it/s]\n",
      "  4%|▍         | 3003/72530 [00:00<00:04, 15431.23it/s]]\n",
      "100%|██████████| 72530/72530 [00:03<00:00, 21866.61it/s]\n",
      "100%|██████████| 72530/72530 [00:03<00:00, 22577.77it/s]\n",
      "100%|██████████| 72530/72530 [00:03<00:00, 21462.82it/s]\n",
      "100%|██████████| 72530/72530 [00:03<00:00, 21561.35it/s]\n",
      "100%|██████████| 72530/72530 [00:02<00:00, 33201.57it/s]\n",
      "100%|██████████| 72530/72530 [00:02<00:00, 33924.84it/s]\n",
      " 11%|█▏        | 8272/72529 [00:00<00:02, 28299.53it/s]]\n",
      "100%|██████████| 72530/72530 [00:02<00:00, 31892.12it/s]\n",
      "100%|██████████| 72530/72530 [00:02<00:00, 30544.74it/s]\n",
      "100%|██████████| 72530/72530 [00:02<00:00, 27813.09it/s]\n",
      "100%|██████████| 72530/72530 [00:02<00:00, 28988.79it/s]\n",
      "100%|██████████| 72530/72530 [00:02<00:00, 25694.33it/s]\n",
      "100%|██████████| 72529/72529 [00:02<00:00, 32168.92it/s]\n",
      "100%|██████████| 72530/72530 [00:02<00:00, 26264.66it/s]\n",
      "100%|██████████| 72529/72529 [00:02<00:00, 25588.00it/s]\n",
      "100%|██████████| 72529/72529 [00:02<00:00, 28714.59it/s]\n",
      "100%|██████████| 72529/72529 [00:02<00:00, 29037.45it/s]\n",
      "100%|██████████| 72529/72529 [00:02<00:00, 30239.61it/s]\n",
      "100%|██████████| 72529/72529 [00:02<00:00, 31697.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing vocabs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 13306/2175894 [00:00<00:16, 133056.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted words:\n",
      "name 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2175894/2175894 [00:15<00:00, 138474.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming text to sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72530/72530 [00:07<00:00, 9361.64it/s]]\n",
      "100%|██████████| 72530/72530 [00:07<00:00, 9265.97it/s] \n",
      "100%|██████████| 72530/72530 [00:08<00:00, 8998.05it/s]\n",
      "100%|██████████| 72530/72530 [00:07<00:00, 9101.70it/s]\n",
      "100%|██████████| 72530/72530 [00:08<00:00, 9019.85it/s]\n",
      "100%|██████████| 72530/72530 [00:08<00:00, 9025.59it/s]\n",
      "100%|██████████| 72530/72530 [00:08<00:00, 8767.98it/s]\n",
      "100%|██████████| 72530/72530 [00:08<00:00, 8986.46it/s]\n",
      "100%|██████████| 72530/72530 [00:07<00:00, 9181.96it/s]\n",
      "100%|██████████| 72530/72530 [00:08<00:00, 8912.42it/s]\n",
      "100%|██████████| 72530/72530 [00:08<00:00, 8925.70it/s]\n",
      "100%|██████████| 72530/72530 [00:08<00:00, 8745.38it/s]\n",
      "100%|██████████| 72530/72530 [00:08<00:00, 8825.58it/s]\n",
      "100%|██████████| 72530/72530 [00:08<00:00, 8862.39it/s]\n",
      "100%|██████████| 72530/72530 [00:08<00:00, 8503.16it/s]\n",
      "100%|██████████| 72530/72530 [00:08<00:00, 8755.29it/s]\n",
      "100%|██████████| 72530/72530 [00:05<00:00, 13059.71it/s]\n",
      "100%|██████████| 72530/72530 [00:05<00:00, 14319.45it/s]\n",
      "100%|██████████| 72530/72530 [00:05<00:00, 13055.55it/s]\n",
      "100%|██████████| 72530/72530 [00:06<00:00, 11181.18it/s]\n",
      "100%|██████████| 72530/72530 [00:06<00:00, 11445.32it/s]\n",
      "100%|██████████| 72530/72530 [00:06<00:00, 11253.24it/s]\n",
      "100%|██████████| 72530/72530 [00:06<00:00, 10820.54it/s]\n",
      " 69%|██████▉   | 50081/72529 [00:04<00:02, 10890.58it/s]\n",
      "100%|██████████| 72529/72529 [00:06<00:00, 11182.31it/s]\n",
      "100%|██████████| 72529/72529 [00:06<00:00, 10437.06it/s]\n",
      "100%|██████████| 72529/72529 [00:06<00:00, 10900.29it/s]\n",
      "100%|██████████| 72529/72529 [00:06<00:00, 10875.93it/s]\n",
      "100%|██████████| 72529/72529 [00:06<00:00, 11436.94it/s]\n",
      "\n",
      "  0%|          | 5528/2175894 [00:00<00:39, 55273.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted words:\n",
      "item_description 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2175894/2175894 [00:32<00:00, 67027.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing out new pickles dataframes\n",
      "CPU times: user 2min 30s, sys: 8.18 s, total: 2min 38s\n",
      "Wall time: 2min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data, test_data = read_data(\"../input\", \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "39a10095-f0e9-445e-a034-7cb5f8bfdd35",
    "_uuid": "cee4601fe1f9f5b34078a47615bea665a4e8ade8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1481658, 11) (693359, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>item_description</th>\n",
       "      <th>name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>train_id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>830</td>\n",
       "      <td>2</td>\n",
       "      <td>[3, 34, 37, 39, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[4084, 9421, 10256, 125, 16, 3, 55, 20981, 326...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>103</td>\n",
       "      <td>774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3890</td>\n",
       "      <td>87</td>\n",
       "      <td>2</td>\n",
       "      <td>[4327, 16, 8, 153, 28, 983, 2, 31, 2, 3952, 65...</td>\n",
       "      <td>[12669, 53875, 34664, 2087, 62723, 0, 0, 0, 0, 0]</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4589</td>\n",
       "      <td>1278</td>\n",
       "      <td>0</td>\n",
       "      <td>[523, 53, 8072, 2, 214, 1076, 1227, 61, 2204, ...</td>\n",
       "      <td>[208, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9</td>\n",
       "      <td>104</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>504</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 32, 154, 14565, 147, 11, 1211, 1797, 109, ...</td>\n",
       "      <td>[119, 2139, 44620, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1205</td>\n",
       "      <td>0</td>\n",
       "      <td>[788, 12732, 2, 2072, 24425, 6156, 0, 0, 0, 0,...</td>\n",
       "      <td>[6132, 46, 1122, 145, 9686, 1480, 93627, 0, 0, 0]</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9</td>\n",
       "      <td>59</td>\n",
       "      <td>543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   brand_name  category_name  item_condition_id  \\\n",
       "0           2            830                  2   \n",
       "1        3890             87                  2   \n",
       "2        4589           1278                  0   \n",
       "3           2            504                  0   \n",
       "4           2           1205                  0   \n",
       "\n",
       "                                    item_description  \\\n",
       "0  [3, 34, 37, 39, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1  [4327, 16, 8, 153, 28, 983, 2, 31, 2, 3952, 65...   \n",
       "2  [523, 53, 8072, 2, 214, 1076, 1227, 61, 2204, ...   \n",
       "3  [0, 32, 154, 14565, 147, 11, 1211, 1797, 109, ...   \n",
       "4  [788, 12732, 2, 2072, 24425, 6156, 0, 0, 0, 0,...   \n",
       "\n",
       "                                                name  price  shipping  \\\n",
       "0  [4084, 9421, 10256, 125, 16, 3, 55, 20981, 326...   10.0         1   \n",
       "1  [12669, 53875, 34664, 2087, 62723, 0, 0, 0, 0, 0]   52.0         0   \n",
       "2                   [208, 0, 0, 0, 0, 0, 0, 0, 0, 0]   10.0         1   \n",
       "3            [119, 2139, 44620, 0, 0, 0, 0, 0, 0, 0]   35.0         1   \n",
       "4  [6132, 46, 1122, 145, 9686, 1480, 93627, 0, 0, 0]   44.0         0   \n",
       "\n",
       "   train_id  cat1  cat2  cat3  \n",
       "0       0.0     5   103   774  \n",
       "1       1.0     1    31   216  \n",
       "2       2.0     9   104    98  \n",
       "3       3.0     3    56   411  \n",
       "4       4.0     9    59   543  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvlist = list(KFold(n_splits=10).split(train_data, train_data.price))\n",
    "print(train_data.shape, test_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_cell_guid": "2cf1463d-35eb-4354-a8f8-5022d684ef69",
    "_uuid": "9f2ed593fa4885bb1444475468aece83e08f0f65"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "class EM_NNRegressor(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, embed_cols=None, dense_cols=None, embed_dims=None, \n",
    "                 text_embed_cols=None, text_embed_seq_lens=None, \n",
    "                 text_embed_dims=None,\n",
    "                 num_layers=2, multiprocess=False,\n",
    "                layer_activations=None, layer_dims=None,layer_dropouts=None, epochs=20, batchsize=32,\n",
    "                optimizer_kwargs=None, val_size=0.1, verbose=1, seed=1):\n",
    "        \n",
    "        self.embed_cols = embed_cols\n",
    "        self.dense_cols = dense_cols\n",
    "        self.embed_dims = embed_dims\n",
    "        self.text_embed_cols = text_embed_cols\n",
    "        self.text_embed_dims = text_embed_dims\n",
    "        #self.text_embed_tokenizers = text_embed_tokenizers\n",
    "        self.text_embed_seq_lens = text_embed_seq_lens\n",
    "        self.dense_dims = None\n",
    "        self.num_layers = num_layers\n",
    "        self.layer_dims = layer_dims\n",
    "        self.layer_activations = layer_activations\n",
    "        self.layer_dropouts = layer_dropouts\n",
    "        self.epochs = epochs\n",
    "        self.batchsize = batchsize\n",
    "        self.optimizer_kwargs = optimizer_kwargs\n",
    "        self.val_size = val_size\n",
    "        self.verbose = verbose\n",
    "        self.multiprocess = multiprocess\n",
    "        self.seed = seed\n",
    "        self.model = None\n",
    "        if self.dense_cols:\n",
    "            self.dense_dims = len(self.dense_cols)\n",
    "            \n",
    "    def _splitX(self, X):\n",
    "        X_splits = []\n",
    "        \n",
    "        if self.embed_cols:\n",
    "            for col in self.embed_cols :\n",
    "                X_splits.append(np.asarray(X[col]))\n",
    "                \n",
    "        if self.text_embed_cols:\n",
    "            for i, col in enumerate(self.text_embed_cols):\n",
    "                X_splits.append(np.asarray([*X[col].values]))\n",
    "                \n",
    "        if self.dense_cols:\n",
    "            X_splits.append(X[self.dense_cols].values.reshape(X.shape[0], -1))\n",
    "            \n",
    "        return X_splits\n",
    "    \n",
    "    \n",
    "    def _build_model(self):\n",
    "        model_inputs = []\n",
    "        model_layers = []\n",
    "        \n",
    "        if self.embed_cols:\n",
    "            for col, dim in zip(self.embed_cols, self.embed_dims):\n",
    "                x1 = Input( shape=(1,), name=col)\n",
    "                model_inputs.append(x1)\n",
    "                x1 = Embedding(input_dim=dim[0], output_dim=dim[1],)(x1)\n",
    "                #x1 = Dropout(0.1)(x1)\n",
    "                x1 = Reshape(target_shape=(dim[1],))(x1)\n",
    "                model_layers.append(x1)\n",
    "                \n",
    "        if self.text_embed_cols:\n",
    "            for col, dim, seq_len in zip(self.text_embed_cols, \n",
    "                                                self.text_embed_dims, \n",
    "                                                self.text_embed_seq_lens):\n",
    "                x3 = Input( shape=(seq_len,))\n",
    "                model_inputs.append(x3)\n",
    "                x3 = Embedding(input_dim=dim[0], output_dim=dim[1], input_length=seq_len,)(x3)\n",
    "                #x3 = Conv1D(16, return_sequences=True)(x3)\n",
    "                x3 = GlobalAveragePooling1D()(x3)\n",
    "                x3 = Reshape(target_shape=(dim[1],))(x3)\n",
    "                model_layers.append(x3)\n",
    "                \n",
    "        if self.dense_cols:\n",
    "            x2 = Input( shape=(self.dense_dims, ), name='dense_cols')\n",
    "            model_inputs.append(x2)\n",
    "            model_layers.append(x2)\n",
    "        print(model_layers)\n",
    "        x = concatenate(model_layers)\n",
    "        \n",
    "        if self.num_layers > 0:\n",
    "            for dim, drops in zip(self.layer_dims, self.layer_dropouts):\n",
    "                x = BatchNormalization()(x)\n",
    "                x = Dropout(rate=drops)(x)\n",
    "                x = Dense(dim, activation='selu', kernel_initializer='he_normal')(x)\n",
    "                #x = PReLU()(x)\n",
    "        \n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.02)(x)\n",
    "        output = Dense(1, activation='linear', kernel_initializer='normal')(x)\n",
    "        \n",
    "        model = Model(inputs=model_inputs, outputs=output)\n",
    "        #print(model.summary())\n",
    "        #adam = RMSprop(lr=0.001, decay=0.001)\n",
    "        adam = Adam(lr=0.001, decay=1e-4)\n",
    "        model.compile(optimizer=adam, loss='mean_squared_error' )\n",
    "        \n",
    "        return model \n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model = self._build_model()\n",
    "        if self.val_size > 0:\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.val_size, random_state=self.seed)\n",
    "            print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "            \n",
    "            callbacks= [ModelCheckpoint(\"embed_NN_\"+str(self.seed)+\".check\", save_best_only=True, verbose=1)]\n",
    "            if self.multiprocess == False:\n",
    "                self.model.fit(self._splitX(X_train), y_train, batch_size=self.batchsize, epochs=self.epochs,\n",
    "                               verbose=self.verbose,\n",
    "                              validation_data=(self._splitX(X_val), y_val), shuffle=True,\n",
    "                              callbacks=callbacks)\n",
    "            else:\n",
    "                X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.val_size, random_state=1)\n",
    "\n",
    "        else:\n",
    "            self.model.fit(self._splitX(X), y, batch_size=self.batchsize, epochs=self.epochs,\n",
    "               verbose=self.verbose, shuffle=True)\n",
    "\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        \n",
    "        if self.model:\n",
    "            self.model = load_model(\"embed_NN_\"+str(self.seed)+\".check\")\n",
    "            y_hat = self.model.predict(self._splitX(X))\n",
    "        else:\n",
    "            raise ValueError(\"Model not fit yet\")\n",
    "            \n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet = EM_NNRegressor(embed_cols=['brand_name','category_name','item_condition_id', 'cat1', 'cat2', 'cat3'], \n",
    "                      embed_dims=[(6000, 30),(1500, 15), (5,4), (15,4), (120, 10), (900, 15)],\n",
    "                      text_embed_cols=['name', 'item_description'],\n",
    "                      text_embed_dims=[(50000, 80), (50000, 80)],\n",
    "                      text_embed_seq_lens =[10, 80], \n",
    "                      dense_cols=['shipping'],\n",
    "                      epochs=10,\n",
    "                      batchsize=2048,\n",
    "                      num_layers = 1,\n",
    "                      layer_dropouts=[0.1],\n",
    "                      layer_dims=[200],\n",
    "                      val_size=0.05\n",
    "                     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_workers = multiprocessing.cpu_count()\n",
    "#batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_48/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_49/Reshape:0' shape=(?, 15) dtype=float32>, <tf.Tensor 'reshape_50/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_51/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_52/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_53/Reshape:0' shape=(?, 15) dtype=float32>, <tf.Tensor 'reshape_54/Reshape:0' shape=(?, 80) dtype=float32>, <tf.Tensor 'reshape_55/Reshape:0' shape=(?, 80) dtype=float32>, <tf.Tensor 'dense_cols_5:0' shape=(?, 1) dtype=float32>]\n",
      "(1407575, 11) (74083, 11) (1407575,) (74083,)\n",
      "Train on 1407575 samples, validate on 74083 samples\n",
      "Epoch 1/10\n",
      "1402880/1407575 [============================>.] - ETA: 0s - loss: 1.0062Epoch 00001: val_loss improved from inf to 0.22000, saving model to embed_NN_0.check\n",
      "1407575/1407575 [==============================] - 15s 11us/step - loss: 1.0037 - val_loss: 0.2200\n",
      "Epoch 2/10\n",
      "1402880/1407575 [============================>.] - ETA: 0s - loss: 0.2257Epoch 00002: val_loss improved from 0.22000 to 0.19323, saving model to embed_NN_0.check\n",
      "1407575/1407575 [==============================] - 13s 9us/step - loss: 0.2257 - val_loss: 0.1932\n",
      "Epoch 3/10\n",
      "1402880/1407575 [============================>.] - ETA: 0s - loss: 0.1950Epoch 00003: val_loss improved from 0.19323 to 0.18556, saving model to embed_NN_0.check\n",
      "1407575/1407575 [==============================] - 13s 9us/step - loss: 0.1950 - val_loss: 0.1856\n",
      "Epoch 4/10\n",
      "1402880/1407575 [============================>.] - ETA: 0s - loss: 0.1755Epoch 00004: val_loss improved from 0.18556 to 0.18446, saving model to embed_NN_0.check\n",
      "1407575/1407575 [==============================] - 14s 10us/step - loss: 0.1755 - val_loss: 0.1845\n",
      "Epoch 5/10\n",
      "1402880/1407575 [============================>.] - ETA: 0s - loss: 0.1608Epoch 00005: val_loss improved from 0.18446 to 0.17834, saving model to embed_NN_0.check\n",
      "1407575/1407575 [==============================] - 13s 10us/step - loss: 0.1608 - val_loss: 0.1783\n",
      "Epoch 6/10\n",
      "1402880/1407575 [============================>.] - ETA: 0s - loss: 0.1494Epoch 00006: val_loss improved from 0.17834 to 0.17827, saving model to embed_NN_0.check\n",
      "1407575/1407575 [==============================] - 14s 10us/step - loss: 0.1494 - val_loss: 0.1783\n",
      "Epoch 7/10\n",
      "1402880/1407575 [============================>.] - ETA: 0s - loss: 0.1395Epoch 00007: val_loss did not improve\n",
      "1407575/1407575 [==============================] - 13s 9us/step - loss: 0.1395 - val_loss: 0.1789\n",
      "Epoch 8/10\n",
      "1402880/1407575 [============================>.] - ETA: 0s - loss: 0.1306Epoch 00008: val_loss did not improve\n",
      "1407575/1407575 [==============================] - 13s 9us/step - loss: 0.1306 - val_loss: 0.1789\n",
      "Epoch 9/10\n",
      "1402880/1407575 [============================>.] - ETA: 0s - loss: 0.1229Epoch 00009: val_loss did not improve\n",
      "1407575/1407575 [==============================] - 13s 9us/step - loss: 0.1229 - val_loss: 0.1806\n",
      "Epoch 10/10\n",
      "1402880/1407575 [============================>.] - ETA: 0s - loss: 0.1158Epoch 00010: val_loss did not improve\n",
      "1407575/1407575 [==============================] - 14s 10us/step - loss: 0.1158 - val_loss: 0.1840\n"
     ]
    }
   ],
   "source": [
    "#scores = cross_val_score(nnet, train_data, np.log1p(train_data.price), scoring=rmse_sklearn, cv=cvlist, verbose =10)\n",
    "#print(scores, np.mean(scores))\n",
    "for seed in range(1):\n",
    "    nnet.set_params(seed=seed).fit(train_data, np.log1p(train_data.price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, 80)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "brand_name (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "category_name (InputLayer)      (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_condition_id (InputLayer)  (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat1 (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat2 (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat3 (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_54 (Embedding)        (None, 10, 80)       4000000     input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_55 (Embedding)        (None, 80, 80)       4000000     input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_48 (Embedding)        (None, 1, 30)        180000      brand_name[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_49 (Embedding)        (None, 1, 15)        22500       category_name[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_50 (Embedding)        (None, 1, 4)         20          item_condition_id[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "embedding_51 (Embedding)        (None, 1, 4)         60          cat1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_52 (Embedding)        (None, 1, 10)        1200        cat2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_53 (Embedding)        (None, 1, 15)        13500       cat3[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 80)           0           embedding_54[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_13 (Gl (None, 80)           0           embedding_55[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_48 (Reshape)            (None, 30)           0           embedding_48[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_49 (Reshape)            (None, 15)           0           embedding_49[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_50 (Reshape)            (None, 4)            0           embedding_50[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_51 (Reshape)            (None, 4)            0           embedding_51[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_52 (Reshape)            (None, 10)           0           embedding_52[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_53 (Reshape)            (None, 15)           0           embedding_53[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_54 (Reshape)            (None, 80)           0           global_average_pooling1d_12[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "reshape_55 (Reshape)            (None, 80)           0           global_average_pooling1d_13[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_cols (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 239)          0           reshape_48[0][0]                 \n",
      "                                                                 reshape_49[0][0]                 \n",
      "                                                                 reshape_50[0][0]                 \n",
      "                                                                 reshape_51[0][0]                 \n",
      "                                                                 reshape_52[0][0]                 \n",
      "                                                                 reshape_53[0][0]                 \n",
      "                                                                 reshape_54[0][0]                 \n",
      "                                                                 reshape_55[0][0]                 \n",
      "                                                                 dense_cols[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 239)          956         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 239)          0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 200)          48000       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 200)          800         dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 200)          0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1)            201         dropout_12[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 8,267,237\n",
      "Trainable params: 8,266,359\n",
      "Non-trainable params: 878\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nnet.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7fbeb917add8>,\n",
       " <keras.engine.topology.InputLayer at 0x7fbeb912ef60>,\n",
       " <keras.engine.topology.InputLayer at 0x7fbebd4ae5f8>,\n",
       " <keras.engine.topology.InputLayer at 0x7fbebd4aef28>,\n",
       " <keras.engine.topology.InputLayer at 0x7fbebaa74d68>,\n",
       " <keras.engine.topology.InputLayer at 0x7fbebaa98ac8>,\n",
       " <keras.engine.topology.InputLayer at 0x7fbebaa95630>,\n",
       " <keras.engine.topology.InputLayer at 0x7fbebaa31198>,\n",
       " <keras.layers.embeddings.Embedding at 0x7fbeb9193a58>,\n",
       " <keras.layers.embeddings.Embedding at 0x7fbeb912e470>,\n",
       " <keras.layers.embeddings.Embedding at 0x7fbebd4aedd8>,\n",
       " <keras.layers.embeddings.Embedding at 0x7fbebaa749e8>,\n",
       " <keras.layers.embeddings.Embedding at 0x7fbebaab5ef0>,\n",
       " <keras.layers.embeddings.Embedding at 0x7fbebaa955f8>,\n",
       " <keras.layers.embeddings.Embedding at 0x7fbebaa31e80>,\n",
       " <keras.layers.embeddings.Embedding at 0x7fbebaa48c50>,\n",
       " <keras.layers.pooling.GlobalAveragePooling1D at 0x7fbeb91937b8>,\n",
       " <keras.layers.pooling.GlobalAveragePooling1D at 0x7fbeb914bef0>,\n",
       " <keras.layers.core.Reshape at 0x7fbebd4bc860>,\n",
       " <keras.layers.core.Reshape at 0x7fbebaa749b0>,\n",
       " <keras.layers.core.Reshape at 0x7fbebaa98f98>,\n",
       " <keras.layers.core.Reshape at 0x7fbebaa95a90>,\n",
       " <keras.layers.core.Reshape at 0x7fbebaa311d0>,\n",
       " <keras.layers.core.Reshape at 0x7fbebaa62e48>,\n",
       " <keras.layers.core.Reshape at 0x7fbeb912ea20>,\n",
       " <keras.layers.core.Reshape at 0x7fbeb9162470>,\n",
       " <keras.engine.topology.InputLayer at 0x7fbebd4ae630>,\n",
       " <keras.layers.merge.Concatenate at 0x7fbebd4ae4e0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fbeb91624a8>,\n",
       " <keras.layers.core.Dropout at 0x7fbeb91172b0>,\n",
       " <keras.layers.core.Dense at 0x7fbeb90b27f0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fbeb9162f98>,\n",
       " <keras.layers.core.Dropout at 0x7fbeb903a080>,\n",
       " <keras.layers.core.Dense at 0x7fbeb900bcf8>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet.model.layers[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "9270ef51-217b-45d0-8f7c-95b648c92783",
    "_uuid": "e2e2b96ae3857a18055ff22f16532a6b5b39ec8a"
   },
   "outputs": [],
   "source": [
    "#Checking sequence lengths\n",
    "#sns.distplot(data.item_description.apply(lambda x: len(str(x).split())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
