{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array('B', [1, 2, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import array\n",
    "arr = array.array('B')\n",
    "arr.fromlist([1,2,3])\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "e6826013-0719-4f1e-ab4e-3bb475ca2f73",
    "_uuid": "69973338b67e6a1d8ac846f009608ac848de6941"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a897a74e4268485bbe4bfb939fe2fc10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/test/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import hashlib\n",
    "import string\n",
    "import unicodedata\n",
    "import re\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm.pandas(tqdm_notebook())\n",
    "\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder, QuantileTransformer\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from itertools import chain\n",
    "\n",
    "# from __future__ import print_function\n",
    "np.random.seed(786)  # for reproducibility\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D, ZeroPadding1D, AveragePooling1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier,  KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "9bc40f4e-17d3-438f-96dc-9df02c9b203a",
    "_uuid": "3d180491dde0e6310b4c6c3b825abee24ea3d2a9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Functions we need - Feature Selector, Fasttext_Estimator, Preprocessing Transformer, Binary_Encoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "from pandas.api.types import is_numeric_dtype, is_string_dtype\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "rmse_sklearn = make_scorer(rmse, greater_is_better=False)    \n",
    "    \n",
    "# the following functions allow for a parallelized batch generator\n",
    "class threadsafe_iter(object):\n",
    "    \"\"\"\n",
    "    Takes an iterator/generator and makes it thread-safe by\n",
    "    serializing call to the `next` method of given iterator/generator.\n",
    "    \"\"\"\n",
    "    def __init__(self, it):\n",
    "        self.it = it\n",
    "        self.lock = threading.Lock()\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        with self.lock:\n",
    "            return next(self.it)\n",
    "\n",
    "def threadsafe_generator(f):\n",
    "    \"\"\"\n",
    "    A decorator that takes a generator function and makes it thread-safe.\n",
    "    \"\"\"\n",
    "    def g(*a, **kw):\n",
    "        return threadsafe_iter(f(*a, **kw))\n",
    "    return g\n",
    "\n",
    "@threadsafe_generator\n",
    "def batch_generator(X_data, y_data, batch_size):\n",
    "    \n",
    "    #index = np.random.permutation(X_data.shape[0])    \n",
    "    #X_data = X_data[index]\n",
    "    #y_data = y_data[index]\n",
    "    \n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    index = np.arange(np.shape(y_data)[0])\n",
    "    #idx = 1\n",
    "    while 1:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:].todense()\n",
    "        y_batch = y_data[index_batch]\n",
    "        counter += 1\n",
    "        yield np.array(X_batch),y_batch\n",
    "        #print(\"\")\n",
    "        #print(X_batch.shape)\n",
    "        #print(\"\")\n",
    "        #print('generator yielded a batch %d' % idx)\n",
    "        #idx += 1\n",
    "        if (counter > number_of_batches):\n",
    "            counter=0\n",
    "            \n",
    "            \n",
    "@threadsafe_generator\n",
    "def batch_generator_x(X_data,batch_size):\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    index = np.arange(np.shape(X_data)[0])\n",
    "    while 1:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:].todense()\n",
    "        counter += 1\n",
    "        yield np.array(X_batch)\n",
    "        if (counter > number_of_batches):\n",
    "            counter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dancer', 'coder', 'programmer', ('dancer', 'coder'), ('coder', 'programmer'), ('dancer', 'coder', 'programmer')]\n"
     ]
    }
   ],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "stop_words = ['ourselves', 'hers', 'between', 'yourself', 'but', 'again', \n",
    "              'there', 'about', 'once', 'during', 'out', 'very', 'having', \n",
    "              'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', \n",
    "              'its', 'yours', 'such', 'into', 'most', 'itself', 'other', \n",
    "              'off', 'is', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', \n",
    "              'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', \n",
    "              'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', \n",
    "              'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', \n",
    "              'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them',\n",
    "              'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', \n",
    "              'then', 'that', 'because', 'what', 'over', 'why’, ‘so', 'can', 'did', 'not',\n",
    "              'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only',\n",
    "              'myself', 'which', 'those', 'i','after', 'few', 'whom', 't', 'being', 'if', \n",
    "              'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?'-:;,])\", r\"\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z0-9.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def _normalize_and_ngrams(sent, ngrams):\n",
    "    input_list = normalizeString(sent).split(' ')\n",
    "    input_list = [w for w in input_list if w not in stop_words]\n",
    "    s = input_list.copy()\n",
    "    for i in range(2, ngrams+1):\n",
    "        #s + = [s[j:j+i] for j in range(n-1)]\n",
    "        s += list((zip(*[input_list[j:] for j in range(i)])))\n",
    "    return s\n",
    "\n",
    "print(_normalize_and_ngrams(\"I am not a dance'r and i am a c-o:d;er programmer\", 3))\n",
    "\n",
    "class Vocab_topwords():\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.index2word = {}\n",
    "        \n",
    "    def fit_data(self, data, col, ngrams=3, max_features=50000):\n",
    "        c = Counter(list(chain.from_iterable(data[col].tolist())))\n",
    "        for i, (w, count) in enumerate(c.most_common(max_features)):\n",
    "            self.word2index[w] = i\n",
    "        del c\n",
    "        return\n",
    "    \n",
    "\n",
    "            \n",
    "            \n",
    "def prepareVocab(name, data, max_features):\n",
    "    vocab = Vocab_topwords(name)\n",
    "    vocab.fit_data(data, name, max_features=max_features)\n",
    "    \n",
    "    print(\"Counted words:\")\n",
    "    print(vocab.name, len(vocab.word2index))\n",
    "    return vocab\n",
    "\n",
    "def indexesFromSentence(vocab, tokens, ngrams, max_len):\n",
    "    num_list = []\n",
    "    for i, item in enumerate(tokens):\n",
    "        if len(num_list) == max_len:\n",
    "            break\n",
    "        elif item in vocab.word2index:\n",
    "            num_list.append(vocab.word2index[item])\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "    if len(num_list) < max_len :\n",
    "        num_list += [0]*(max_len - len(num_list) )\n",
    "        \n",
    "    return num_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "36c127ae-46e3-47ba-94e0-0eaa6aa888b4",
    "_uuid": "54695716f99aaaf38b03dd8f11987f21e93ce490",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(in_path, out_path):\n",
    "    if os.path.exists(os.path.join(out_path, 'train_2.pkl')) and os.path.exists(os.path.join(out_path, 'test_2.pkl')):\n",
    "        train_data = pd.read_pickle(os.path.join(out_path, 'train_2.pkl'))\n",
    "        test_data  = pd.read_pickle(os.path.join(out_path, 'test_2.pkl'))\n",
    "        \n",
    "        return train_data, test_data\n",
    "    \n",
    "    else:\n",
    "        train_data = pd.read_table(os.path.join(in_path, 'train.tsv'))\n",
    "        test_data  = pd.read_table(os.path.join(in_path, 'test.tsv'))\n",
    "    \n",
    "        train_rows = len(train_data)\n",
    "        data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "        \n",
    "        data['name'] = data['name'].astype(str)\n",
    "        data['item_description'] = data['item_description'].astype(str)\n",
    "        data['cat1'] = data['category_name'].apply(lambda x: str(x).split('/')[0])\n",
    "        data['cat2'] = data['category_name'].apply(lambda x: str(x).split('/')[1] \n",
    "                                                   if len(str(x).split('/')) > 1 else -1)\n",
    "        data['cat3'] = data['category_name'].apply(lambda x: ' '.join(str(x).split('/')[2:]) \n",
    "                                                   if len(str(x).split('/')) > 2 else -1)\n",
    "        data.fillna(-1, inplace=True)\n",
    "        cat_cols = ['category_name', 'brand_name', 'item_condition_id', 'cat1', 'cat2', 'cat3']\n",
    "        print(\"Label enoding categoricals\")\n",
    "        for col in cat_cols:\n",
    "            data[col] = LabelEncoder().fit_transform(data[col].astype(str)).astype(int)\n",
    "            \n",
    "        print(\"Tokenizing text columns\")\n",
    "        data['name'] = data['name'].progress_apply(lambda s: _normalize_and_ngrams(s, 3))\n",
    "        \n",
    "        print(\"Preparing vocabs\")\n",
    "        name_vocab = prepareVocab('name', data, 100000)\n",
    "        data['name'] = data['name'].progress_apply(lambda x: indexesFromSentence(name_vocab, x, 3, 15))\n",
    "        del name_vocab\n",
    "        \n",
    "        print(\"Transforming text to sequences\")\n",
    "        data['item_description'] = data['item_description'].progress_apply(lambda s: _normalize_and_ngrams(s, 2))\n",
    "        desc_vocab = prepareVocab('item_description', data, 200000)\n",
    "        data['item_description'] = data['item_description'].progress_apply(lambda x: indexesFromSentence(desc_vocab, x, 2, 100))\n",
    "        del desc_vocab\n",
    "        \n",
    "        train_data = data.loc[: train_rows - 1, :]\n",
    "        train_data = train_data.loc[(train_data.price >= 3) & (train_data.price <= 2000), :].reset_index(drop=True)\n",
    "        test_data  = data.loc[train_rows: , :].reset_index(drop=True)\n",
    "        \n",
    "        del train_data['test_id']\n",
    "        del test_data['train_id']\n",
    "        del data\n",
    "        gc.collect()\n",
    "        print(\"Writing out new pickles dataframes\")\n",
    "        train_data.to_pickle(os.path.join(out_path, 'train_2.pkl'))\n",
    "        test_data.to_pickle(os.path.join(out_path, 'test_2.pkl'))\n",
    "        \n",
    "        return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "c2e58e31-d868-43b6-bd09-946767c39010",
    "_uuid": "212d78f6a1cd4ba08c63f561508fcd165f1c01fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.3 s, sys: 1.18 s, total: 20.5 s\n",
      "Wall time: 21.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data, test_data = read_data(\"../input\", \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "39a10095-f0e9-445e-a034-7cb5f8bfdd35",
    "_uuid": "cee4601fe1f9f5b34078a47615bea665a4e8ade8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1481658, 11) (693359, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>item_description</th>\n",
       "      <th>name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>train_id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>830</td>\n",
       "      <td>2</td>\n",
       "      <td>[27, 31, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[3870, 8999, 9785, 16, 4, 39, 20034, 885, 600,...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>103</td>\n",
       "      <td>774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3890</td>\n",
       "      <td>87</td>\n",
       "      <td>2</td>\n",
       "      <td>[4014, 13, 6, 141, 23, 912, 2, 25, 2, 3644, 61...</td>\n",
       "      <td>[12104, 51440, 34579, 2032, 59792, 0, 0, 0, 0,...</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4589</td>\n",
       "      <td>1278</td>\n",
       "      <td>0</td>\n",
       "      <td>[488, 45, 7295, 2, 199, 994, 1140, 53, 2065, 2...</td>\n",
       "      <td>[200, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9</td>\n",
       "      <td>104</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>504</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 26, 143, 13183, 137, 8, 1130, 1637, 101, 2...</td>\n",
       "      <td>[108, 2040, 42618, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1205</td>\n",
       "      <td>0</td>\n",
       "      <td>[736, 11479, 2, 1909, 21953, 5567, 0, 0, 0, 0,...</td>\n",
       "      <td>[151, 44, 1066, 136, 629, 1395, 2633, 0, 0, 0,...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9</td>\n",
       "      <td>59</td>\n",
       "      <td>543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   brand_name  category_name  item_condition_id  \\\n",
       "0           2            830                  2   \n",
       "1        3890             87                  2   \n",
       "2        4589           1278                  0   \n",
       "3           2            504                  0   \n",
       "4           2           1205                  0   \n",
       "\n",
       "                                    item_description  \\\n",
       "0  [27, 31, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [4014, 13, 6, 141, 23, 912, 2, 25, 2, 3644, 61...   \n",
       "2  [488, 45, 7295, 2, 199, 994, 1140, 53, 2065, 2...   \n",
       "3  [0, 26, 143, 13183, 137, 8, 1130, 1637, 101, 2...   \n",
       "4  [736, 11479, 2, 1909, 21953, 5567, 0, 0, 0, 0,...   \n",
       "\n",
       "                                                name  price  shipping  \\\n",
       "0  [3870, 8999, 9785, 16, 4, 39, 20034, 885, 600,...   10.0         1   \n",
       "1  [12104, 51440, 34579, 2032, 59792, 0, 0, 0, 0,...   52.0         0   \n",
       "2    [200, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   10.0         1   \n",
       "3  [108, 2040, 42618, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   35.0         1   \n",
       "4  [151, 44, 1066, 136, 629, 1395, 2633, 0, 0, 0,...   44.0         0   \n",
       "\n",
       "   train_id  cat1  cat2  cat3  \n",
       "0       0.0     5   103   774  \n",
       "1       1.0     1    31   216  \n",
       "2       2.0     9   104    98  \n",
       "3       3.0     3    56   411  \n",
       "4       4.0     9    59   543  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvlist = list(KFold(n_splits=5).split(train_data, train_data.price))\n",
    "print(train_data.shape, test_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "2cf1463d-35eb-4354-a8f8-5022d684ef69",
    "_uuid": "9f2ed593fa4885bb1444475468aece83e08f0f65",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EM_NNRegressor(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, embed_cols=None, dense_cols=None, embed_dims=None, \n",
    "                 text_embed_cols=None, text_embed_seq_lens=None, \n",
    "                 text_embed_dims=None,\n",
    "                 num_layers=2, multiprocess=False,\n",
    "                layer_activations=None, layer_dims=None,layer_dropouts=None, epochs=20, batchsize=32,\n",
    "                optimizer_kwargs=None, val_size=0.1, verbose=1, seed=1):\n",
    "        \n",
    "        self.embed_cols = embed_cols\n",
    "        self.dense_cols = dense_cols\n",
    "        self.embed_dims = embed_dims\n",
    "        self.text_embed_cols = text_embed_cols\n",
    "        self.text_embed_dims = text_embed_dims\n",
    "        #self.text_embed_tokenizers = text_embed_tokenizers\n",
    "        self.text_embed_seq_lens = text_embed_seq_lens\n",
    "        self.dense_dims = None\n",
    "        self.num_layers = num_layers\n",
    "        self.layer_dims = layer_dims\n",
    "        self.layer_activations = layer_activations\n",
    "        self.layer_dropouts = layer_dropouts\n",
    "        self.epochs = epochs\n",
    "        self.batchsize = batchsize\n",
    "        self.optimizer_kwargs = optimizer_kwargs\n",
    "        self.val_size = val_size\n",
    "        self.verbose = verbose\n",
    "        self.multiprocess = multiprocess\n",
    "        self.seed = seed\n",
    "        self.model = None\n",
    "        if self.dense_cols:\n",
    "            self.dense_dims = len(self.dense_cols)\n",
    "            \n",
    "    def _splitX(self, X):\n",
    "        X_splits = []\n",
    "        \n",
    "        if self.embed_cols:\n",
    "            for col in self.embed_cols :\n",
    "                X_splits.append(np.asarray(X[col]))\n",
    "                \n",
    "        if self.text_embed_cols:\n",
    "            for i, col in enumerate(self.text_embed_cols):\n",
    "                X_splits.append(np.asarray([*X[col].values]))\n",
    "                \n",
    "        if self.dense_cols:\n",
    "            X_splits.append(X[self.dense_cols].values.reshape(X.shape[0], -1))\n",
    "            \n",
    "        return X_splits\n",
    "    \n",
    "    \n",
    "    def _build_model(self):\n",
    "        model_inputs = []\n",
    "        model_layers = []\n",
    "        \n",
    "        if self.embed_cols:\n",
    "            for col, dim in zip(self.embed_cols, self.embed_dims):\n",
    "                x1 = Input( shape=(1,), name=col)\n",
    "                model_inputs.append(x1)\n",
    "                x1 = Embedding(input_dim=dim[0], output_dim=dim[1],)(x1)\n",
    "                #x1 = Dropout(0.1)(x1)\n",
    "                x1 = Reshape(target_shape=(dim[1],))(x1)\n",
    "                model_layers.append(x1)\n",
    "                \n",
    "        if self.text_embed_cols:\n",
    "            for col, dim, seq_len in zip(self.text_embed_cols, \n",
    "                                                self.text_embed_dims, \n",
    "                                                self.text_embed_seq_lens):\n",
    "                x3 = Input( shape=(seq_len,))\n",
    "                model_inputs.append(x3)\n",
    "                x3 = Embedding(input_dim=dim[0], output_dim=dim[1], input_length=seq_len,)(x3)\n",
    "                x3 = GlobalAveragePooling1D()(x3)\n",
    "                x3 = Reshape(target_shape=(dim[1],))(x3)\n",
    "                model_layers.append(x3)\n",
    "                \n",
    "        if self.dense_cols:\n",
    "            x2 = Input( shape=(self.dense_dims, ), name='dense_cols')\n",
    "            model_inputs.append(x2)\n",
    "            model_layers.append(x2)\n",
    "        print(model_layers)\n",
    "        x = concatenate(model_layers)\n",
    "        \n",
    "        if self.num_layers > 0:\n",
    "            for dim, drops in zip(self.layer_dims, self.layer_dropouts):\n",
    "                x = BatchNormalization()(x)\n",
    "                x = Dropout(rate=drops)(x)\n",
    "                x = Dense(dim, activation='relu',kernel_initializer='he_normal')(x)\n",
    "                #x = ()(x)\n",
    "        \n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.02)(x)\n",
    "        output = Dense(1, activation='linear', kernel_initializer='normal')(x)\n",
    "        \n",
    "        model = Model(inputs=model_inputs, outputs=output)\n",
    "        #print(model.summary())\n",
    "        adam = Nadam(lr=0.001, schedule_decay=0.004)\n",
    "        model.compile(optimizer=adam, loss='mean_squared_error' )\n",
    "        \n",
    "        return model \n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model = self._build_model()\n",
    "        if self.val_size > 0:\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.val_size, random_state=self.seed)\n",
    "            print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "            \n",
    "            callbacks= [ModelCheckpoint(\"embed_NN_\"+str(self.seed)+\".check\", save_best_only=True, verbose=1)]\n",
    "            if self.multiprocess == False:\n",
    "                self.model.fit(self._splitX(X_train), y_train, batch_size=self.batchsize, epochs=self.epochs,\n",
    "                               verbose=self.verbose,\n",
    "                              validation_data=(self._splitX(X_val), y_val), shuffle=True,\n",
    "                              callbacks=callbacks)\n",
    "            else:\n",
    "                X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.val_size, random_state=1)\n",
    "\n",
    "        else:\n",
    "            self.model.fit(self._splitX(X), y, batch_size=self.batchsize, epochs=self.epochs,\n",
    "               verbose=self.verbose, shuffle=True)\n",
    "\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        \n",
    "        if self.model:\n",
    "            model = load_model(\"embed_NN_\"+str(self.seed)+\".check\")\n",
    "            y_hat = model.predict(self._splitX(X))\n",
    "        else:\n",
    "            raise ValueError(\"Model not fit yet\")\n",
    "            \n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nnet = EM_NNRegressor(embed_cols=['brand_name','category_name','item_condition_id', 'cat1', 'cat2', 'cat3'], \n",
    "                      embed_dims=[(6000, 20),(1500, 20), (5,4), (15,4), (120, 10), (900, 20)],\n",
    "                      text_embed_cols=['name', 'item_description'],\n",
    "                      text_embed_dims=[(100000, 50), (200000, 50)],\n",
    "                      text_embed_seq_lens =[15, 100], \n",
    "                      dense_cols=['shipping'],\n",
    "                      epochs=4,\n",
    "                      batchsize=2056 ,\n",
    "                      num_layers = 1,\n",
    "                      layer_dropouts=[0.3, 0.1],\n",
    "                      layer_dims=[150, 50],\n",
    "                     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#n_workers = multiprocessing.cpu_count()\n",
    "#batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_9/Reshape:0' shape=(?, 20) dtype=float64>, <tf.Tensor 'reshape_10/Reshape:0' shape=(?, 20) dtype=float64>, <tf.Tensor 'reshape_11/Reshape:0' shape=(?, 4) dtype=float64>, <tf.Tensor 'reshape_12/Reshape:0' shape=(?, 4) dtype=float64>, <tf.Tensor 'reshape_13/Reshape:0' shape=(?, 10) dtype=float64>, <tf.Tensor 'reshape_14/Reshape:0' shape=(?, 20) dtype=float64>, <tf.Tensor 'reshape_15/Reshape:0' shape=(?, 50) dtype=float64>, <tf.Tensor 'reshape_16/Reshape:0' shape=(?, 50) dtype=float64>, <tf.Tensor 'dense_cols_1:0' shape=(?, 1) dtype=float64>]\n",
      "(1333492, 11) (148166, 11) (1333492,) (148166,)\n",
      "Train on 1333492 samples, validate on 148166 samples\n",
      "Epoch 1/4\n",
      "1332288/1333492 [============================>.] - ETA: 0s - loss: 1.5399Epoch 00001: val_loss improved from inf to 0.22394, saving model to embed_NN_1.check\n",
      "1333492/1333492 [==============================] - 387s 290us/step - loss: 1.5388 - val_loss: 0.2239\n",
      "Epoch 2/4\n",
      "1332288/1333492 [============================>.] - ETA: 0s - loss: 0.2287Epoch 00002: val_loss improved from 0.22394 to 0.19189, saving model to embed_NN_1.check\n",
      "1333492/1333492 [==============================] - 411s 308us/step - loss: 0.2287 - val_loss: 0.1919\n",
      "Epoch 3/4\n",
      "1332288/1333492 [============================>.] - ETA: 0s - loss: 0.1958Epoch 00003: val_loss improved from 0.19189 to 0.18538, saving model to embed_NN_1.check\n",
      "1333492/1333492 [==============================] - 421s 316us/step - loss: 0.1958 - val_loss: 0.1854\n",
      "Epoch 4/4\n",
      "1332288/1333492 [============================>.] - ETA: 0s - loss: 0.1740Epoch 00004: val_loss improved from 0.18538 to 0.18329, saving model to embed_NN_1.check\n",
      "1333492/1333492 [==============================] - 403s 302us/step - loss: 0.1740 - val_loss: 0.1833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EM_NNRegressor(batchsize=2056, dense_cols=['shipping'],\n",
       "        embed_cols=['brand_name', 'category_name', 'item_condition_id', 'cat1', 'cat2', 'cat3'],\n",
       "        embed_dims=[(6000, 20), (1500, 20), (5, 4), (15, 4), (120, 10), (900, 20)],\n",
       "        epochs=4, layer_activations=None, layer_dims=[150, 50],\n",
       "        layer_dropouts=[0.3, 0.1], multiprocess=False, num_layers=1,\n",
       "        optimizer_kwargs=None, seed=1,\n",
       "        text_embed_cols=['name', 'item_description'],\n",
       "        text_embed_dims=[(100000, 50), (200000, 50)],\n",
       "        text_embed_seq_lens=[15, 100], val_size=0.1, verbose=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet.fit(train_data, np.log1p(train_data.price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "451f6fc1-8746-4ddc-a7b9-4e4e95a4d503",
    "_uuid": "a3c8c03bd93fd18e8088f196e6760dc1efe0db86",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_ngram_set(input_list, ngram_value=2):\n",
    "    \"\"\"\n",
    "    Extract a set of n-grams from a list of integers.\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n",
    "    {(4, 9), (4, 1), (1, 4), (9, 4)}\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n",
    "    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n",
    "    \"\"\"\n",
    "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n",
    "\n",
    "\n",
    "def add_ngram(sequences, token_indice, ngram_range=2):\n",
    "    \"\"\"\n",
    "    Augment the input list of list (sequences) by appending n-grams values.\n",
    "    Example: adding bi-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=2)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n",
    "    Example: adding tri-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=3)\n",
    "    [[1, 3, 4, 5, 1337], [1, 3, 7, 9, 2, 1337, 2018]]\n",
    "    \"\"\"\n",
    "    new_sequences = []\n",
    "    for input_list in sequences:\n",
    "        new_list = input_list[:]\n",
    "        for i in range(len(new_list) - ngram_range + 1):\n",
    "            for ngram_value in range(2, ngram_range + 1):\n",
    "                ngram = tuple(new_list[i:i + ngram_value])\n",
    "                if ngram in token_indice:\n",
    "                    new_list.append(token_indice[ngram])\n",
    "        new_sequences.append(new_list)\n",
    "\n",
    "    return new_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3895b7b7-d54d-4674-850f-9f2b2d7e8292",
    "_uuid": "f326cf1a595686904639ce6e31c8ee9fd977ed2f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#no bigrams - define maxlen\n",
    "#maxlen = 100\n",
    "#print('Pad sequences (samples x time)')\n",
    "#X_train = sequence.pad_sequences(X_train_list, maxlen=maxlen)\n",
    "#X_test = sequence.pad_sequences(X_test_list, maxlen=maxlen)\n",
    "\n",
    "#print('train shape',X_train.shape)\n",
    "#print('test shape',X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9270ef51-217b-45d0-8f7c-95b648c92783",
    "_uuid": "e2e2b96ae3857a18055ff22f16532a6b5b39ec8a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Checking sequence lengths\n",
    "#sns.distplot(data.item_description.apply(lambda x: len(str(x).split())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
