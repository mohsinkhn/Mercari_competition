{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "e6826013-0719-4f1e-ab4e-3bb475ca2f73",
    "_uuid": "69973338b67e6a1d8ac846f009608ac848de6941"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f8e3fe601a466eabdaa55f46d822bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/mohsin/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import hashlib\n",
    "import string\n",
    "import unicodedata\n",
    "import re\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm.pandas(tqdm_notebook())\n",
    "\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder, QuantileTransformer\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "#from nltk.corpus import stopwords\n",
    "#from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from itertools import chain\n",
    "\n",
    "# from __future__ import print_function\n",
    "np.random.seed(786)  # for reproducibility\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D, ZeroPadding1D, AveragePooling1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier,  KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "9bc40f4e-17d3-438f-96dc-9df02c9b203a",
    "_uuid": "3d180491dde0e6310b4c6c3b825abee24ea3d2a9"
   },
   "outputs": [],
   "source": [
    "#Functions we need - Feature Selector, Fasttext_Estimator, Preprocessing Transformer, Binary_Encoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "from pandas.api.types import is_numeric_dtype, is_string_dtype\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "rmse_sklearn = make_scorer(rmse, greater_is_better=False)    \n",
    "    \n",
    "# the following functions allow for a parallelized batch generator\n",
    "class threadsafe_iter(object):\n",
    "    \"\"\"\n",
    "    Takes an iterator/generator and makes it thread-safe by\n",
    "    serializing call to the `next` method of given iterator/generator.\n",
    "    \"\"\"\n",
    "    def __init__(self, it):\n",
    "        self.it = it\n",
    "        self.lock = threading.Lock()\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        with self.lock:\n",
    "            return next(self.it)\n",
    "\n",
    "def threadsafe_generator(f):\n",
    "    \"\"\"\n",
    "    A decorator that takes a generator function and makes it thread-safe.\n",
    "    \"\"\"\n",
    "    def g(*a, **kw):\n",
    "        return threadsafe_iter(f(*a, **kw))\n",
    "    return g\n",
    "\n",
    "@threadsafe_generator\n",
    "def batch_generator(X_data, y_data, batch_size):\n",
    "    \n",
    "    #index = np.random.permutation(X_data.shape[0])    \n",
    "    #X_data = X_data[index]\n",
    "    #y_data = y_data[index]\n",
    "    \n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    index = np.arange(np.shape(y_data)[0])\n",
    "    #idx = 1\n",
    "    while 1:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:].todense()\n",
    "        y_batch = y_data[index_batch]\n",
    "        counter += 1\n",
    "        yield np.array(X_batch),y_batch\n",
    "        #print(\"\")\n",
    "        #print(X_batch.shape)\n",
    "        #print(\"\")\n",
    "        #print('generator yielded a batch %d' % idx)\n",
    "        #idx += 1\n",
    "        if (counter > number_of_batches):\n",
    "            counter=0\n",
    "            \n",
    "            \n",
    "@threadsafe_generator\n",
    "def batch_generator_x(X_data,batch_size):\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    index = np.arange(np.shape(X_data)[0])\n",
    "    while 1:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:].todense()\n",
    "        counter += 1\n",
    "        yield np.array(X_batch)\n",
    "        if (counter > number_of_batches):\n",
    "            counter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "36c127ae-46e3-47ba-94e0-0eaa6aa888b4",
    "_uuid": "54695716f99aaaf38b03dd8f11987f21e93ce490"
   },
   "outputs": [],
   "source": [
    "num_partitions = 30\n",
    "num_cores = 16\n",
    "from multiprocessing import Pool, cpu_count\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "stop_words = ['ourselves', 'hers', 'between', 'yourself', 'but', 'again', \n",
    "              'there', 'about', 'once', 'during', 'out', 'very', 'having', \n",
    "              'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', \n",
    "              'its', 'yours', 'such', 'into', 'most', 'itself', 'other', \n",
    "              'off', 'is', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', \n",
    "              'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', \n",
    "              'through', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', \n",
    "              'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', \n",
    "              'ours', 'had', 'she', 'all', 'when', 'at', 'any', 'before', 'them',\n",
    "              'same', 'and', 'been', 'have', 'in', 'will', 'does', 'yourselves', \n",
    "              'then', 'that', 'because', 'what', 'over', 'why’, ‘so', 'can', 'did',\n",
    "              'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only',\n",
    "              'myself', 'which', 'those', 'i','after', 'few', 'whom', 'being', 'if', \n",
    "              'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return  unicodedata.normalize('NFKC', s)\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"'\", r\"\", s)\n",
    "    s = re.sub(r\"[.!?':;,]\", r\" \", s)\n",
    "    s = re.sub(r\"-\", r\"\", s)\n",
    "    s = re.sub(r\"[^0-9a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"0\", r\"zero\", s)\n",
    "    s = re.sub(r\"1\", r\"one\", s)\n",
    "    s = re.sub(r\"2\", r\"two\", s)\n",
    "    s = re.sub(r\"3\", r\"three\", s)\n",
    "    s = re.sub(r\"4\", r\"four\", s)\n",
    "    s = re.sub(r\"5\", r\"five\", s)\n",
    "    s = re.sub(r\"6\", r\"six\", s)\n",
    "    s = re.sub(r\"7\", r\"seven\", s)\n",
    "    s = re.sub(r\"8\", r\"eight\", s)\n",
    "    #s = re.sub(r\"/s/s\", r\"/s\", s)\n",
    "    return s\n",
    "\n",
    "def _normalize_and_ngrams(sent, ngrams):\n",
    "    input_list = normalizeString(sent).split()\n",
    "    input_list = [word for word in input_list if word not in stop_words]\n",
    "    s = input_list.copy()\n",
    "    for i in range(2, ngrams+1):\n",
    "        s += [' '.join(input_list[j:j+i]) for j in range(len(input_list)-i + 1)]\n",
    "        #s += list((zip(*[input_list[j:] for j in range(i)])))\n",
    "    return s\n",
    "\n",
    "#tmp = \"I am not a dance'r and i am a 6ixy   c-o:d;er programmer\"\n",
    "#print(normalizeString(tmp))\n",
    "#print(_normalize_and_ngrams(tmp, 3))\n",
    "\n",
    "class Vocab_topwords():\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.index2word = {}\n",
    "        \n",
    "    def fit_data(self, data, col, ngrams=3, max_features=50000):\n",
    "        c = Counter(list(chain.from_iterable(data[col].tolist())))\n",
    "        for i, (w, count) in enumerate(c.most_common(max_features)):\n",
    "            self.word2index[w] = i\n",
    "        return\n",
    "    \n",
    "\n",
    "            \n",
    "            \n",
    "def prepareVocab(name, data, max_features):\n",
    "    vocab = Vocab_topwords(name)\n",
    "    vocab.fit_data(data, name, max_features=max_features)\n",
    "    \n",
    "    print(\"Counted words:\")\n",
    "    print(vocab.name, len(vocab.word2index))\n",
    "    return vocab\n",
    "\n",
    "def indexesFromSentence(vocab, tokens, ngrams, max_len):\n",
    "    num_list = []\n",
    "    for i, item in enumerate(tokens):\n",
    "        if len(num_list) == max_len:\n",
    "            break\n",
    "        elif item in vocab.word2index:\n",
    "            num_list.append(vocab.word2index[item])\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "    if len(num_list) < max_len :\n",
    "        num_list += [0]*(max_len - len(num_list) )\n",
    "        \n",
    "    return num_list\n",
    "\n",
    "def parallelize_dataframe(df, func):\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def get_cat_1(x): return str(x).split('/')[0]\n",
    "def get_cat_2(x): return str(x).split('/')[1] if len(str(x).split('/')) > 1 else -1\n",
    "def get_cat_3(x): return ' '.join(str(x).split('/')[2:]) if len(str(x).split('/')) > 2 else -1\n",
    "\n",
    "def applycat1(df): \n",
    "    df['cat1'] = df['category_name'].progress_apply(get_cat_1)\n",
    "    return df\n",
    "\n",
    "def applycat2(df): \n",
    "    df['cat2'] = df['category_name'].progress_apply(get_cat_2)\n",
    "    return df\n",
    "\n",
    "def applycat3(df): \n",
    "    df['cat3'] = df['category_name'].progress_apply(get_cat_3)\n",
    "    return df\n",
    "\n",
    "def norm3grams(s): return _normalize_and_ngrams(s, 3)\n",
    "\n",
    "def applyname(series): return series.progress_apply(norm3grams)\n",
    "\n",
    "def index2sent1(x, name_vocab): return indexesFromSentence(name_vocab, x, 3, 10)\n",
    "\n",
    "def name2index(series): return series.progress_apply(lambda x: index2sent1(x, name_vocab))\n",
    "\n",
    "def norm2grams(s): return _normalize_and_ngrams(s, 1)\n",
    "\n",
    "def applydesc(series):return series.progress_apply(norm2grams)\n",
    "\n",
    "def index2sent2(x, desc_vocab): return indexesFromSentence(desc_vocab, x, 1, 50)\n",
    "\n",
    "def desc2index(series): return series.progress_apply(lambda x: index2sent2(x, desc_vocab))\n",
    "\n",
    "def read_data(in_path, out_path):\n",
    "    if False and os.path.exists(os.path.join(out_path, 'train_2.pkl')) and os.path.exists(os.path.join(out_path, 'test_2.pkl')):\n",
    "        train_data = pd.read_pickle(os.path.join(out_path, 'train_2.pkl'))\n",
    "        test_data  = pd.read_pickle(os.path.join(out_path, 'test_2.pkl'))\n",
    "        \n",
    "        return train_data, test_data\n",
    "    \n",
    "    else:\n",
    "        train_data = pd.read_table(os.path.join(in_path, 'train.tsv'))\n",
    "        test_data  = pd.read_table(os.path.join(in_path, 'test.tsv'))\n",
    "    \n",
    "        train_rows = len(train_data)\n",
    "        data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "        \n",
    "        data['name'] = data['name'].astype(str)\n",
    "        data['item_description'] = data['item_description'].astype(str)\n",
    "        \n",
    "        #ddata = dd.from_pandas(data, 4)\n",
    "\n",
    "        \n",
    "        data = applycat1(data)\n",
    "        data = applycat2(data)\n",
    "        data = applycat3(data)\n",
    "        data.fillna(-1, inplace=True)\n",
    "        cat_cols = ['category_name', 'brand_name', 'item_condition_id', 'cat1', 'cat2', 'cat3']\n",
    "        print(\"Label enoding categoricals\")\n",
    "        for col in cat_cols:\n",
    "            data[col] = LabelEncoder().fit_transform(data[col].astype(str)).astype(np.int32)\n",
    "            \n",
    "        print(\"Tokenizing text columns\")\n",
    "        data['name'] = parallelize_dataframe(data['name'], applyname)\n",
    "        print(\"Preparing vocabs\")\n",
    "        global name_vocab\n",
    "        name_vocab = prepareVocab('name', data[['name']], 50000)\n",
    "        data['name'] = name2index(data['name'])\n",
    "        del name_vocab\n",
    "        \n",
    "        print(\"Transforming text to sequences\")\n",
    "        data['item_description'] = parallelize_dataframe(data['item_description'], applydesc)\n",
    "        global desc_vocab\n",
    "        desc_vocab = prepareVocab('item_description', data[['item_description']], 100000)\n",
    "        data['item_description'] = desc2index(data['item_description'])\n",
    "        del desc_vocab\n",
    "        \n",
    "        train_data = data.iloc[: train_rows, :]\n",
    "        train_data = train_data.loc[(train_data.price >= 1) & (train_data.price <= 2100), :].reset_index(drop=True)\n",
    "        test_data  = data.iloc[train_rows: , :].reset_index(drop=True)\n",
    "        \n",
    "        del train_data['test_id']\n",
    "        del test_data['train_id']\n",
    "        del data\n",
    "        gc.collect()\n",
    "        print(\"Writing out new pickles dataframes\")\n",
    "        train_data.to_pickle(os.path.join(out_path, 'train_2.pkl'))\n",
    "        test_data.to_pickle(os.path.join(out_path, 'test_2.pkl'))\n",
    "        \n",
    "        return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "c2e58e31-d868-43b6-bd09-946767c39010",
    "_uuid": "212d78f6a1cd4ba08c63f561508fcd165f1c01fa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2175894/2175894 [00:02<00:00, 826506.36it/s]\n",
      "100%|██████████| 2175894/2175894 [00:03<00:00, 630439.43it/s]\n",
      "100%|██████████| 2175894/2175894 [00:03<00:00, 580924.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label enoding categoricals\n",
      "Tokenizing text columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72530/72530 [00:03<00:00, 23790.67it/s]\n",
      "100%|██████████| 72530/72530 [00:03<00:00, 23150.42it/s]\n",
      "100%|██████████| 72530/72530 [00:03<00:00, 22581.22it/s]\n",
      "100%|██████████| 72530/72530 [00:03<00:00, 22843.36it/s]\n",
      "100%|██████████| 72530/72530 [00:03<00:00, 22968.39it/s]\n",
      "100%|██████████| 72530/72530 [00:03<00:00, 21695.02it/s]\n",
      "100%|██████████| 72530/72530 [00:03<00:00, 21988.37it/s]\n",
      "100%|██████████| 72530/72530 [00:03<00:00, 21651.73it/s]\n",
      "100%|██████████| 72530/72530 [00:03<00:00, 23512.84it/s]\n",
      " 97%|█████████▋| 70605/72530 [00:03<00:00, 20270.24it/s]\n",
      "100%|██████████| 72530/72530 [00:03<00:00, 22421.63it/s]\n",
      "100%|██████████| 72530/72530 [00:03<00:00, 21427.91it/s]\n",
      "100%|██████████| 72530/72530 [00:03<00:00, 22181.12it/s]\n",
      "100%|██████████| 72530/72530 [00:03<00:00, 20776.13it/s]\n",
      "100%|██████████| 72530/72530 [00:03<00:00, 22882.67it/s]\n",
      "100%|██████████| 72530/72530 [00:03<00:00, 21452.73it/s]\n",
      "100%|██████████| 72530/72530 [00:02<00:00, 30976.15it/s]\n",
      "100%|██████████| 72530/72530 [00:02<00:00, 32241.13it/s]\n",
      "100%|██████████| 72530/72530 [00:02<00:00, 34560.18it/s]\n",
      " 95%|█████████▌| 68970/72530 [00:01<00:00, 38423.82it/s]\n",
      "100%|██████████| 72530/72530 [00:02<00:00, 35388.39it/s]\n",
      "100%|██████████| 72530/72530 [00:02<00:00, 29916.65it/s]\n",
      "100%|██████████| 72530/72530 [00:02<00:00, 34545.73it/s]\n",
      " 92%|█████████▏| 67011/72529 [00:02<00:00, 37199.69it/s]\n",
      "100%|██████████| 72529/72529 [00:01<00:00, 36384.34it/s]\n",
      " 51%|█████     | 36876/72529 [00:01<00:00, 37181.69it/s]\n",
      "100%|██████████| 72529/72529 [00:02<00:00, 34822.65it/s]\n",
      " 96%|█████████▌| 69501/72529 [00:02<00:00, 39555.73it/s]\n",
      " 78%|███████▊  | 56361/72529 [00:01<00:00, 38609.64it/s]\n",
      "100%|██████████| 72529/72529 [00:01<00:00, 36439.87it/s]\n",
      "100%|██████████| 72529/72529 [00:02<00:00, 34972.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing vocabs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 13379/2175894 [00:00<00:16, 133785.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted words:\n",
      "name 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2175894/2175894 [00:13<00:00, 164640.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming text to sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72530/72530 [00:07<00:00, 10005.93it/s]\n",
      "100%|██████████| 72530/72530 [00:07<00:00, 10209.21it/s]\n",
      "100%|██████████| 72530/72530 [00:07<00:00, 9986.49it/s]]\n",
      "100%|██████████| 72530/72530 [00:07<00:00, 9962.38it/s] \n",
      "100%|██████████| 72530/72530 [00:07<00:00, 9848.44it/s] \n",
      "100%|██████████| 72530/72530 [00:07<00:00, 10017.17it/s]\n",
      "100%|██████████| 72530/72530 [00:07<00:00, 9839.57it/s] \n",
      "100%|██████████| 72530/72530 [00:07<00:00, 9630.90it/s]]\n",
      "100%|██████████| 72530/72530 [00:07<00:00, 9715.35it/s] \n",
      "100%|██████████| 72530/72530 [00:07<00:00, 9990.75it/s]\n",
      "100%|██████████| 72530/72530 [00:07<00:00, 9598.46it/s] \n",
      "100%|██████████| 72530/72530 [00:07<00:00, 9495.60it/s]\n",
      "100%|██████████| 72530/72530 [00:07<00:00, 9717.20it/s]\n",
      "100%|██████████| 72530/72530 [00:07<00:00, 9698.85it/s]\n",
      "  2%|▏         | 1564/72530 [00:00<00:08, 8144.62it/s]]\n",
      "100%|██████████| 72530/72530 [00:07<00:00, 9092.62it/s]\n",
      "100%|██████████| 72530/72530 [00:04<00:00, 14620.21it/s]\n",
      "100%|██████████| 72530/72530 [00:04<00:00, 15081.42it/s]\n",
      "100%|██████████| 72530/72530 [00:04<00:00, 15000.80it/s]\n",
      "100%|██████████| 72530/72530 [00:05<00:00, 12204.25it/s]\n",
      "100%|██████████| 72530/72530 [00:05<00:00, 13623.11it/s]\n",
      "100%|██████████| 72530/72530 [00:06<00:00, 11864.24it/s]\n",
      "100%|██████████| 72529/72529 [00:05<00:00, 12860.75it/s]\n",
      "100%|██████████| 72530/72530 [00:06<00:00, 11757.69it/s]\n",
      "100%|██████████| 72530/72530 [00:06<00:00, 11793.46it/s]\n",
      "100%|██████████| 72529/72529 [00:06<00:00, 11919.93it/s]\n",
      "100%|██████████| 72529/72529 [00:05<00:00, 13352.12it/s]\n",
      "100%|██████████| 72529/72529 [00:05<00:00, 13125.66it/s]\n",
      "100%|██████████| 72529/72529 [00:05<00:00, 13518.38it/s]\n",
      " 99%|█████████▊| 71549/72529 [00:05<00:00, 17112.91it/s]\n",
      "  0%|          | 8861/2175894 [00:00<00:24, 88607.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted words:\n",
      "item_description 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2175894/2175894 [00:20<00:00, 106609.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing out new pickles dataframes\n",
      "CPU times: user 1min 50s, sys: 5.12 s, total: 1min 55s\n",
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data, test_data = read_data(\"../input\", \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "39a10095-f0e9-445e-a034-7cb5f8bfdd35",
    "_uuid": "cee4601fe1f9f5b34078a47615bea665a4e8ade8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1481661, 11) (693359, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>item_description</th>\n",
       "      <th>name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>train_id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>830</td>\n",
       "      <td>2</td>\n",
       "      <td>[3, 32, 35, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[4084, 9421, 10256, 125, 16, 3, 55, 20981, 326...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>103</td>\n",
       "      <td>774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3890</td>\n",
       "      <td>87</td>\n",
       "      <td>2</td>\n",
       "      <td>[2820, 15, 8, 133, 26, 800, 2, 29, 2, 2633, 54...</td>\n",
       "      <td>[12669, 34664, 2087, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4589</td>\n",
       "      <td>1278</td>\n",
       "      <td>0</td>\n",
       "      <td>[445, 48, 4458, 2, 190, 872, 978, 53, 1650, 23...</td>\n",
       "      <td>[208, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9</td>\n",
       "      <td>104</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>504</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 30, 134, 6481, 127, 10, 965, 1378, 94, 208...</td>\n",
       "      <td>[119, 2139, 44620, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1205</td>\n",
       "      <td>0</td>\n",
       "      <td>[647, 5978, 2, 1564, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[6132, 46, 1122, 145, 9686, 1480, 0, 0, 0, 0]</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9</td>\n",
       "      <td>59</td>\n",
       "      <td>543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   brand_name  category_name  item_condition_id  \\\n",
       "0           2            830                  2   \n",
       "1        3890             87                  2   \n",
       "2        4589           1278                  0   \n",
       "3           2            504                  0   \n",
       "4           2           1205                  0   \n",
       "\n",
       "                                    item_description  \\\n",
       "0  [3, 32, 35, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "1  [2820, 15, 8, 133, 26, 800, 2, 29, 2, 2633, 54...   \n",
       "2  [445, 48, 4458, 2, 190, 872, 978, 53, 1650, 23...   \n",
       "3  [0, 30, 134, 6481, 127, 10, 965, 1378, 94, 208...   \n",
       "4  [647, 5978, 2, 1564, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "\n",
       "                                                name  price  shipping  \\\n",
       "0  [4084, 9421, 10256, 125, 16, 3, 55, 20981, 326...   10.0         1   \n",
       "1          [12669, 34664, 2087, 0, 0, 0, 0, 0, 0, 0]   52.0         0   \n",
       "2                   [208, 0, 0, 0, 0, 0, 0, 0, 0, 0]   10.0         1   \n",
       "3            [119, 2139, 44620, 0, 0, 0, 0, 0, 0, 0]   35.0         1   \n",
       "4      [6132, 46, 1122, 145, 9686, 1480, 0, 0, 0, 0]   44.0         0   \n",
       "\n",
       "   train_id  cat1  cat2  cat3  \n",
       "0       0.0     5   103   774  \n",
       "1       1.0     1    31   216  \n",
       "2       2.0     9   104    98  \n",
       "3       3.0     3    56   411  \n",
       "4       4.0     9    59   543  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvlist = list(KFold(n_splits=10).split(train_data, train_data.price))\n",
    "print(train_data.shape, test_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "2cf1463d-35eb-4354-a8f8-5022d684ef69",
    "_uuid": "9f2ed593fa4885bb1444475468aece83e08f0f65"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "class EM_NNRegressor(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, embed_cols=None, dense_cols=None, embed_dims=None, \n",
    "                 text_embed_cols=None, text_embed_seq_lens=None, \n",
    "                 text_embed_dims=None,\n",
    "                 num_layers=2, multiprocess=False,\n",
    "                layer_activations=None, layer_dims=None,layer_dropouts=None, epochs=20, batchsize=32,\n",
    "                optimizer_kwargs=None, val_size=0.1, verbose=1, seed=1):\n",
    "        \n",
    "        self.embed_cols = embed_cols\n",
    "        self.dense_cols = dense_cols\n",
    "        self.embed_dims = embed_dims\n",
    "        self.text_embed_cols = text_embed_cols\n",
    "        self.text_embed_dims = text_embed_dims\n",
    "        #self.text_embed_tokenizers = text_embed_tokenizers\n",
    "        self.text_embed_seq_lens = text_embed_seq_lens\n",
    "        self.dense_dims = None\n",
    "        self.num_layers = num_layers\n",
    "        self.layer_dims = layer_dims\n",
    "        self.layer_activations = layer_activations\n",
    "        self.layer_dropouts = layer_dropouts\n",
    "        self.epochs = epochs\n",
    "        self.batchsize = batchsize\n",
    "        self.optimizer_kwargs = optimizer_kwargs\n",
    "        self.val_size = val_size\n",
    "        self.verbose = verbose\n",
    "        self.multiprocess = multiprocess\n",
    "        self.seed = seed\n",
    "        self.model = None\n",
    "        if self.dense_cols:\n",
    "            self.dense_dims = len(self.dense_cols)\n",
    "            \n",
    "    def _splitX(self, X):\n",
    "        X_splits = []\n",
    "        \n",
    "        if self.embed_cols:\n",
    "            for col in self.embed_cols :\n",
    "                X_splits.append(np.asarray(X[col]))\n",
    "                \n",
    "        if self.text_embed_cols:\n",
    "            for i, col in enumerate(self.text_embed_cols):\n",
    "                X_splits.append(np.asarray([*X[col].values]))\n",
    "                \n",
    "        if self.dense_cols:\n",
    "            X_splits.append(X[self.dense_cols].values.reshape(X.shape[0], -1))\n",
    "            \n",
    "        return X_splits\n",
    "    \n",
    "    \n",
    "    def _build_model(self):\n",
    "        np.random.seed(786)\n",
    "        model_inputs = []\n",
    "        model_layers = []\n",
    "        \n",
    "        if self.embed_cols:\n",
    "            for col, dim in zip(self.embed_cols, self.embed_dims):\n",
    "                x1 = Input( shape=(1,), name=col)\n",
    "                model_inputs.append(x1)\n",
    "                x1 = Embedding(input_dim=dim[0], output_dim=dim[1],)(x1)\n",
    "                #x1 = Dropout(0.1)(x1)\n",
    "                x1 = Reshape(target_shape=(dim[1],))(x1)\n",
    "                model_layers.append(x1)\n",
    "                \n",
    "        if self.text_embed_cols:\n",
    "            for col, dim, seq_len in zip(self.text_embed_cols, \n",
    "                                                self.text_embed_dims, \n",
    "                                                self.text_embed_seq_lens):\n",
    "                x3 = Input( shape=(seq_len,))\n",
    "                model_inputs.append(x3)\n",
    "                x3 = Embedding(input_dim=dim[0], output_dim=dim[1], input_length=seq_len,)(x3)\n",
    "                #x3 = Conv1D(16, return_sequences=True)(x3)\n",
    "                x3 = GlobalAveragePooling1D()(x3)\n",
    "                x3 = Reshape(target_shape=(dim[1],))(x3)\n",
    "                model_layers.append(x3)\n",
    "                \n",
    "        if self.dense_cols:\n",
    "            x2 = Input( shape=(self.dense_dims, ), name='dense_cols')\n",
    "            model_inputs.append(x2)\n",
    "            model_layers.append(x2)\n",
    "        print(model_layers)\n",
    "        x = concatenate(model_layers)\n",
    "        \n",
    "        if self.num_layers > 0:\n",
    "            for dim, drops in zip(self.layer_dims, self.layer_dropouts):\n",
    "                x = BatchNormalization()(x)\n",
    "                x = Dropout(rate=drops, seed=self.seed)(x)\n",
    "                x = Dense(dim, kernel_initializer='he_normal')(x)\n",
    "                #x = Dense(dim, activation='selu', kernel_initializer='he_normal')(x)\n",
    "                x = LeakyReLU()(x)\n",
    "        \n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.05, seed=self.seed)(x)\n",
    "        output = Dense(1, activation='linear', kernel_initializer='he_normal')(x)\n",
    "        \n",
    "        model = Model(inputs=model_inputs, outputs=output)\n",
    "        #print(model.summary())\n",
    "        adam = RMSprop(lr=0.001, decay=0.001)\n",
    "        #adam = Adam(lr=0.001, decay=1e-4)\n",
    "        model.compile(optimizer=adam, loss='mean_squared_error')\n",
    "        \n",
    "        return model \n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model = self._build_model()\n",
    "        if self.val_size > 0:\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.val_size, random_state=self.seed)\n",
    "            print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "            \n",
    "            callbacks= [ModelCheckpoint(\"embed_NN_\"+str(self.seed)+\".check\", save_best_only=True, verbose=1)]\n",
    "            if self.multiprocess == False:\n",
    "                self.model.fit(self._splitX(X_train), y_train, batch_size=self.batchsize, epochs=self.epochs,\n",
    "                               verbose=self.verbose,\n",
    "                              validation_data=(self._splitX(X_val), y_val), shuffle=True,\n",
    "                              callbacks=callbacks)\n",
    "            else:\n",
    "                X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.val_size, random_state=1)\n",
    "\n",
    "        else:\n",
    "            self.model.fit(self._splitX(X), y, batch_size=self.batchsize, epochs=self.epochs,\n",
    "               verbose=self.verbose, shuffle=True,)\n",
    "\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        \n",
    "        if self.model:\n",
    "            self.model = load_model(\"embed_NN_\"+str(self.seed)+\".check\")\n",
    "            y_hat = self.model.predict(self._splitX(X))\n",
    "        else:\n",
    "            raise ValueError(\"Model not fit yet\")\n",
    "            \n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet = EM_NNRegressor(embed_cols=['brand_name','category_name','item_condition_id', 'cat1', 'cat2', 'cat3'], \n",
    "                      embed_dims=[(6000, 40),(1500, 30), (5,4), (15,4), (120, 10), (900, 15)],\n",
    "                      text_embed_cols=['name', 'item_description'],\n",
    "                      text_embed_dims=[(50000, 70), (100000, 70)],\n",
    "                      text_embed_seq_lens =[10, 50], \n",
    "                      dense_cols=['shipping'],\n",
    "                      epochs=6,\n",
    "                      batchsize=2048,\n",
    "                      num_layers = 1,\n",
    "                      layer_dropouts=[0.1],\n",
    "                      layer_dims=[200],\n",
    "                      val_size=0.05\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_workers = multiprocessing.cpu_count()\n",
    "#batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_25/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_26/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_27/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_28/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_29/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_30/Reshape:0' shape=(?, 15) dtype=float32>, <tf.Tensor 'reshape_31/Reshape:0' shape=(?, 70) dtype=float32>, <tf.Tensor 'reshape_32/Reshape:0' shape=(?, 70) dtype=float32>, <tf.Tensor 'dense_cols_4:0' shape=(?, 1) dtype=float32>]\n",
      "(1407577, 11) (74084, 11) (1407577,) (74084,)\n",
      "Train on 1407577 samples, validate on 74084 samples\n",
      "Epoch 1/6\n",
      "1406976/1407577 [============================>.] - ETA: 0s - loss: 0.9423Epoch 00001: val_loss improved from inf to 0.23859, saving model to embed_NN_1.check\n",
      "1407577/1407577 [==============================] - 13s 9us/step - loss: 0.9420 - val_loss: 0.2386\n",
      "Epoch 2/6\n",
      "1406976/1407577 [============================>.] - ETA: 0s - loss: 0.2202Epoch 00002: val_loss improved from 0.23859 to 0.19292, saving model to embed_NN_1.check\n",
      "1407577/1407577 [==============================] - 12s 8us/step - loss: 0.2202 - val_loss: 0.1929\n",
      "Epoch 3/6\n",
      "1402880/1407577 [============================>.] - ETA: 0s - loss: 0.1916Epoch 00003: val_loss improved from 0.19292 to 0.18334, saving model to embed_NN_1.check\n",
      "1407577/1407577 [==============================] - 12s 8us/step - loss: 0.1916 - val_loss: 0.1833\n",
      "Epoch 4/6\n",
      "1406976/1407577 [============================>.] - ETA: 0s - loss: 0.1787Epoch 00004: val_loss improved from 0.18334 to 0.18094, saving model to embed_NN_1.check\n",
      "1407577/1407577 [==============================] - 12s 8us/step - loss: 0.1787 - val_loss: 0.1809\n",
      "Epoch 5/6\n",
      "1404928/1407577 [============================>.] - ETA: 0s - loss: 0.1701Epoch 00005: val_loss improved from 0.18094 to 0.17535, saving model to embed_NN_1.check\n",
      "1407577/1407577 [==============================] - 12s 8us/step - loss: 0.1701 - val_loss: 0.1753\n",
      "Epoch 6/6\n",
      "1406976/1407577 [============================>.] - ETA: 0s - loss: 0.1640Epoch 00006: val_loss did not improve\n",
      "1407577/1407577 [==============================] - 12s 8us/step - loss: 0.1641 - val_loss: 0.1814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EM_NNRegressor(batchsize=2048, dense_cols=['shipping'],\n",
       "        embed_cols=['brand_name', 'category_name', 'item_condition_id', 'cat1', 'cat2', 'cat3'],\n",
       "        embed_dims=[(6000, 40), (1500, 30), (5, 4), (15, 4), (120, 10), (900, 15)],\n",
       "        epochs=6, layer_activations=None, layer_dims=[200],\n",
       "        layer_dropouts=[0.1], multiprocess=False, num_layers=1,\n",
       "        optimizer_kwargs=None, seed=1,\n",
       "        text_embed_cols=['name', 'item_description'],\n",
       "        text_embed_dims=[(50000, 70), (100000, 70)],\n",
       "        text_embed_seq_lens=[10, 50], val_size=0.05, verbose=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scores = cross_val_score(nnet, train_data, np.log1p(train_data.price), scoring=rmse_sklearn, cv=cvlist, verbose =10)\n",
    "#print(scores, np.mean(scores))\n",
    "#for seed in range(1):\n",
    "nnet.set_params(seed=1).fit(train_data, np.log1p(train_data.price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_preds = nnet.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "brand_name (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "category_name (InputLayer)      (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_condition_id (InputLayer)  (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat1 (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat2 (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat3 (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_31 (Embedding)        (None, 10, 70)       3500000     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_32 (Embedding)        (None, 50, 70)       7000000     input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_25 (Embedding)        (None, 1, 40)        240000      brand_name[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_26 (Embedding)        (None, 1, 30)        45000       category_name[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_27 (Embedding)        (None, 1, 4)         20          item_condition_id[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "embedding_28 (Embedding)        (None, 1, 4)         60          cat1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_29 (Embedding)        (None, 1, 10)        1200        cat2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_30 (Embedding)        (None, 1, 15)        13500       cat3[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 70)           0           embedding_31[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 70)           0           embedding_32[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_25 (Reshape)            (None, 40)           0           embedding_25[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_26 (Reshape)            (None, 30)           0           embedding_26[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_27 (Reshape)            (None, 4)            0           embedding_27[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_28 (Reshape)            (None, 4)            0           embedding_28[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_29 (Reshape)            (None, 10)           0           embedding_29[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_30 (Reshape)            (None, 15)           0           embedding_30[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_31 (Reshape)            (None, 70)           0           global_average_pooling1d_7[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "reshape_32 (Reshape)            (None, 70)           0           global_average_pooling1d_8[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_cols (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 244)          0           reshape_25[0][0]                 \n",
      "                                                                 reshape_26[0][0]                 \n",
      "                                                                 reshape_27[0][0]                 \n",
      "                                                                 reshape_28[0][0]                 \n",
      "                                                                 reshape_29[0][0]                 \n",
      "                                                                 reshape_30[0][0]                 \n",
      "                                                                 reshape_31[0][0]                 \n",
      "                                                                 reshape_32[0][0]                 \n",
      "                                                                 dense_cols[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 244)          976         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 244)          0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 200)          49000       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 200)          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 200)          800         leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 200)          0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            201         dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 10,850,757\n",
      "Trainable params: 10,849,869\n",
      "Non-trainable params: 888\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nnet.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeds(data, col, max_features, max_len, embed_dim, model, layer_num):\n",
    "    embed_model = Sequential()\n",
    "    embed_model.add(Embedding(max_features, embed_dim, input_length=max_len, weights=model.model.layers[layer_num].get_weights()))\n",
    "    embed_model.add(GlobalAveragePooling1D())\n",
    "    #name_embed_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    embeddings = embed_model.predict(np.asarray([*data[col]]), batch_size=2500)\n",
    "    return pd.DataFrame(embeddings, columns=[col+'_'+str(i) for i in range(embeddings.shape[1])])\n",
    "\n",
    "def get_embeds2(data, col, max_features, max_len, embed_dim, model, layer_num):\n",
    "    embed_model = Sequential()\n",
    "    embed_model.add(Embedding(max_features, embed_dim, input_length=max_len, weights=model.model.layers[layer_num].get_weights()))\n",
    "    #name_embed_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    embeddings = embed_model.predict(np.asarray(data[col]), batch_size=2500).reshape(len(data), embed_dim)\n",
    "    print(embeddings.shape)\n",
    "    return pd.DataFrame(embeddings, columns=[col+'_'+str(i) for i in range(embeddings.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1481661, 40)\n",
      "(1481661, 30)\n",
      "(1481661, 4)\n",
      "(1481661, 4)\n",
      "(1481661, 10)\n",
      "(1481661, 15)\n",
      "(1481661, 70) (1481661, 70)\n"
     ]
    }
   ],
   "source": [
    "name_train_df = get_embeds(train_data, 'name', 50000, 10, 70, nnet, 8)\n",
    "desc_train_df = get_embeds(train_data, 'item_description', 100000, 50, 70, nnet, 9)\n",
    "brand_train_df = get_embeds2(train_data, 'brand_name', 6000, 1, 40, nnet, 10)\n",
    "category_train_df = get_embeds2(train_data, 'category_name', 1500, 1, 30, nnet, 11)\n",
    "condition_train_df = get_embeds2(train_data, 'item_condition_id', 5, 1, 4, nnet, 12)\n",
    "cat1_train_df = get_embeds2(train_data, 'cat1', 15, 1, 4, nnet, 13)\n",
    "cat2_train_df = get_embeds2(train_data, 'cat2', 120, 1, 10, nnet, 14)\n",
    "cat3_train_df = get_embeds2(train_data, 'cat3', 900, 1, 15, nnet, 15)\n",
    "print(name_train_df.shape, desc_train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19896"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_df(data, name_df, desc_df, brand_df, category_df, cat1_df, cat2_df, cat3_df, condition_df):\n",
    "    data = data[['shipping', 'price', 'id']]\n",
    "    data = pd.concat([data, name_df, desc_df, brand_df, category_df, \n",
    "                      condition_df, cat1_df, cat2_df, cat3_df], axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['id'] = train_data['train_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shipping</th>\n",
       "      <th>price</th>\n",
       "      <th>id</th>\n",
       "      <th>name_0</th>\n",
       "      <th>name_1</th>\n",
       "      <th>name_2</th>\n",
       "      <th>name_3</th>\n",
       "      <th>name_4</th>\n",
       "      <th>name_5</th>\n",
       "      <th>name_6</th>\n",
       "      <th>...</th>\n",
       "      <th>cat3_5</th>\n",
       "      <th>cat3_6</th>\n",
       "      <th>cat3_7</th>\n",
       "      <th>cat3_8</th>\n",
       "      <th>cat3_9</th>\n",
       "      <th>cat3_10</th>\n",
       "      <th>cat3_11</th>\n",
       "      <th>cat3_12</th>\n",
       "      <th>cat3_13</th>\n",
       "      <th>cat3_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005436</td>\n",
       "      <td>-0.002834</td>\n",
       "      <td>-0.000620</td>\n",
       "      <td>-0.014320</td>\n",
       "      <td>-0.018339</td>\n",
       "      <td>0.011182</td>\n",
       "      <td>0.037989</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034057</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>-0.049982</td>\n",
       "      <td>0.015964</td>\n",
       "      <td>-0.012996</td>\n",
       "      <td>0.065730</td>\n",
       "      <td>-0.072058</td>\n",
       "      <td>0.039482</td>\n",
       "      <td>0.013817</td>\n",
       "      <td>0.037860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.003587</td>\n",
       "      <td>-0.006315</td>\n",
       "      <td>0.008053</td>\n",
       "      <td>-0.011680</td>\n",
       "      <td>-0.004148</td>\n",
       "      <td>-0.005934</td>\n",
       "      <td>0.002647</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058369</td>\n",
       "      <td>0.026939</td>\n",
       "      <td>0.016904</td>\n",
       "      <td>0.038118</td>\n",
       "      <td>-0.004726</td>\n",
       "      <td>0.017666</td>\n",
       "      <td>0.049773</td>\n",
       "      <td>0.017442</td>\n",
       "      <td>-0.039765</td>\n",
       "      <td>0.034741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.013486</td>\n",
       "      <td>-0.009317</td>\n",
       "      <td>-0.000395</td>\n",
       "      <td>-0.020650</td>\n",
       "      <td>0.001478</td>\n",
       "      <td>-0.005680</td>\n",
       "      <td>0.010997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005834</td>\n",
       "      <td>-0.019087</td>\n",
       "      <td>-0.030805</td>\n",
       "      <td>0.063070</td>\n",
       "      <td>0.007766</td>\n",
       "      <td>-0.035301</td>\n",
       "      <td>0.014713</td>\n",
       "      <td>-0.014809</td>\n",
       "      <td>0.047389</td>\n",
       "      <td>-0.007801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.009502</td>\n",
       "      <td>-0.001996</td>\n",
       "      <td>0.005002</td>\n",
       "      <td>-0.016129</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>-0.013970</td>\n",
       "      <td>-0.005269</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020680</td>\n",
       "      <td>0.057065</td>\n",
       "      <td>0.048931</td>\n",
       "      <td>0.014143</td>\n",
       "      <td>-0.013839</td>\n",
       "      <td>-0.005063</td>\n",
       "      <td>-0.016318</td>\n",
       "      <td>0.024021</td>\n",
       "      <td>0.055805</td>\n",
       "      <td>-0.019555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.008966</td>\n",
       "      <td>0.003316</td>\n",
       "      <td>0.001357</td>\n",
       "      <td>-0.021735</td>\n",
       "      <td>-0.001377</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>0.008127</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035260</td>\n",
       "      <td>0.046141</td>\n",
       "      <td>-0.003048</td>\n",
       "      <td>-0.020771</td>\n",
       "      <td>0.009553</td>\n",
       "      <td>0.036631</td>\n",
       "      <td>0.057082</td>\n",
       "      <td>0.017131</td>\n",
       "      <td>-0.029955</td>\n",
       "      <td>-0.056519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 246 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   shipping  price   id    name_0    name_1    name_2    name_3    name_4  \\\n",
       "0         1   10.0  0.0  0.005436 -0.002834 -0.000620 -0.014320 -0.018339   \n",
       "1         0   52.0  1.0 -0.003587 -0.006315  0.008053 -0.011680 -0.004148   \n",
       "2         1   10.0  2.0  0.013486 -0.009317 -0.000395 -0.020650  0.001478   \n",
       "3         1   35.0  3.0  0.009502 -0.001996  0.005002 -0.016129  0.001382   \n",
       "4         0   44.0  4.0 -0.008966  0.003316  0.001357 -0.021735 -0.001377   \n",
       "\n",
       "     name_5    name_6    ...       cat3_5    cat3_6    cat3_7    cat3_8  \\\n",
       "0  0.011182  0.037989    ...    -0.034057  0.011300 -0.049982  0.015964   \n",
       "1 -0.005934  0.002647    ...    -0.058369  0.026939  0.016904  0.038118   \n",
       "2 -0.005680  0.010997    ...     0.005834 -0.019087 -0.030805  0.063070   \n",
       "3 -0.013970 -0.005269    ...    -0.020680  0.057065  0.048931  0.014143   \n",
       "4  0.003187  0.008127    ...    -0.035260  0.046141 -0.003048 -0.020771   \n",
       "\n",
       "     cat3_9   cat3_10   cat3_11   cat3_12   cat3_13   cat3_14  \n",
       "0 -0.012996  0.065730 -0.072058  0.039482  0.013817  0.037860  \n",
       "1 -0.004726  0.017666  0.049773  0.017442 -0.039765  0.034741  \n",
       "2  0.007766 -0.035301  0.014713 -0.014809  0.047389 -0.007801  \n",
       "3 -0.013839 -0.005063 -0.016318  0.024021  0.055805 -0.019555  \n",
       "4  0.009553  0.036631  0.057082  0.017131 -0.029955 -0.056519  \n",
       "\n",
       "[5 rows x 246 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train2 = embed_df(train_data, name_train_df, desc_train_df, brand_train_df, category_train_df,\n",
    "                 cat1_train_df, cat2_train_df, cat3_train_df, condition_train_df)\n",
    "train2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#del train2, train_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[10]\ttraining's rmse: 0.620978\tvalid_1's rmse: 0.625592\n",
      "[20]\ttraining's rmse: 0.557749\tvalid_1's rmse: 0.564743\n",
      "[30]\ttraining's rmse: 0.522137\tvalid_1's rmse: 0.530665\n",
      "[40]\ttraining's rmse: 0.500487\tvalid_1's rmse: 0.510183\n",
      "[50]\ttraining's rmse: 0.485791\tvalid_1's rmse: 0.496141\n",
      "[60]\ttraining's rmse: 0.47564\tvalid_1's rmse: 0.486761\n",
      "[70]\ttraining's rmse: 0.468834\tvalid_1's rmse: 0.480691\n",
      "[80]\ttraining's rmse: 0.463613\tvalid_1's rmse: 0.476041\n",
      "[90]\ttraining's rmse: 0.459809\tvalid_1's rmse: 0.472817\n",
      "[100]\ttraining's rmse: 0.456954\tvalid_1's rmse: 0.470382\n",
      "[110]\ttraining's rmse: 0.454698\tvalid_1's rmse: 0.46848\n",
      "[120]\ttraining's rmse: 0.452728\tvalid_1's rmse: 0.466801\n",
      "[130]\ttraining's rmse: 0.451057\tvalid_1's rmse: 0.465346\n",
      "[140]\ttraining's rmse: 0.449584\tvalid_1's rmse: 0.464085\n",
      "[150]\ttraining's rmse: 0.448289\tvalid_1's rmse: 0.463015\n",
      "[160]\ttraining's rmse: 0.447054\tvalid_1's rmse: 0.461941\n",
      "[170]\ttraining's rmse: 0.445895\tvalid_1's rmse: 0.46098\n",
      "[180]\ttraining's rmse: 0.444834\tvalid_1's rmse: 0.460112\n",
      "[190]\ttraining's rmse: 0.443823\tvalid_1's rmse: 0.45925\n",
      "[200]\ttraining's rmse: 0.442906\tvalid_1's rmse: 0.458501\n",
      "[210]\ttraining's rmse: 0.44208\tvalid_1's rmse: 0.457845\n",
      "[220]\ttraining's rmse: 0.441221\tvalid_1's rmse: 0.457133\n",
      "[230]\ttraining's rmse: 0.440409\tvalid_1's rmse: 0.456496\n",
      "[240]\ttraining's rmse: 0.43966\tvalid_1's rmse: 0.455882\n",
      "[250]\ttraining's rmse: 0.438907\tvalid_1's rmse: 0.455266\n",
      "[260]\ttraining's rmse: 0.438281\tvalid_1's rmse: 0.454733\n",
      "[270]\ttraining's rmse: 0.437643\tvalid_1's rmse: 0.454253\n",
      "[280]\ttraining's rmse: 0.437012\tvalid_1's rmse: 0.453763\n",
      "[290]\ttraining's rmse: 0.436452\tvalid_1's rmse: 0.453323\n",
      "[300]\ttraining's rmse: 0.435852\tvalid_1's rmse: 0.452854\n",
      "[310]\ttraining's rmse: 0.435283\tvalid_1's rmse: 0.452379\n",
      "[320]\ttraining's rmse: 0.434763\tvalid_1's rmse: 0.451961\n",
      "[330]\ttraining's rmse: 0.434272\tvalid_1's rmse: 0.451589\n",
      "[340]\ttraining's rmse: 0.433764\tvalid_1's rmse: 0.451187\n",
      "[350]\ttraining's rmse: 0.433273\tvalid_1's rmse: 0.450819\n",
      "[360]\ttraining's rmse: 0.432768\tvalid_1's rmse: 0.450461\n",
      "[370]\ttraining's rmse: 0.432327\tvalid_1's rmse: 0.450147\n",
      "[380]\ttraining's rmse: 0.431871\tvalid_1's rmse: 0.449788\n",
      "[390]\ttraining's rmse: 0.431424\tvalid_1's rmse: 0.449428\n",
      "[400]\ttraining's rmse: 0.431016\tvalid_1's rmse: 0.449119\n",
      "[410]\ttraining's rmse: 0.430576\tvalid_1's rmse: 0.448778\n",
      "[420]\ttraining's rmse: 0.430161\tvalid_1's rmse: 0.44849\n",
      "[430]\ttraining's rmse: 0.4298\tvalid_1's rmse: 0.448219\n",
      "[440]\ttraining's rmse: 0.429438\tvalid_1's rmse: 0.447962\n",
      "[450]\ttraining's rmse: 0.429103\tvalid_1's rmse: 0.447722\n",
      "[460]\ttraining's rmse: 0.428746\tvalid_1's rmse: 0.447434\n",
      "[470]\ttraining's rmse: 0.428328\tvalid_1's rmse: 0.447126\n",
      "[480]\ttraining's rmse: 0.427973\tvalid_1's rmse: 0.446867\n",
      "[490]\ttraining's rmse: 0.427645\tvalid_1's rmse: 0.446612\n",
      "[500]\ttraining's rmse: 0.427345\tvalid_1's rmse: 0.446412\n",
      "[510]\ttraining's rmse: 0.426996\tvalid_1's rmse: 0.446188\n",
      "[520]\ttraining's rmse: 0.426677\tvalid_1's rmse: 0.445938\n",
      "[530]\ttraining's rmse: 0.426346\tvalid_1's rmse: 0.445682\n",
      "[540]\ttraining's rmse: 0.426023\tvalid_1's rmse: 0.445468\n",
      "[550]\ttraining's rmse: 0.42571\tvalid_1's rmse: 0.445221\n",
      "[560]\ttraining's rmse: 0.425442\tvalid_1's rmse: 0.445031\n",
      "[570]\ttraining's rmse: 0.42518\tvalid_1's rmse: 0.444835\n",
      "[580]\ttraining's rmse: 0.424916\tvalid_1's rmse: 0.444663\n",
      "[590]\ttraining's rmse: 0.424624\tvalid_1's rmse: 0.444451\n",
      "[600]\ttraining's rmse: 0.424321\tvalid_1's rmse: 0.444234\n",
      "[610]\ttraining's rmse: 0.423985\tvalid_1's rmse: 0.444003\n",
      "[620]\ttraining's rmse: 0.42374\tvalid_1's rmse: 0.443828\n",
      "[630]\ttraining's rmse: 0.423454\tvalid_1's rmse: 0.443641\n",
      "[640]\ttraining's rmse: 0.423227\tvalid_1's rmse: 0.443476\n",
      "[650]\ttraining's rmse: 0.422914\tvalid_1's rmse: 0.443243\n",
      "[660]\ttraining's rmse: 0.422662\tvalid_1's rmse: 0.443083\n",
      "[670]\ttraining's rmse: 0.42242\tvalid_1's rmse: 0.442909\n",
      "[680]\ttraining's rmse: 0.422145\tvalid_1's rmse: 0.442715\n",
      "[690]\ttraining's rmse: 0.421883\tvalid_1's rmse: 0.442533\n",
      "[700]\ttraining's rmse: 0.421654\tvalid_1's rmse: 0.442358\n",
      "[710]\ttraining's rmse: 0.421447\tvalid_1's rmse: 0.44219\n",
      "[720]\ttraining's rmse: 0.4212\tvalid_1's rmse: 0.442021\n",
      "[730]\ttraining's rmse: 0.421006\tvalid_1's rmse: 0.441888\n",
      "[740]\ttraining's rmse: 0.420752\tvalid_1's rmse: 0.441718\n",
      "[750]\ttraining's rmse: 0.420553\tvalid_1's rmse: 0.441562\n",
      "[760]\ttraining's rmse: 0.420322\tvalid_1's rmse: 0.441408\n",
      "[770]\ttraining's rmse: 0.420091\tvalid_1's rmse: 0.441252\n",
      "[780]\ttraining's rmse: 0.419899\tvalid_1's rmse: 0.441128\n",
      "[790]\ttraining's rmse: 0.41966\tvalid_1's rmse: 0.440999\n",
      "[800]\ttraining's rmse: 0.41942\tvalid_1's rmse: 0.440812\n",
      "[810]\ttraining's rmse: 0.419189\tvalid_1's rmse: 0.440662\n",
      "[820]\ttraining's rmse: 0.419007\tvalid_1's rmse: 0.440538\n",
      "[830]\ttraining's rmse: 0.418808\tvalid_1's rmse: 0.440383\n",
      "[840]\ttraining's rmse: 0.418621\tvalid_1's rmse: 0.440261\n",
      "[850]\ttraining's rmse: 0.418408\tvalid_1's rmse: 0.440125\n",
      "[860]\ttraining's rmse: 0.418232\tvalid_1's rmse: 0.440016\n",
      "[870]\ttraining's rmse: 0.418034\tvalid_1's rmse: 0.439863\n",
      "[880]\ttraining's rmse: 0.41787\tvalid_1's rmse: 0.439736\n",
      "[890]\ttraining's rmse: 0.417669\tvalid_1's rmse: 0.43961\n",
      "[900]\ttraining's rmse: 0.417432\tvalid_1's rmse: 0.439445\n",
      "[910]\ttraining's rmse: 0.417273\tvalid_1's rmse: 0.439331\n",
      "[920]\ttraining's rmse: 0.417086\tvalid_1's rmse: 0.439221\n",
      "[930]\ttraining's rmse: 0.416938\tvalid_1's rmse: 0.439117\n",
      "[940]\ttraining's rmse: 0.41675\tvalid_1's rmse: 0.438987\n",
      "[950]\ttraining's rmse: 0.416558\tvalid_1's rmse: 0.438864\n",
      "[960]\ttraining's rmse: 0.41638\tvalid_1's rmse: 0.438732\n",
      "[970]\ttraining's rmse: 0.41621\tvalid_1's rmse: 0.438626\n",
      "[980]\ttraining's rmse: 0.416039\tvalid_1's rmse: 0.438528\n",
      "[990]\ttraining's rmse: 0.415864\tvalid_1's rmse: 0.438394\n",
      "[1000]\ttraining's rmse: 0.415653\tvalid_1's rmse: 0.438257\n",
      "[1010]\ttraining's rmse: 0.415472\tvalid_1's rmse: 0.438141\n",
      "[1020]\ttraining's rmse: 0.41528\tvalid_1's rmse: 0.438022\n",
      "[1030]\ttraining's rmse: 0.415121\tvalid_1's rmse: 0.437899\n",
      "[1040]\ttraining's rmse: 0.41497\tvalid_1's rmse: 0.437796\n",
      "[1050]\ttraining's rmse: 0.414823\tvalid_1's rmse: 0.437715\n",
      "[1060]\ttraining's rmse: 0.41464\tvalid_1's rmse: 0.437589\n",
      "[1070]\ttraining's rmse: 0.414505\tvalid_1's rmse: 0.437491\n",
      "[1080]\ttraining's rmse: 0.414333\tvalid_1's rmse: 0.437399\n",
      "[1090]\ttraining's rmse: 0.41415\tvalid_1's rmse: 0.437293\n",
      "[1100]\ttraining's rmse: 0.413961\tvalid_1's rmse: 0.437147\n",
      "[1110]\ttraining's rmse: 0.413799\tvalid_1's rmse: 0.437044\n",
      "[1120]\ttraining's rmse: 0.413637\tvalid_1's rmse: 0.436954\n",
      "[1130]\ttraining's rmse: 0.4135\tvalid_1's rmse: 0.436876\n",
      "[1140]\ttraining's rmse: 0.413353\tvalid_1's rmse: 0.436794\n",
      "[1150]\ttraining's rmse: 0.413236\tvalid_1's rmse: 0.436736\n",
      "[1160]\ttraining's rmse: 0.413083\tvalid_1's rmse: 0.43666\n",
      "[1170]\ttraining's rmse: 0.412917\tvalid_1's rmse: 0.436541\n",
      "[1180]\ttraining's rmse: 0.412787\tvalid_1's rmse: 0.436461\n",
      "[1190]\ttraining's rmse: 0.412628\tvalid_1's rmse: 0.436343\n",
      "[1200]\ttraining's rmse: 0.412492\tvalid_1's rmse: 0.436258\n",
      "[1210]\ttraining's rmse: 0.412348\tvalid_1's rmse: 0.436136\n",
      "[1220]\ttraining's rmse: 0.412194\tvalid_1's rmse: 0.436045\n",
      "[1230]\ttraining's rmse: 0.412048\tvalid_1's rmse: 0.435943\n",
      "[1240]\ttraining's rmse: 0.411935\tvalid_1's rmse: 0.435875\n",
      "[1250]\ttraining's rmse: 0.41181\tvalid_1's rmse: 0.435801\n",
      "[1260]\ttraining's rmse: 0.41167\tvalid_1's rmse: 0.43573\n",
      "[1270]\ttraining's rmse: 0.411516\tvalid_1's rmse: 0.43561\n",
      "[1280]\ttraining's rmse: 0.411386\tvalid_1's rmse: 0.435531\n",
      "[1290]\ttraining's rmse: 0.411245\tvalid_1's rmse: 0.435458\n",
      "[1300]\ttraining's rmse: 0.411119\tvalid_1's rmse: 0.435391\n",
      "[1310]\ttraining's rmse: 0.411011\tvalid_1's rmse: 0.435344\n",
      "[1320]\ttraining's rmse: 0.410857\tvalid_1's rmse: 0.435248\n",
      "[1330]\ttraining's rmse: 0.410744\tvalid_1's rmse: 0.435163\n",
      "[1340]\ttraining's rmse: 0.410632\tvalid_1's rmse: 0.435096\n",
      "[1350]\ttraining's rmse: 0.410502\tvalid_1's rmse: 0.435016\n",
      "[1360]\ttraining's rmse: 0.410368\tvalid_1's rmse: 0.434931\n",
      "[1370]\ttraining's rmse: 0.41021\tvalid_1's rmse: 0.434839\n",
      "[1380]\ttraining's rmse: 0.41012\tvalid_1's rmse: 0.434796\n",
      "[1390]\ttraining's rmse: 0.410007\tvalid_1's rmse: 0.434739\n",
      "[1400]\ttraining's rmse: 0.409878\tvalid_1's rmse: 0.434676\n",
      "[1410]\ttraining's rmse: 0.409751\tvalid_1's rmse: 0.434611\n",
      "[1420]\ttraining's rmse: 0.409625\tvalid_1's rmse: 0.434529\n",
      "[1430]\ttraining's rmse: 0.409512\tvalid_1's rmse: 0.434463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1440]\ttraining's rmse: 0.409402\tvalid_1's rmse: 0.434388\n",
      "[1450]\ttraining's rmse: 0.409261\tvalid_1's rmse: 0.434301\n",
      "[1460]\ttraining's rmse: 0.409168\tvalid_1's rmse: 0.434228\n",
      "[1470]\ttraining's rmse: 0.409028\tvalid_1's rmse: 0.434129\n",
      "[1480]\ttraining's rmse: 0.408907\tvalid_1's rmse: 0.434074\n",
      "[1490]\ttraining's rmse: 0.408804\tvalid_1's rmse: 0.434014\n",
      "[1500]\ttraining's rmse: 0.408684\tvalid_1's rmse: 0.433944\n",
      "[1510]\ttraining's rmse: 0.408578\tvalid_1's rmse: 0.433875\n",
      "[1520]\ttraining's rmse: 0.408461\tvalid_1's rmse: 0.433811\n",
      "[1530]\ttraining's rmse: 0.408352\tvalid_1's rmse: 0.433729\n",
      "[1540]\ttraining's rmse: 0.408249\tvalid_1's rmse: 0.433658\n",
      "[1550]\ttraining's rmse: 0.408102\tvalid_1's rmse: 0.433582\n",
      "[1560]\ttraining's rmse: 0.407978\tvalid_1's rmse: 0.433495\n",
      "[1570]\ttraining's rmse: 0.40788\tvalid_1's rmse: 0.433442\n",
      "[1580]\ttraining's rmse: 0.407767\tvalid_1's rmse: 0.43338\n",
      "[1590]\ttraining's rmse: 0.407678\tvalid_1's rmse: 0.433336\n",
      "[1600]\ttraining's rmse: 0.407576\tvalid_1's rmse: 0.433268\n",
      "[1610]\ttraining's rmse: 0.407465\tvalid_1's rmse: 0.433206\n",
      "[1620]\ttraining's rmse: 0.407335\tvalid_1's rmse: 0.433121\n",
      "[1630]\ttraining's rmse: 0.407217\tvalid_1's rmse: 0.433073\n",
      "[1640]\ttraining's rmse: 0.407122\tvalid_1's rmse: 0.433023\n",
      "[1650]\ttraining's rmse: 0.407041\tvalid_1's rmse: 0.432982\n",
      "[1660]\ttraining's rmse: 0.406922\tvalid_1's rmse: 0.432903\n",
      "[1670]\ttraining's rmse: 0.406809\tvalid_1's rmse: 0.432841\n",
      "[1680]\ttraining's rmse: 0.406689\tvalid_1's rmse: 0.432775\n",
      "[1690]\ttraining's rmse: 0.406586\tvalid_1's rmse: 0.432715\n",
      "[1700]\ttraining's rmse: 0.406503\tvalid_1's rmse: 0.432683\n",
      "[1710]\ttraining's rmse: 0.406384\tvalid_1's rmse: 0.432627\n",
      "[1720]\ttraining's rmse: 0.406283\tvalid_1's rmse: 0.432578\n",
      "[1730]\ttraining's rmse: 0.406196\tvalid_1's rmse: 0.432527\n",
      "[1740]\ttraining's rmse: 0.406097\tvalid_1's rmse: 0.432487\n",
      "[1750]\ttraining's rmse: 0.405995\tvalid_1's rmse: 0.432445\n",
      "[1760]\ttraining's rmse: 0.405904\tvalid_1's rmse: 0.432392\n",
      "[1770]\ttraining's rmse: 0.405818\tvalid_1's rmse: 0.43234\n",
      "[1780]\ttraining's rmse: 0.405733\tvalid_1's rmse: 0.432291\n",
      "[1790]\ttraining's rmse: 0.405616\tvalid_1's rmse: 0.432211\n",
      "[1800]\ttraining's rmse: 0.405513\tvalid_1's rmse: 0.432167\n",
      "[1810]\ttraining's rmse: 0.405394\tvalid_1's rmse: 0.432095\n",
      "[1820]\ttraining's rmse: 0.405283\tvalid_1's rmse: 0.432033\n",
      "[1830]\ttraining's rmse: 0.405177\tvalid_1's rmse: 0.431968\n",
      "[1840]\ttraining's rmse: 0.405055\tvalid_1's rmse: 0.431892\n",
      "[1850]\ttraining's rmse: 0.404992\tvalid_1's rmse: 0.431871\n",
      "[1860]\ttraining's rmse: 0.404886\tvalid_1's rmse: 0.431821\n",
      "[1870]\ttraining's rmse: 0.404779\tvalid_1's rmse: 0.43175\n",
      "[1880]\ttraining's rmse: 0.404689\tvalid_1's rmse: 0.431712\n",
      "[1890]\ttraining's rmse: 0.404592\tvalid_1's rmse: 0.43165\n",
      "[1900]\ttraining's rmse: 0.404472\tvalid_1's rmse: 0.431593\n",
      "[1910]\ttraining's rmse: 0.404397\tvalid_1's rmse: 0.431549\n",
      "[1920]\ttraining's rmse: 0.404308\tvalid_1's rmse: 0.43152\n",
      "[1930]\ttraining's rmse: 0.404212\tvalid_1's rmse: 0.431456\n",
      "[1940]\ttraining's rmse: 0.404117\tvalid_1's rmse: 0.431404\n",
      "[1950]\ttraining's rmse: 0.404032\tvalid_1's rmse: 0.431353\n",
      "[1960]\ttraining's rmse: 0.403959\tvalid_1's rmse: 0.431314\n",
      "[1970]\ttraining's rmse: 0.403872\tvalid_1's rmse: 0.43126\n",
      "[1980]\ttraining's rmse: 0.403797\tvalid_1's rmse: 0.431216\n",
      "[1990]\ttraining's rmse: 0.40372\tvalid_1's rmse: 0.431186\n",
      "[2000]\ttraining's rmse: 0.403634\tvalid_1's rmse: 0.431136\n",
      "[2010]\ttraining's rmse: 0.403541\tvalid_1's rmse: 0.431085\n",
      "[2020]\ttraining's rmse: 0.403465\tvalid_1's rmse: 0.431039\n",
      "[2030]\ttraining's rmse: 0.403384\tvalid_1's rmse: 0.431003\n",
      "[2040]\ttraining's rmse: 0.403279\tvalid_1's rmse: 0.430935\n",
      "[2050]\ttraining's rmse: 0.403202\tvalid_1's rmse: 0.430896\n",
      "[2060]\ttraining's rmse: 0.403106\tvalid_1's rmse: 0.430849\n",
      "[2070]\ttraining's rmse: 0.403015\tvalid_1's rmse: 0.43081\n",
      "[2080]\ttraining's rmse: 0.402923\tvalid_1's rmse: 0.430736\n",
      "[2090]\ttraining's rmse: 0.402832\tvalid_1's rmse: 0.430673\n",
      "[2100]\ttraining's rmse: 0.402731\tvalid_1's rmse: 0.430628\n",
      "[2110]\ttraining's rmse: 0.402642\tvalid_1's rmse: 0.430577\n",
      "[2120]\ttraining's rmse: 0.402545\tvalid_1's rmse: 0.43052\n",
      "[2130]\ttraining's rmse: 0.402465\tvalid_1's rmse: 0.430475\n",
      "[2140]\ttraining's rmse: 0.402385\tvalid_1's rmse: 0.430435\n",
      "[2150]\ttraining's rmse: 0.402317\tvalid_1's rmse: 0.430403\n",
      "[2160]\ttraining's rmse: 0.402242\tvalid_1's rmse: 0.43037\n",
      "[2170]\ttraining's rmse: 0.402172\tvalid_1's rmse: 0.430344\n",
      "[2180]\ttraining's rmse: 0.402101\tvalid_1's rmse: 0.430323\n",
      "[2190]\ttraining's rmse: 0.402024\tvalid_1's rmse: 0.430279\n",
      "[2200]\ttraining's rmse: 0.401929\tvalid_1's rmse: 0.430218\n",
      "[2210]\ttraining's rmse: 0.401854\tvalid_1's rmse: 0.43017\n",
      "[2220]\ttraining's rmse: 0.401782\tvalid_1's rmse: 0.430137\n",
      "[2230]\ttraining's rmse: 0.401708\tvalid_1's rmse: 0.430099\n",
      "[2240]\ttraining's rmse: 0.40164\tvalid_1's rmse: 0.430065\n",
      "[2250]\ttraining's rmse: 0.401575\tvalid_1's rmse: 0.430032\n",
      "[2260]\ttraining's rmse: 0.401491\tvalid_1's rmse: 0.429999\n",
      "[2270]\ttraining's rmse: 0.401383\tvalid_1's rmse: 0.429932\n",
      "[2280]\ttraining's rmse: 0.401305\tvalid_1's rmse: 0.4299\n",
      "[2290]\ttraining's rmse: 0.40125\tvalid_1's rmse: 0.429871\n",
      "[2300]\ttraining's rmse: 0.401163\tvalid_1's rmse: 0.42983\n",
      "[2310]\ttraining's rmse: 0.401085\tvalid_1's rmse: 0.429789\n",
      "[2320]\ttraining's rmse: 0.401004\tvalid_1's rmse: 0.429754\n",
      "[2330]\ttraining's rmse: 0.400938\tvalid_1's rmse: 0.429739\n",
      "[2340]\ttraining's rmse: 0.400858\tvalid_1's rmse: 0.429693\n",
      "[2350]\ttraining's rmse: 0.400795\tvalid_1's rmse: 0.42966\n",
      "[2360]\ttraining's rmse: 0.400724\tvalid_1's rmse: 0.429619\n",
      "[2370]\ttraining's rmse: 0.400641\tvalid_1's rmse: 0.429574\n",
      "[2380]\ttraining's rmse: 0.400577\tvalid_1's rmse: 0.429552\n",
      "[2390]\ttraining's rmse: 0.400529\tvalid_1's rmse: 0.429537\n",
      "[2400]\ttraining's rmse: 0.400449\tvalid_1's rmse: 0.429509\n",
      "[2410]\ttraining's rmse: 0.400366\tvalid_1's rmse: 0.429467\n",
      "[2420]\ttraining's rmse: 0.400296\tvalid_1's rmse: 0.429426\n",
      "[2430]\ttraining's rmse: 0.400214\tvalid_1's rmse: 0.429391\n",
      "[2440]\ttraining's rmse: 0.400151\tvalid_1's rmse: 0.429358\n",
      "[2450]\ttraining's rmse: 0.400072\tvalid_1's rmse: 0.429319\n",
      "[2460]\ttraining's rmse: 0.400007\tvalid_1's rmse: 0.429286\n",
      "[2470]\ttraining's rmse: 0.399935\tvalid_1's rmse: 0.429249\n",
      "[2480]\ttraining's rmse: 0.399862\tvalid_1's rmse: 0.429212\n",
      "[2490]\ttraining's rmse: 0.399776\tvalid_1's rmse: 0.429181\n",
      "[2500]\ttraining's rmse: 0.399705\tvalid_1's rmse: 0.42914\n",
      "[2510]\ttraining's rmse: 0.399614\tvalid_1's rmse: 0.429087\n",
      "[2520]\ttraining's rmse: 0.399538\tvalid_1's rmse: 0.429049\n",
      "[2530]\ttraining's rmse: 0.39947\tvalid_1's rmse: 0.429019\n",
      "[2540]\ttraining's rmse: 0.399394\tvalid_1's rmse: 0.428987\n",
      "[2550]\ttraining's rmse: 0.399321\tvalid_1's rmse: 0.42896\n",
      "[2560]\ttraining's rmse: 0.399251\tvalid_1's rmse: 0.428929\n",
      "[2570]\ttraining's rmse: 0.399181\tvalid_1's rmse: 0.428908\n",
      "[2580]\ttraining's rmse: 0.399101\tvalid_1's rmse: 0.428874\n",
      "[2590]\ttraining's rmse: 0.399037\tvalid_1's rmse: 0.428839\n",
      "[2600]\ttraining's rmse: 0.398978\tvalid_1's rmse: 0.428813\n",
      "[2610]\ttraining's rmse: 0.39891\tvalid_1's rmse: 0.428776\n",
      "[2620]\ttraining's rmse: 0.398849\tvalid_1's rmse: 0.428754\n",
      "[2630]\ttraining's rmse: 0.398751\tvalid_1's rmse: 0.428693\n",
      "[2640]\ttraining's rmse: 0.398673\tvalid_1's rmse: 0.428655\n",
      "[2650]\ttraining's rmse: 0.398602\tvalid_1's rmse: 0.428622\n",
      "[2660]\ttraining's rmse: 0.398536\tvalid_1's rmse: 0.428597\n",
      "[2670]\ttraining's rmse: 0.39847\tvalid_1's rmse: 0.428564\n",
      "[2680]\ttraining's rmse: 0.398366\tvalid_1's rmse: 0.428513\n",
      "[2690]\ttraining's rmse: 0.39829\tvalid_1's rmse: 0.428467\n",
      "[2700]\ttraining's rmse: 0.398218\tvalid_1's rmse: 0.42844\n",
      "[2710]\ttraining's rmse: 0.398166\tvalid_1's rmse: 0.428418\n",
      "[2720]\ttraining's rmse: 0.398105\tvalid_1's rmse: 0.428392\n",
      "[2730]\ttraining's rmse: 0.398049\tvalid_1's rmse: 0.428369\n",
      "[2740]\ttraining's rmse: 0.397984\tvalid_1's rmse: 0.428332\n",
      "[2750]\ttraining's rmse: 0.397918\tvalid_1's rmse: 0.428309\n",
      "[2760]\ttraining's rmse: 0.397854\tvalid_1's rmse: 0.428272\n",
      "[2770]\ttraining's rmse: 0.397802\tvalid_1's rmse: 0.428243\n",
      "[2780]\ttraining's rmse: 0.397746\tvalid_1's rmse: 0.428228\n",
      "[2790]\ttraining's rmse: 0.397683\tvalid_1's rmse: 0.428193\n",
      "[2800]\ttraining's rmse: 0.397613\tvalid_1's rmse: 0.428155\n",
      "[2810]\ttraining's rmse: 0.397541\tvalid_1's rmse: 0.428121\n",
      "[2820]\ttraining's rmse: 0.397467\tvalid_1's rmse: 0.428092\n",
      "[2830]\ttraining's rmse: 0.397392\tvalid_1's rmse: 0.428039\n",
      "[2840]\ttraining's rmse: 0.397323\tvalid_1's rmse: 0.428016\n",
      "[2850]\ttraining's rmse: 0.397258\tvalid_1's rmse: 0.428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2860]\ttraining's rmse: 0.397181\tvalid_1's rmse: 0.427951\n",
      "[2870]\ttraining's rmse: 0.397102\tvalid_1's rmse: 0.427907\n",
      "[2880]\ttraining's rmse: 0.397043\tvalid_1's rmse: 0.427874\n",
      "[2890]\ttraining's rmse: 0.396976\tvalid_1's rmse: 0.427847\n",
      "[2900]\ttraining's rmse: 0.396912\tvalid_1's rmse: 0.427826\n",
      "[2910]\ttraining's rmse: 0.396854\tvalid_1's rmse: 0.427791\n",
      "[2920]\ttraining's rmse: 0.396785\tvalid_1's rmse: 0.427762\n",
      "[2930]\ttraining's rmse: 0.396724\tvalid_1's rmse: 0.427717\n",
      "[2940]\ttraining's rmse: 0.396664\tvalid_1's rmse: 0.427688\n",
      "[2950]\ttraining's rmse: 0.396604\tvalid_1's rmse: 0.427661\n",
      "[2960]\ttraining's rmse: 0.396541\tvalid_1's rmse: 0.427633\n",
      "[2970]\ttraining's rmse: 0.39647\tvalid_1's rmse: 0.427592\n",
      "[2980]\ttraining's rmse: 0.396401\tvalid_1's rmse: 0.42755\n",
      "[2990]\ttraining's rmse: 0.396342\tvalid_1's rmse: 0.42753\n",
      "[3000]\ttraining's rmse: 0.396282\tvalid_1's rmse: 0.42751\n",
      "[3010]\ttraining's rmse: 0.39622\tvalid_1's rmse: 0.427476\n",
      "[3020]\ttraining's rmse: 0.396166\tvalid_1's rmse: 0.427456\n",
      "[3030]\ttraining's rmse: 0.396116\tvalid_1's rmse: 0.427443\n",
      "[3040]\ttraining's rmse: 0.396064\tvalid_1's rmse: 0.427417\n",
      "[3050]\ttraining's rmse: 0.395989\tvalid_1's rmse: 0.427366\n",
      "[3060]\ttraining's rmse: 0.395924\tvalid_1's rmse: 0.427331\n",
      "[3070]\ttraining's rmse: 0.39586\tvalid_1's rmse: 0.427299\n",
      "[3080]\ttraining's rmse: 0.395808\tvalid_1's rmse: 0.427282\n",
      "[3090]\ttraining's rmse: 0.395744\tvalid_1's rmse: 0.427261\n",
      "[3100]\ttraining's rmse: 0.395689\tvalid_1's rmse: 0.427238\n",
      "[3110]\ttraining's rmse: 0.395625\tvalid_1's rmse: 0.427203\n",
      "[3120]\ttraining's rmse: 0.39557\tvalid_1's rmse: 0.427185\n",
      "[3130]\ttraining's rmse: 0.395512\tvalid_1's rmse: 0.427169\n",
      "[3140]\ttraining's rmse: 0.395464\tvalid_1's rmse: 0.42716\n",
      "[3150]\ttraining's rmse: 0.395401\tvalid_1's rmse: 0.427128\n",
      "[3160]\ttraining's rmse: 0.39534\tvalid_1's rmse: 0.427102\n",
      "[3170]\ttraining's rmse: 0.395281\tvalid_1's rmse: 0.427081\n",
      "[3180]\ttraining's rmse: 0.395221\tvalid_1's rmse: 0.427062\n",
      "[3190]\ttraining's rmse: 0.395139\tvalid_1's rmse: 0.427022\n",
      "[3200]\ttraining's rmse: 0.395067\tvalid_1's rmse: 0.426976\n",
      "[3210]\ttraining's rmse: 0.39501\tvalid_1's rmse: 0.426946\n",
      "[3220]\ttraining's rmse: 0.394949\tvalid_1's rmse: 0.426916\n",
      "[3230]\ttraining's rmse: 0.394883\tvalid_1's rmse: 0.426895\n",
      "[3240]\ttraining's rmse: 0.394827\tvalid_1's rmse: 0.426866\n",
      "[3250]\ttraining's rmse: 0.394756\tvalid_1's rmse: 0.426833\n",
      "[3260]\ttraining's rmse: 0.394695\tvalid_1's rmse: 0.426812\n",
      "[3270]\ttraining's rmse: 0.394634\tvalid_1's rmse: 0.42678\n",
      "[3280]\ttraining's rmse: 0.394558\tvalid_1's rmse: 0.426743\n",
      "[3290]\ttraining's rmse: 0.394499\tvalid_1's rmse: 0.426726\n",
      "[3300]\ttraining's rmse: 0.394449\tvalid_1's rmse: 0.426707\n",
      "[3310]\ttraining's rmse: 0.394403\tvalid_1's rmse: 0.42669\n",
      "[3320]\ttraining's rmse: 0.394351\tvalid_1's rmse: 0.426679\n",
      "[3330]\ttraining's rmse: 0.394295\tvalid_1's rmse: 0.426661\n",
      "[3340]\ttraining's rmse: 0.394234\tvalid_1's rmse: 0.426623\n",
      "[3350]\ttraining's rmse: 0.394186\tvalid_1's rmse: 0.426612\n",
      "[3360]\ttraining's rmse: 0.394122\tvalid_1's rmse: 0.42659\n",
      "[3370]\ttraining's rmse: 0.394054\tvalid_1's rmse: 0.426557\n",
      "[3380]\ttraining's rmse: 0.393994\tvalid_1's rmse: 0.426523\n",
      "[3390]\ttraining's rmse: 0.39394\tvalid_1's rmse: 0.426492\n",
      "[3400]\ttraining's rmse: 0.393886\tvalid_1's rmse: 0.426471\n",
      "[3410]\ttraining's rmse: 0.393832\tvalid_1's rmse: 0.426458\n",
      "[3420]\ttraining's rmse: 0.393773\tvalid_1's rmse: 0.426436\n",
      "[3430]\ttraining's rmse: 0.393718\tvalid_1's rmse: 0.426413\n",
      "[3440]\ttraining's rmse: 0.393666\tvalid_1's rmse: 0.426404\n",
      "[3450]\ttraining's rmse: 0.393622\tvalid_1's rmse: 0.426386\n",
      "[3460]\ttraining's rmse: 0.393565\tvalid_1's rmse: 0.426365\n",
      "[3470]\ttraining's rmse: 0.393517\tvalid_1's rmse: 0.426339\n",
      "[3480]\ttraining's rmse: 0.393454\tvalid_1's rmse: 0.426322\n",
      "[3490]\ttraining's rmse: 0.393406\tvalid_1's rmse: 0.426309\n",
      "[3500]\ttraining's rmse: 0.393348\tvalid_1's rmse: 0.426287\n",
      "[3510]\ttraining's rmse: 0.39328\tvalid_1's rmse: 0.426247\n",
      "[3520]\ttraining's rmse: 0.393206\tvalid_1's rmse: 0.426215\n",
      "[3530]\ttraining's rmse: 0.39314\tvalid_1's rmse: 0.426177\n",
      "[3540]\ttraining's rmse: 0.393063\tvalid_1's rmse: 0.426139\n",
      "[3550]\ttraining's rmse: 0.392984\tvalid_1's rmse: 0.426108\n",
      "[3560]\ttraining's rmse: 0.392927\tvalid_1's rmse: 0.426083\n",
      "[3570]\ttraining's rmse: 0.392871\tvalid_1's rmse: 0.426064\n",
      "[3580]\ttraining's rmse: 0.392817\tvalid_1's rmse: 0.426028\n",
      "[3590]\ttraining's rmse: 0.392767\tvalid_1's rmse: 0.426013\n",
      "[3600]\ttraining's rmse: 0.392704\tvalid_1's rmse: 0.425987\n",
      "[3610]\ttraining's rmse: 0.392643\tvalid_1's rmse: 0.425949\n",
      "[3620]\ttraining's rmse: 0.392574\tvalid_1's rmse: 0.425909\n",
      "[3630]\ttraining's rmse: 0.392519\tvalid_1's rmse: 0.42588\n",
      "[3640]\ttraining's rmse: 0.39247\tvalid_1's rmse: 0.425866\n",
      "[3650]\ttraining's rmse: 0.392435\tvalid_1's rmse: 0.425868\n",
      "[3660]\ttraining's rmse: 0.392388\tvalid_1's rmse: 0.425847\n",
      "[3670]\ttraining's rmse: 0.392319\tvalid_1's rmse: 0.425815\n",
      "[3680]\ttraining's rmse: 0.392264\tvalid_1's rmse: 0.425792\n",
      "[3690]\ttraining's rmse: 0.392212\tvalid_1's rmse: 0.425771\n",
      "[3700]\ttraining's rmse: 0.392166\tvalid_1's rmse: 0.425755\n",
      "[3710]\ttraining's rmse: 0.392132\tvalid_1's rmse: 0.425743\n",
      "[3720]\ttraining's rmse: 0.392085\tvalid_1's rmse: 0.42572\n",
      "[3730]\ttraining's rmse: 0.392028\tvalid_1's rmse: 0.425693\n",
      "[3740]\ttraining's rmse: 0.391973\tvalid_1's rmse: 0.425663\n",
      "[3750]\ttraining's rmse: 0.391917\tvalid_1's rmse: 0.425636\n",
      "[3760]\ttraining's rmse: 0.391859\tvalid_1's rmse: 0.425612\n",
      "[3770]\ttraining's rmse: 0.391798\tvalid_1's rmse: 0.42559\n",
      "[3780]\ttraining's rmse: 0.391739\tvalid_1's rmse: 0.425561\n",
      "[3790]\ttraining's rmse: 0.391679\tvalid_1's rmse: 0.425542\n",
      "[3800]\ttraining's rmse: 0.391622\tvalid_1's rmse: 0.425521\n",
      "[3810]\ttraining's rmse: 0.391567\tvalid_1's rmse: 0.425501\n",
      "[3820]\ttraining's rmse: 0.391507\tvalid_1's rmse: 0.42546\n",
      "[3830]\ttraining's rmse: 0.391452\tvalid_1's rmse: 0.425439\n",
      "[3840]\ttraining's rmse: 0.391384\tvalid_1's rmse: 0.425423\n",
      "[3850]\ttraining's rmse: 0.391325\tvalid_1's rmse: 0.425406\n",
      "[3860]\ttraining's rmse: 0.391256\tvalid_1's rmse: 0.425394\n",
      "[3870]\ttraining's rmse: 0.391193\tvalid_1's rmse: 0.425366\n",
      "[3880]\ttraining's rmse: 0.391135\tvalid_1's rmse: 0.42533\n",
      "[3890]\ttraining's rmse: 0.391092\tvalid_1's rmse: 0.42531\n",
      "[3900]\ttraining's rmse: 0.391048\tvalid_1's rmse: 0.42529\n",
      "[3910]\ttraining's rmse: 0.391008\tvalid_1's rmse: 0.425278\n",
      "[3920]\ttraining's rmse: 0.39095\tvalid_1's rmse: 0.425247\n",
      "[3930]\ttraining's rmse: 0.390901\tvalid_1's rmse: 0.425222\n",
      "[3940]\ttraining's rmse: 0.390859\tvalid_1's rmse: 0.425208\n",
      "[3950]\ttraining's rmse: 0.390817\tvalid_1's rmse: 0.4252\n",
      "[3960]\ttraining's rmse: 0.390772\tvalid_1's rmse: 0.425184\n",
      "[3970]\ttraining's rmse: 0.390724\tvalid_1's rmse: 0.425164\n",
      "[3980]\ttraining's rmse: 0.390671\tvalid_1's rmse: 0.425144\n",
      "[3990]\ttraining's rmse: 0.390627\tvalid_1's rmse: 0.425132\n",
      "[4000]\ttraining's rmse: 0.390567\tvalid_1's rmse: 0.425113\n",
      "[4010]\ttraining's rmse: 0.390506\tvalid_1's rmse: 0.425082\n",
      "[4020]\ttraining's rmse: 0.390449\tvalid_1's rmse: 0.425054\n",
      "[4030]\ttraining's rmse: 0.390402\tvalid_1's rmse: 0.425029\n",
      "[4040]\ttraining's rmse: 0.390353\tvalid_1's rmse: 0.425015\n",
      "[4050]\ttraining's rmse: 0.390306\tvalid_1's rmse: 0.425012\n",
      "[4060]\ttraining's rmse: 0.390261\tvalid_1's rmse: 0.425004\n",
      "[4070]\ttraining's rmse: 0.390197\tvalid_1's rmse: 0.424983\n",
      "[4080]\ttraining's rmse: 0.390147\tvalid_1's rmse: 0.424956\n",
      "[4090]\ttraining's rmse: 0.390099\tvalid_1's rmse: 0.424936\n",
      "[4100]\ttraining's rmse: 0.390049\tvalid_1's rmse: 0.424914\n",
      "[4110]\ttraining's rmse: 0.390001\tvalid_1's rmse: 0.424902\n",
      "[4120]\ttraining's rmse: 0.389954\tvalid_1's rmse: 0.424884\n",
      "[4130]\ttraining's rmse: 0.389914\tvalid_1's rmse: 0.42486\n",
      "[4140]\ttraining's rmse: 0.38986\tvalid_1's rmse: 0.424839\n",
      "[4150]\ttraining's rmse: 0.389804\tvalid_1's rmse: 0.424812\n",
      "[4160]\ttraining's rmse: 0.38975\tvalid_1's rmse: 0.424788\n",
      "[4170]\ttraining's rmse: 0.389697\tvalid_1's rmse: 0.424772\n",
      "[4180]\ttraining's rmse: 0.389638\tvalid_1's rmse: 0.42475\n",
      "[4190]\ttraining's rmse: 0.38958\tvalid_1's rmse: 0.424732\n",
      "[4200]\ttraining's rmse: 0.389524\tvalid_1's rmse: 0.424698\n",
      "[4210]\ttraining's rmse: 0.389468\tvalid_1's rmse: 0.424665\n",
      "[4220]\ttraining's rmse: 0.389419\tvalid_1's rmse: 0.424647\n",
      "[4230]\ttraining's rmse: 0.389363\tvalid_1's rmse: 0.424626\n",
      "[4240]\ttraining's rmse: 0.389326\tvalid_1's rmse: 0.424614\n",
      "[4250]\ttraining's rmse: 0.389274\tvalid_1's rmse: 0.424585\n",
      "[4260]\ttraining's rmse: 0.389224\tvalid_1's rmse: 0.424564\n",
      "[4270]\ttraining's rmse: 0.38918\tvalid_1's rmse: 0.424549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4280]\ttraining's rmse: 0.389122\tvalid_1's rmse: 0.424532\n",
      "[4290]\ttraining's rmse: 0.389066\tvalid_1's rmse: 0.424502\n",
      "[4300]\ttraining's rmse: 0.389028\tvalid_1's rmse: 0.424497\n",
      "[4310]\ttraining's rmse: 0.38898\tvalid_1's rmse: 0.424481\n",
      "[4320]\ttraining's rmse: 0.388935\tvalid_1's rmse: 0.424455\n",
      "[4330]\ttraining's rmse: 0.388896\tvalid_1's rmse: 0.424444\n",
      "[4340]\ttraining's rmse: 0.388844\tvalid_1's rmse: 0.424434\n",
      "[4350]\ttraining's rmse: 0.388806\tvalid_1's rmse: 0.424426\n",
      "[4360]\ttraining's rmse: 0.388761\tvalid_1's rmse: 0.424415\n",
      "[4370]\ttraining's rmse: 0.388714\tvalid_1's rmse: 0.424395\n",
      "[4380]\ttraining's rmse: 0.388678\tvalid_1's rmse: 0.424395\n",
      "[4390]\ttraining's rmse: 0.388633\tvalid_1's rmse: 0.424377\n",
      "[4400]\ttraining's rmse: 0.388584\tvalid_1's rmse: 0.424347\n",
      "[4410]\ttraining's rmse: 0.388533\tvalid_1's rmse: 0.424329\n",
      "[4420]\ttraining's rmse: 0.388494\tvalid_1's rmse: 0.424313\n",
      "[4430]\ttraining's rmse: 0.388443\tvalid_1's rmse: 0.424306\n",
      "[4440]\ttraining's rmse: 0.388397\tvalid_1's rmse: 0.424292\n",
      "[4450]\ttraining's rmse: 0.388357\tvalid_1's rmse: 0.424278\n",
      "[4460]\ttraining's rmse: 0.38831\tvalid_1's rmse: 0.424258\n",
      "[4470]\ttraining's rmse: 0.388249\tvalid_1's rmse: 0.424242\n",
      "[4480]\ttraining's rmse: 0.388207\tvalid_1's rmse: 0.424234\n",
      "[4490]\ttraining's rmse: 0.388153\tvalid_1's rmse: 0.424208\n",
      "[4500]\ttraining's rmse: 0.388115\tvalid_1's rmse: 0.424196\n",
      "[4510]\ttraining's rmse: 0.388065\tvalid_1's rmse: 0.424167\n",
      "[4520]\ttraining's rmse: 0.388007\tvalid_1's rmse: 0.42415\n",
      "[4530]\ttraining's rmse: 0.387956\tvalid_1's rmse: 0.424124\n",
      "[4540]\ttraining's rmse: 0.38792\tvalid_1's rmse: 0.424118\n",
      "[4550]\ttraining's rmse: 0.387877\tvalid_1's rmse: 0.42411\n",
      "[4560]\ttraining's rmse: 0.387818\tvalid_1's rmse: 0.424092\n",
      "[4570]\ttraining's rmse: 0.387767\tvalid_1's rmse: 0.42407\n",
      "[4580]\ttraining's rmse: 0.387718\tvalid_1's rmse: 0.424046\n",
      "[4590]\ttraining's rmse: 0.387672\tvalid_1's rmse: 0.424032\n",
      "[4600]\ttraining's rmse: 0.387624\tvalid_1's rmse: 0.424002\n",
      "[4610]\ttraining's rmse: 0.387583\tvalid_1's rmse: 0.423987\n",
      "[4620]\ttraining's rmse: 0.387536\tvalid_1's rmse: 0.423973\n",
      "[4630]\ttraining's rmse: 0.387494\tvalid_1's rmse: 0.423969\n",
      "[4640]\ttraining's rmse: 0.387456\tvalid_1's rmse: 0.423961\n",
      "[4650]\ttraining's rmse: 0.387403\tvalid_1's rmse: 0.423941\n",
      "[4660]\ttraining's rmse: 0.387359\tvalid_1's rmse: 0.423936\n",
      "[4670]\ttraining's rmse: 0.387306\tvalid_1's rmse: 0.42392\n",
      "[4680]\ttraining's rmse: 0.387258\tvalid_1's rmse: 0.423901\n",
      "[4690]\ttraining's rmse: 0.38721\tvalid_1's rmse: 0.423889\n",
      "[4700]\ttraining's rmse: 0.387165\tvalid_1's rmse: 0.423864\n",
      "[4710]\ttraining's rmse: 0.387118\tvalid_1's rmse: 0.423843\n",
      "[4720]\ttraining's rmse: 0.387083\tvalid_1's rmse: 0.423833\n",
      "[4730]\ttraining's rmse: 0.387041\tvalid_1's rmse: 0.423825\n",
      "[4740]\ttraining's rmse: 0.386997\tvalid_1's rmse: 0.423807\n",
      "[4750]\ttraining's rmse: 0.386956\tvalid_1's rmse: 0.423785\n",
      "[4760]\ttraining's rmse: 0.386914\tvalid_1's rmse: 0.423766\n",
      "[4770]\ttraining's rmse: 0.386868\tvalid_1's rmse: 0.423758\n",
      "[4780]\ttraining's rmse: 0.38683\tvalid_1's rmse: 0.423736\n",
      "[4790]\ttraining's rmse: 0.386779\tvalid_1's rmse: 0.42371\n",
      "[4800]\ttraining's rmse: 0.38674\tvalid_1's rmse: 0.4237\n",
      "[4810]\ttraining's rmse: 0.3867\tvalid_1's rmse: 0.423688\n",
      "[4820]\ttraining's rmse: 0.386661\tvalid_1's rmse: 0.423681\n",
      "[4830]\ttraining's rmse: 0.386614\tvalid_1's rmse: 0.423659\n",
      "[4840]\ttraining's rmse: 0.386562\tvalid_1's rmse: 0.423634\n",
      "[4850]\ttraining's rmse: 0.386518\tvalid_1's rmse: 0.423618\n",
      "[4860]\ttraining's rmse: 0.386484\tvalid_1's rmse: 0.423609\n",
      "[4870]\ttraining's rmse: 0.386437\tvalid_1's rmse: 0.423595\n",
      "[4880]\ttraining's rmse: 0.386396\tvalid_1's rmse: 0.423583\n",
      "[4890]\ttraining's rmse: 0.386348\tvalid_1's rmse: 0.423565\n",
      "[4900]\ttraining's rmse: 0.386306\tvalid_1's rmse: 0.423551\n",
      "[4910]\ttraining's rmse: 0.386268\tvalid_1's rmse: 0.423533\n",
      "[4920]\ttraining's rmse: 0.386215\tvalid_1's rmse: 0.423523\n",
      "[4930]\ttraining's rmse: 0.386174\tvalid_1's rmse: 0.423513\n",
      "[4940]\ttraining's rmse: 0.386125\tvalid_1's rmse: 0.423502\n",
      "[4950]\ttraining's rmse: 0.386075\tvalid_1's rmse: 0.42349\n",
      "[4960]\ttraining's rmse: 0.386016\tvalid_1's rmse: 0.423464\n",
      "[4970]\ttraining's rmse: 0.385972\tvalid_1's rmse: 0.423448\n",
      "[4980]\ttraining's rmse: 0.385925\tvalid_1's rmse: 0.423437\n",
      "[4990]\ttraining's rmse: 0.385887\tvalid_1's rmse: 0.423428\n",
      "[5000]\ttraining's rmse: 0.38585\tvalid_1's rmse: 0.42341\n",
      "[5010]\ttraining's rmse: 0.385803\tvalid_1's rmse: 0.423394\n",
      "[5020]\ttraining's rmse: 0.385765\tvalid_1's rmse: 0.423376\n",
      "[5030]\ttraining's rmse: 0.385725\tvalid_1's rmse: 0.423363\n",
      "[5040]\ttraining's rmse: 0.385682\tvalid_1's rmse: 0.42335\n",
      "[5050]\ttraining's rmse: 0.385641\tvalid_1's rmse: 0.423343\n",
      "[5060]\ttraining's rmse: 0.385605\tvalid_1's rmse: 0.423331\n",
      "[5070]\ttraining's rmse: 0.385567\tvalid_1's rmse: 0.423321\n",
      "[5080]\ttraining's rmse: 0.385524\tvalid_1's rmse: 0.423306\n",
      "[5090]\ttraining's rmse: 0.385488\tvalid_1's rmse: 0.423301\n",
      "[5100]\ttraining's rmse: 0.38545\tvalid_1's rmse: 0.42329\n",
      "[5110]\ttraining's rmse: 0.385397\tvalid_1's rmse: 0.423267\n",
      "[5120]\ttraining's rmse: 0.385355\tvalid_1's rmse: 0.423249\n",
      "[5130]\ttraining's rmse: 0.385312\tvalid_1's rmse: 0.423237\n",
      "[5140]\ttraining's rmse: 0.385272\tvalid_1's rmse: 0.42323\n",
      "[5150]\ttraining's rmse: 0.385222\tvalid_1's rmse: 0.423196\n",
      "[5160]\ttraining's rmse: 0.385187\tvalid_1's rmse: 0.423187\n",
      "[5170]\ttraining's rmse: 0.385134\tvalid_1's rmse: 0.423162\n",
      "[5180]\ttraining's rmse: 0.385088\tvalid_1's rmse: 0.423148\n",
      "[5190]\ttraining's rmse: 0.38505\tvalid_1's rmse: 0.42314\n",
      "[5200]\ttraining's rmse: 0.384997\tvalid_1's rmse: 0.423123\n",
      "[5210]\ttraining's rmse: 0.384961\tvalid_1's rmse: 0.423114\n",
      "[5220]\ttraining's rmse: 0.384923\tvalid_1's rmse: 0.423104\n",
      "[5230]\ttraining's rmse: 0.384871\tvalid_1's rmse: 0.423085\n",
      "[5240]\ttraining's rmse: 0.384828\tvalid_1's rmse: 0.423069\n",
      "[5250]\ttraining's rmse: 0.384787\tvalid_1's rmse: 0.423061\n",
      "[5260]\ttraining's rmse: 0.384748\tvalid_1's rmse: 0.423048\n",
      "[5270]\ttraining's rmse: 0.384711\tvalid_1's rmse: 0.423043\n",
      "[5280]\ttraining's rmse: 0.38468\tvalid_1's rmse: 0.423038\n",
      "[5290]\ttraining's rmse: 0.384649\tvalid_1's rmse: 0.423028\n",
      "[5300]\ttraining's rmse: 0.384612\tvalid_1's rmse: 0.42302\n",
      "[5310]\ttraining's rmse: 0.384563\tvalid_1's rmse: 0.423009\n",
      "[5320]\ttraining's rmse: 0.384522\tvalid_1's rmse: 0.423009\n",
      "[5330]\ttraining's rmse: 0.384478\tvalid_1's rmse: 0.422991\n",
      "[5340]\ttraining's rmse: 0.384436\tvalid_1's rmse: 0.422972\n",
      "[5350]\ttraining's rmse: 0.384392\tvalid_1's rmse: 0.422946\n",
      "[5360]\ttraining's rmse: 0.384355\tvalid_1's rmse: 0.422928\n",
      "[5370]\ttraining's rmse: 0.384325\tvalid_1's rmse: 0.422924\n",
      "[5380]\ttraining's rmse: 0.384285\tvalid_1's rmse: 0.422916\n",
      "[5390]\ttraining's rmse: 0.384255\tvalid_1's rmse: 0.42292\n",
      "[5400]\ttraining's rmse: 0.384213\tvalid_1's rmse: 0.422904\n",
      "[5410]\ttraining's rmse: 0.384174\tvalid_1's rmse: 0.42289\n",
      "[5420]\ttraining's rmse: 0.384138\tvalid_1's rmse: 0.422885\n",
      "[5430]\ttraining's rmse: 0.384093\tvalid_1's rmse: 0.422866\n",
      "[5440]\ttraining's rmse: 0.384051\tvalid_1's rmse: 0.422854\n",
      "[5450]\ttraining's rmse: 0.38401\tvalid_1's rmse: 0.422841\n",
      "[5460]\ttraining's rmse: 0.383966\tvalid_1's rmse: 0.422826\n",
      "[5470]\ttraining's rmse: 0.383926\tvalid_1's rmse: 0.422814\n",
      "[5480]\ttraining's rmse: 0.383879\tvalid_1's rmse: 0.422796\n",
      "[5490]\ttraining's rmse: 0.383839\tvalid_1's rmse: 0.422782\n",
      "[5500]\ttraining's rmse: 0.383795\tvalid_1's rmse: 0.422756\n",
      "[5510]\ttraining's rmse: 0.383759\tvalid_1's rmse: 0.422747\n",
      "[5520]\ttraining's rmse: 0.383723\tvalid_1's rmse: 0.422741\n",
      "[5530]\ttraining's rmse: 0.383691\tvalid_1's rmse: 0.422743\n",
      "[5540]\ttraining's rmse: 0.383651\tvalid_1's rmse: 0.422733\n",
      "[5550]\ttraining's rmse: 0.383602\tvalid_1's rmse: 0.422715\n",
      "[5560]\ttraining's rmse: 0.38356\tvalid_1's rmse: 0.422694\n",
      "[5570]\ttraining's rmse: 0.383516\tvalid_1's rmse: 0.422666\n",
      "[5580]\ttraining's rmse: 0.383474\tvalid_1's rmse: 0.422647\n",
      "[5590]\ttraining's rmse: 0.383431\tvalid_1's rmse: 0.422629\n",
      "[5600]\ttraining's rmse: 0.383388\tvalid_1's rmse: 0.422605\n",
      "[5610]\ttraining's rmse: 0.383351\tvalid_1's rmse: 0.422587\n",
      "[5620]\ttraining's rmse: 0.383317\tvalid_1's rmse: 0.422578\n",
      "[5630]\ttraining's rmse: 0.383276\tvalid_1's rmse: 0.422562\n",
      "[5640]\ttraining's rmse: 0.383225\tvalid_1's rmse: 0.422551\n",
      "[5650]\ttraining's rmse: 0.383178\tvalid_1's rmse: 0.422541\n",
      "[5660]\ttraining's rmse: 0.383126\tvalid_1's rmse: 0.422525\n",
      "[5670]\ttraining's rmse: 0.38308\tvalid_1's rmse: 0.422516\n",
      "[5680]\ttraining's rmse: 0.383046\tvalid_1's rmse: 0.422504\n",
      "[5690]\ttraining's rmse: 0.383006\tvalid_1's rmse: 0.422505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5700]\ttraining's rmse: 0.38296\tvalid_1's rmse: 0.422484\n",
      "[5710]\ttraining's rmse: 0.382922\tvalid_1's rmse: 0.422468\n",
      "[5720]\ttraining's rmse: 0.382877\tvalid_1's rmse: 0.422447\n",
      "[5730]\ttraining's rmse: 0.382844\tvalid_1's rmse: 0.422439\n",
      "[5740]\ttraining's rmse: 0.382807\tvalid_1's rmse: 0.42243\n",
      "[5750]\ttraining's rmse: 0.382767\tvalid_1's rmse: 0.422414\n",
      "[5760]\ttraining's rmse: 0.38273\tvalid_1's rmse: 0.422409\n",
      "[5770]\ttraining's rmse: 0.382687\tvalid_1's rmse: 0.422395\n",
      "[5780]\ttraining's rmse: 0.382646\tvalid_1's rmse: 0.422377\n",
      "[5790]\ttraining's rmse: 0.382614\tvalid_1's rmse: 0.422366\n",
      "[5800]\ttraining's rmse: 0.382567\tvalid_1's rmse: 0.422347\n",
      "[5810]\ttraining's rmse: 0.382522\tvalid_1's rmse: 0.422343\n",
      "[5820]\ttraining's rmse: 0.38248\tvalid_1's rmse: 0.422342\n",
      "[5830]\ttraining's rmse: 0.382442\tvalid_1's rmse: 0.422335\n",
      "[5840]\ttraining's rmse: 0.382399\tvalid_1's rmse: 0.422327\n",
      "[5850]\ttraining's rmse: 0.382354\tvalid_1's rmse: 0.422317\n",
      "[5860]\ttraining's rmse: 0.382318\tvalid_1's rmse: 0.422316\n",
      "[5870]\ttraining's rmse: 0.382282\tvalid_1's rmse: 0.422305\n",
      "[5880]\ttraining's rmse: 0.38225\tvalid_1's rmse: 0.422302\n",
      "[5890]\ttraining's rmse: 0.382216\tvalid_1's rmse: 0.422289\n",
      "[5900]\ttraining's rmse: 0.382185\tvalid_1's rmse: 0.422287\n",
      "[5910]\ttraining's rmse: 0.382146\tvalid_1's rmse: 0.422282\n",
      "[5920]\ttraining's rmse: 0.382105\tvalid_1's rmse: 0.422278\n",
      "[5930]\ttraining's rmse: 0.382071\tvalid_1's rmse: 0.422268\n",
      "[5940]\ttraining's rmse: 0.382037\tvalid_1's rmse: 0.422251\n",
      "[5950]\ttraining's rmse: 0.382003\tvalid_1's rmse: 0.42224\n",
      "[5960]\ttraining's rmse: 0.381969\tvalid_1's rmse: 0.422233\n",
      "[5970]\ttraining's rmse: 0.381939\tvalid_1's rmse: 0.422227\n",
      "[5980]\ttraining's rmse: 0.381899\tvalid_1's rmse: 0.422215\n",
      "[5990]\ttraining's rmse: 0.381862\tvalid_1's rmse: 0.422199\n",
      "[6000]\ttraining's rmse: 0.381821\tvalid_1's rmse: 0.422185\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's rmse: 0.381821\tvalid_1's rmse: 0.422185\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "       learning_rate=0.1, max_depth=-1, min_child_samples=20,\n",
       "       min_child_weight=0.001, min_split_gain=0.0, n_estimators=6000,\n",
       "       n_jobs=4, num_leaves=8, objective=None, random_state=None,\n",
       "       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "       subsample_for_bin=200000, subsample_freq=1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import RobustScaler, QuantileTransformer\n",
    "lgb1 = lgb.LGBMRegressor(learning_rate=0.1, n_estimators=6000, num_leaves=15, n_jobs=4)\n",
    "\n",
    "dtrain, dvalid = train_test_split(train2, test_size=0.05, random_state=1)\n",
    "feats = [col for col in train2.columns if col not in ['price','id']]\n",
    "#scaler = QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "Xtrain = dtrain[feats]\n",
    "Xvalid = dvalid[feats]\n",
    "y = np.log1p(dtrain['price'].values)\n",
    "yvalid = np.log1p(dvalid['price'].values)\n",
    "lgb1.fit(Xtrain, y, eval_set=[(Xtrain, y), (Xvalid, yvalid)], eval_metric='l2_root', verbose=10, early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "rd = Ridge()\n",
    "rd.fit(Xtrain, y)\n",
    "rmse(yvalid, rd.predict(Xvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(10, 20))\n",
    "lgb.plot_importance(lgb1, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9270ef51-217b-45d0-8f7c-95b648c92783",
    "_uuid": "e2e2b96ae3857a18055ff22f16532a6b5b39ec8a"
   },
   "outputs": [],
   "source": [
    "#Checking sequence lengths\n",
    "#sns.distplot(data.item_description.apply(lambda x: len(str(x).split())))\n",
    "test_data['id'] = test_data['test_id']\n",
    "del name_train_df, desc_train_df, train_data, brand_train_df, condition_train_df\n",
    "name_test_df = get_embeds(test_data, 'name', 50000, 10, 70, nnet, 8)\n",
    "desc_test_df = get_embeds(test_data, 'item_description', 100000, 80, 70, nnet, 9)\n",
    "brand_test_df = get_embeds2(test_data, 'brand_name', 6000, 1, 40, nnet, 10)\n",
    "category_test_df = get_embeds2(test_data, 'category_name', 1500, 1, 30, nnet, 11)\n",
    "condition_test_df = get_embeds2(test_data, 'item_condition_id', 5, 1, 4, nnet, 12)\n",
    "cat1_test_df = get_embeds2(test_data, 'cat1', 15, 1, 4, nnet, 13)\n",
    "cat2_test_df = get_embeds2(test_data, 'cat2', 120, 1, 10, nnet, 14)\n",
    "cat3_test_df = get_embeds2(test_data, 'cat3', 900, 1, 15, nnet, 15)\n",
    "\n",
    "train2 = embed_df(test_data, name_test_df, desc_test_df, brand_test_df, category_test_df,\n",
    "                 cat1_test_df, cat2_test_df, cat3_test_df, condition_test_df)\n",
    "test_preds = np.expm1(lgb1.predict(test2[feats]))\n",
    "\n",
    "print(\"Write out submission\")\n",
    "sub = pd.DataFrame({'test_id': test_data['id'].values})\n",
    "sub['price']= test_preds\n",
    "sub['test_id'] =sub['test_id'].astype(np.int32)\n",
    "sub['price'] = sub['price'].clip(3, 2000)\n",
    "sub.to_csv(\"embed_lgb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.distplot(np.log1p(train_data['price']))\n",
    "sns.distplot(np.log1p(test_preds))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
