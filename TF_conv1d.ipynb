{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohsin/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c838e9949c6948dc8a851429631ce94a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "from time import time\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from fastcache import clru_cache as lru_cache\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn import metrics\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm.pandas(tqdm_notebook())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_start = time()\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(metrics.mean_squared_error(y_true, y_pred))\n",
    "\n",
    "@lru_cache(1024)\n",
    "def stem(s):\n",
    "    return stemmer.stem(s)\n",
    "\n",
    "whitespace = re.compile(r'\\s+')\n",
    "non_letter = re.compile(r'\\W+')\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = non_letter.sub(' ', text)\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for t in text.split():\n",
    "        #t = stem(t)\n",
    "        tokens.append(t)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, min_df=10, tokenizer=str.split):\n",
    "        self.min_df = min_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.doc_freq = None\n",
    "        self.vocab = None\n",
    "        self.vocab_idx = None\n",
    "        self.max_len = None\n",
    "\n",
    "    def fit_transform(self, texts):\n",
    "        tokenized = []\n",
    "        doc_freq = Counter()\n",
    "        n = len(texts)\n",
    "\n",
    "        for text in texts:\n",
    "            sentence = self.tokenizer(text)\n",
    "            tokenized.append(sentence)\n",
    "            doc_freq.update(set(sentence))\n",
    "\n",
    "        vocab = sorted([t for (t, c) in doc_freq.items() if c >= self.min_df])\n",
    "        vocab_idx = {t: (i + 1) for (i, t) in enumerate(vocab)}\n",
    "        doc_freq = [doc_freq[t] for t in vocab]\n",
    "\n",
    "        self.doc_freq = doc_freq\n",
    "        self.vocab = vocab\n",
    "        self.vocab_idx = vocab_idx\n",
    "\n",
    "        max_len = 0\n",
    "        result_list = []\n",
    "        for text in tokenized:\n",
    "            text = self.text_to_idx(text)\n",
    "            max_len = max(max_len, len(text))\n",
    "            result_list.append(text)\n",
    "\n",
    "        self.max_len = max_len\n",
    "        result = np.zeros(shape=(n, max_len), dtype=np.int32)\n",
    "        for i in range(n):\n",
    "            text = result_list[i]\n",
    "            result[i, :len(text)] = text\n",
    "\n",
    "        return result    \n",
    "\n",
    "    def text_to_idx(self, tokenized):\n",
    "        return [self.vocab_idx[t] for t in tokenized if t in self.vocab_idx]\n",
    "\n",
    "    def transform(self, texts):\n",
    "        n = len(texts)\n",
    "        result = np.zeros(shape=(n, self.max_len), dtype=np.int32)\n",
    "\n",
    "        for i in range(n):\n",
    "            text = self.tokenizer(texts[i])\n",
    "            text = self.text_to_idx(text)[:self.max_len]\n",
    "            result[i, :len(text)] = text\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def vocabulary_size(self):\n",
    "        return len(self.vocab) + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading train data...\n",
      "CPU times: user 4.15 s, sys: 289 ms, total: 4.44 s\n",
      "Wall time: 4.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('reading train data...')\n",
    "df_train = pd.read_csv('../input/train.tsv', sep='\\t')\n",
    "df_train = df_train[df_train.price != 0].reset_index(drop=True)\n",
    "\n",
    "price = df_train.pop('price')\n",
    "y = np.log1p(price.values).reshape(-1,1)\n",
    "mean = y.mean()\n",
    "std = y.std()\n",
    "ynorm = (y - mean) / std\n",
    "ynorm = ynorm.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 297 ms, sys: 1.25 ms, total: 298 ms\n",
      "Wall time: 297 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_train.name.fillna('unkname', inplace=True)\n",
    "df_train.category_name.fillna('unk_cat', inplace=True)\n",
    "df_train.brand_name.fillna('unk_brand', inplace=True)\n",
    "df_train.item_description.fillna('nodesc', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 414/1481661 [00:00<05:57, 4139.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1481661/1481661 [01:13<00:00, 20173.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 13s, sys: 220 ms, total: 1min 14s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#impute brand names using flashtext\n",
    "def get_brands(x, keyword_processor):\n",
    "    matches = keyword_processor.extract_keywords(' '.join(x))\n",
    "    if len(matches) > 0:\n",
    "        return matches[0]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "from flashtext import KeywordProcessor\n",
    "all_brands = df_train.loc[df_train.brand_name.value_counts().values > 3, 'brand_name'].tolist()\n",
    "#all_brands.remove('always')\n",
    "\n",
    "keyword_processor = KeywordProcessor(case_sensitive=True)\n",
    "keyword_processor.add_keywords_from_list(all_brands)\n",
    "print(len(keyword_processor))\n",
    "\n",
    "tmp = df_train[['name','item_description']].progress_apply(lambda x: get_brands(x, keyword_processor), axis=1).fillna(\"unk_brand\")\n",
    "missing_indices = df_train['brand_name'] == 'unk_brand'\n",
    "df_train.loc[missing_indices, 'brand_name'] = tmp[missing_indices]\n",
    "df_train.brand_name.fillna(\"unk_brand\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bmiss = df_train.loc[df_train.brand_name == 'unk_brand'].index\n",
    "#sum([1 if len(x)> 0 else 0 for x in tmp[bmiss]])\n",
    "#\n",
    "#c=Counter(df_train.brand_name.tolist())\n",
    "#c.most_common(600)\n",
    "\n",
    "#c=Counter([x[0] for x in tmp if len(x)>0])\n",
    "#c.most_common(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing category...\n",
      "853\n",
      "CPU times: user 10.5 s, sys: 180 ms, total: 10.6 s\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('processing category...')\n",
    "\n",
    "def paths(tokens):\n",
    "    all_paths = ['/'.join(tokens[0:(i+1)]) for i in range(len(tokens))]\n",
    "    return ' '.join(all_paths)\n",
    "\n",
    "@lru_cache(1024)\n",
    "def cat_process(cat):\n",
    "    cat = cat.lower()\n",
    "    cat = whitespace.sub('', cat)\n",
    "    split = cat.split('/')\n",
    "    return paths(split)\n",
    "\n",
    "df_train.category_name = df_train.category_name.apply(cat_process)\n",
    "\n",
    "cat_tok = Tokenizer(min_df=50)\n",
    "X_cat = cat_tok.fit_transform(df_train.category_name)\n",
    "cat_voc_size = cat_tok.vocabulary_size()\n",
    "print(cat_voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing title...\n",
      "25587\n",
      "CPU times: user 16.5 s, sys: 220 ms, total: 16.8 s\n",
      "Wall time: 16.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('processing title...')\n",
    "\n",
    "name_tok = Tokenizer(min_df=5, tokenizer=tokenize)\n",
    "X_name = name_tok.fit_transform(df_train.name)\n",
    "name_voc_size = name_tok.vocabulary_size()\n",
    "print(name_voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing description...\n",
      "32139\n",
      "CPU times: user 47.9 s, sys: 932 ms, total: 48.8 s\n",
      "Wall time: 48.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('processing description...')\n",
    "\n",
    "desc_num_col = 40\n",
    "desc_tok = Tokenizer(min_df=10, tokenizer=tokenize)\n",
    "X_desc = desc_tok.fit_transform(df_train.item_description)\n",
    "X_desc = X_desc[:, :desc_num_col]\n",
    "desc_voc_size = desc_tok.vocabulary_size()\n",
    "print(desc_voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing brand...\n",
      "3002\n",
      "CPU times: user 1.63 s, sys: 16 ms, total: 1.65 s\n",
      "Wall time: 1.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('processing brand...')\n",
    "\n",
    "df_train.brand_name = df_train.brand_name.str.lower()\n",
    "df_train.brand_name = df_train.brand_name.str.replace(' ', '_')\n",
    "\n",
    "brand_cnt = Counter(df_train.brand_name[df_train.brand_name != 'unk_brand'])\n",
    "brands = sorted(b for (b, c) in brand_cnt.items() if c >= 3)\n",
    "brands_idx = {b: (i + 1) for (i, b) in enumerate(brands)}\n",
    "\n",
    "X_brand = df_train.brand_name.apply(lambda b: brands_idx.get(b, 0))\n",
    "X_brand = X_brand.values.reshape(-1, 1) \n",
    "brand_voc_size = len(brands) + 1\n",
    "print(brand_voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing other features...\n",
      "CPU times: user 8.19 ms, sys: 55.7 ms, total: 63.8 ms\n",
      "Wall time: 101 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('processing other features...')\n",
    "\n",
    "X_item_cond = (df_train.item_condition_id - 1).astype('uint8').values.reshape(-1, 1)\n",
    "X_shipping = df_train.shipping.astype('float32').values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defining the model...\n"
     ]
    }
   ],
   "source": [
    "print('defining the model...')\n",
    "\n",
    "def prepare_batches(seq, step):\n",
    "    n = len(seq)\n",
    "    res = []\n",
    "    for i in range(0, n, step):\n",
    "        res.append(seq[i:i+step])\n",
    "    return res\n",
    "\n",
    "def conv1d(inputs, num_filters, filter_size, padding='same'):\n",
    "    he_std = np.sqrt(2 / (filter_size * num_filters))\n",
    "    out = tf.layers.conv1d(\n",
    "        inputs=inputs, filters=num_filters, padding=padding,\n",
    "        kernel_size=filter_size,\n",
    "        activation=tf.nn.relu, \n",
    "        kernel_initializer=tf.random_normal_initializer(stddev=he_std))\n",
    "    return out\n",
    "\n",
    "def dense(X, size, reg=0.0, activation=None):\n",
    "    he_std = np.sqrt(2 / int(X.shape[1]))\n",
    "    out = tf.layers.dense(X, units=size, activation=activation, \n",
    "                     kernel_initializer=tf.random_normal_initializer(stddev=he_std),\n",
    "                     kernel_regularizer=tf.contrib.layers.l2_regularizer(reg))\n",
    "    return out\n",
    "\n",
    "def embed(inputs, size, dim):\n",
    "    std = np.sqrt(2 / dim)\n",
    "    emb = tf.Variable(tf.random_uniform([size, dim], -std, std))\n",
    "    lookup = tf.nn.embedding_lookup(emb, inputs)\n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 20)\n",
      "(?, 20)\n",
      "(?, 64)\n",
      "(?, 32)\n",
      "(?, 1)\n",
      "(?, 5)\n",
      "concatenated dim: (?, 142)\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n"
     ]
    }
   ],
   "source": [
    "name_embeddings_dim = 128\n",
    "name_seq_len = X_name.shape[1]\n",
    "desc_embeddings_dim = 128\n",
    "desc_seq_len = X_desc.shape[1]\n",
    "\n",
    "brand_embeddings_dim = 64\n",
    "\n",
    "cat_embeddings_dim = 32\n",
    "cat_seq_len = X_cat.shape[1]\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "graph.seed = 1\n",
    "\n",
    "with graph.as_default():\n",
    "    place_name = tf.placeholder(tf.int32, shape=(None, name_seq_len))\n",
    "    place_desc = tf.placeholder(tf.int32, shape=(None, desc_seq_len))\n",
    "    place_brand = tf.placeholder(tf.int32, shape=(None, 1))\n",
    "    place_cat = tf.placeholder(tf.int32, shape=(None, cat_seq_len))\n",
    "    place_ship = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "    place_cond = tf.placeholder(tf.uint8, shape=(None, 1))\n",
    "\n",
    "    place_y = tf.placeholder(dtype=tf.float32, shape=(None, 1))\n",
    "\n",
    "    place_lr = tf.placeholder(tf.float32, shape=(), )\n",
    "\n",
    "    name = embed(place_name, name_voc_size, name_embeddings_dim)\n",
    "    desc = embed(place_desc, desc_voc_size, desc_embeddings_dim)\n",
    "    brand = embed(place_brand, brand_voc_size, brand_embeddings_dim)\n",
    "    cat = embed(place_cat, cat_voc_size, cat_embeddings_dim)\n",
    "\n",
    "    name = conv1d(name, num_filters=20, filter_size=3)\n",
    "    name = tf.layers.dropout(name, rate=0.05)\n",
    "    name = tf.layers.average_pooling1d(name, pool_size=name_seq_len, strides=1, padding='valid')\n",
    "    name = tf.contrib.layers.flatten(name)\n",
    "    print(name.shape)\n",
    "\n",
    "    desc = conv1d(desc, num_filters=20, filter_size=3)\n",
    "    desc = tf.layers.dropout(desc, rate=0.1)\n",
    "    desc = tf.layers.average_pooling1d(desc, pool_size=desc_seq_len, strides=1, padding='valid')\n",
    "\n",
    "    desc = tf.contrib.layers.flatten(desc)\n",
    "    print(desc.shape)\n",
    "\n",
    "    brand = tf.contrib.layers.flatten(brand)\n",
    "    print(brand.shape)\n",
    "\n",
    "    cat = tf.layers.average_pooling1d(cat, pool_size=cat_seq_len, strides=1, padding='valid')\n",
    "    cat = tf.contrib.layers.flatten(cat)\n",
    "    print(cat.shape)\n",
    "    \n",
    "    ship = place_ship\n",
    "    print(ship.shape)\n",
    "\n",
    "    cond = tf.one_hot(place_cond, 5)\n",
    "    cond = tf.contrib.layers.flatten(cond)\n",
    "    print(cond.shape)\n",
    "\n",
    "    out = tf.concat([name, desc, brand, cat, ship, cond], axis=1)\n",
    "    print('concatenated dim:', out.shape)\n",
    "    #out = tf.contrib.layers.batch_norm(out, decay=0.9)\n",
    "    out = dense(out, 256, activation=tf.nn.relu)\n",
    "    out = tf.layers.dropout(out, rate=0.0)\n",
    "    #out = dense(out, 64, activation=tf.nn.relu)\n",
    "    #out = tf.layers.dropout(out, rate=0.03)\n",
    "    #out = tf.contrib.layers.batch_norm(out, decay=0.9)\n",
    "    out = dense(out, 1)\n",
    "\n",
    "    loss = tf.losses.mean_squared_error(place_y, out)\n",
    "    rmse = tf.sqrt(loss)\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=place_lr)\n",
    "    train_step = opt.minimize(loss)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "session = tf.Session(config=None, graph=graph)\n",
    "session.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training the model...\n",
      "0.004\n",
      "Training epoch 0 took 19.590s\n",
      "Validation rmse is  0.4353417382652538\n",
      "0.0028000000000000004\n",
      "Training epoch 1 took 19.414s\n",
      "Validation rmse is  0.42810988971528074\n",
      "0.0016000000000000003\n",
      "Training epoch 2 took 19.629s\n",
      "Validation rmse is  0.4169708445792268\n",
      "0.0004000000000000002\n",
      "Training epoch 3 took 19.797s\n",
      "Validation rmse is  0.4158965267751313\n"
     ]
    }
   ],
   "source": [
    "print('training the model...')\n",
    "\n",
    "train_idx, val_idx= list(ShuffleSplit(1, test_size=0.05, random_state=2).split(X_name))[0]\n",
    "lr_init=0.004\n",
    "lr_decay=0.0012\n",
    "lr = lr_init\n",
    "for i in range(4):\n",
    "    t0 = time()\n",
    "    np.random.seed(i)\n",
    "    np.random.shuffle(train_idx)\n",
    "    batches = prepare_batches(train_idx, 500)\n",
    "\n",
    "    #if i <= 2:\n",
    "    #    lr = 0.006\n",
    "    #else:\n",
    "    #    lr = 0.001\n",
    "    lr = lr_init - lr_decay*i\n",
    "    print(lr)\n",
    "    for j, idx in enumerate(batches):\n",
    "        feed_dict = {\n",
    "            place_name: X_name[idx],\n",
    "            place_desc: X_desc[idx],\n",
    "            place_brand: X_brand[idx],\n",
    "            place_cat: X_cat[idx],\n",
    "            place_cond: X_item_cond[idx],\n",
    "            place_ship: X_shipping[idx],\n",
    "            place_y: y[idx],\n",
    "            place_lr: lr,\n",
    "        }\n",
    "        session.run(train_step, feed_dict=feed_dict)\n",
    "\n",
    "    took = time() - t0\n",
    "    print('Training epoch %d took %.3fs' % (i, took))\n",
    "    val_batches = prepare_batches(val_idx, 5000)\n",
    "    y_pred = np.zeros(len(X_name))\n",
    "    for idx in val_batches:\n",
    "        feed_dict = {\n",
    "            place_name: X_name[idx],\n",
    "            place_desc: X_desc[idx],\n",
    "            place_brand: X_brand[idx],\n",
    "            place_cat: X_cat[idx],\n",
    "            place_cond: X_item_cond[idx],\n",
    "            place_ship: X_shipping[idx],\n",
    "        }\n",
    "        batch_pred = session.run(out, feed_dict=feed_dict)\n",
    "        y_pred[idx] = batch_pred[:, 0]\n",
    "    y_pred_val = y_pred[val_idx]\n",
    "    y_true_val = y[val_idx][:,0]\n",
    "    print(\"Validation rmse is \", np.sqrt(metrics.mean_squared_error(y_true_val, y_pred_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.396548787979571"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(metrics.mean_squared_error(y_true_val[np.abs(y_true_val- y_pred_val) <= 1.5], y_pred_val[np.abs(y_true_val- y_pred_val) <= 1.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "other other/other other/other/other                                                                                                                                              12\n",
       "electronics electronics/videogames&consoles electronics/videogames&consoles/games                                                                                                11\n",
       "women women/jewelry women/jewelry/necklaces                                                                                                                                      10\n",
       "women women/jewelry women/jewelry/bracelets                                                                                                                                      10\n",
       "women women/jewelry women/jewelry/rings                                                                                                                                           9\n",
       "beauty beauty/skincare beauty/skincare/face                                                                                                                                       8\n",
       "kids kids/toys kids/toys/dolls&accessories                                                                                                                                        8\n",
       "women women/tops&blouses women/tops&blouses/t-shirts                                                                                                                              7\n",
       "vintage&collectibles vintage&collectibles/toy vintage&collectibles/toy/actionfigure                                                                                               6\n",
       "women women/women'shandbags women/women'shandbags/shoulderbag                                                                                                                     6\n",
       "men men/shoes men/shoes/athletic                                                                                                                                                  5\n",
       "women women/women'shandbags women/women'shandbags/messenger&crossbody                                                                                                             5\n",
       "electronics electronics/media electronics/media/dvd                                                                                                                               5\n",
       "women women/athleticapparel women/athleticapparel/pants,tights,leggings                                                                                                           5\n",
       "women women/other women/other/other                                                                                                                                               4\n",
       "vintage&collectibles vintage&collectibles/collectibles vintage&collectibles/collectibles/other                                                                                    4\n",
       "vintage&collectibles vintage&collectibles/tradingcards vintage&collectibles/tradingcards/sports                                                                                   4\n",
       "kids kids/toys kids/toys/buildingtoys                                                                                                                                             4\n",
       "vintage&collectibles vintage&collectibles/tradingcards vintage&collectibles/tradingcards/animation                                                                                4\n",
       "electronics electronics/cellphones&accessories electronics/cellphones&accessories/cellphones&smartphones                                                                          4\n",
       "beauty beauty/makeup beauty/makeup/lips                                                                                                                                           4\n",
       "women women/women'saccessories women/women'saccessories/wallets                                                                                                                   3\n",
       "electronics electronics/cameras&photography electronics/cameras&photography/digitalcameras                                                                                        3\n",
       "home home/kitchen&dining home/kitchen&dining/coffee&teaaccessories                                                                                                                3\n",
       "beauty beauty/makeup beauty/makeup/nails                                                                                                                                          3\n",
       "women women/dresses women/dresses/knee-length                                                                                                                                     3\n",
       "home home/homedécor home/homedécor/homedécoraccents                                                                                                                               3\n",
       "beauty beauty/makeup beauty/makeup/makeupsets                                                                                                                                     3\n",
       "kids kids/toys kids/toys/actionfigures&statues                                                                                                                                    3\n",
       "electronics electronics/videogames&consoles electronics/videogames&consoles/accessories                                                                                           3\n",
       "                                                                                                                                                                                 ..\n",
       "vintage&collectibles vintage&collectibles/jewelry vintage&collectibles/jewelry/ring                                                                                               1\n",
       "other other/musicalinstruments other/musicalinstruments/dj,electronicmusic&karaoke                                                                                                1\n",
       "other other/daily&travelitems other/daily&travelitems/personalcare                                                                                                                1\n",
       "other other/musicalinstruments other/musicalinstruments/microphones&accessories                                                                                                   1\n",
       "women women/pants women/pants/other                                                                                                                                               1\n",
       "other other/books other/books/literature&fiction                                                                                                                                  1\n",
       "women women/athleticapparel women/athleticapparel/shorts                                                                                                                          1\n",
       "kids kids/strollers kids/strollers/travelsystems                                                                                                                                  1\n",
       "women women/shoes women/shoes/athletic                                                                                                                                            1\n",
       "kids kids/strollers kids/strollers/lightweight                                                                                                                                    1\n",
       "women women/coats&jackets women/coats&jackets/fleecejacket                                                                                                                        1\n",
       "women women/women'shandbags women/women'shandbags/cosmeticbags                                                                                                                    1\n",
       "men men/sweats&hoodies men/sweats&hoodies/hoodie                                                                                                                                  1\n",
       "sports&outdoors sports&outdoors/exercise sports&outdoors/exercise/other                                                                                                           1\n",
       "beauty beauty/makeup beauty/makeup/makeuppalettes                                                                                                                                 1\n",
       "men men/athleticapparel men/athleticapparel/jerseys                                                                                                                               1\n",
       "kids kids/diapering kids/diapering/clothdiapers                                                                                                                                   1\n",
       "kids kids/toys kids/toys/hobbies                                                                                                                                                  1\n",
       "women women/shoes women/shoes/fashionsneakers                                                                                                                                     1\n",
       "women women/tops&blouses women/tops&blouses/knittop                                                                                                                               1\n",
       "electronics electronics/computers&tablets electronics/computers&tablets/ipad electronics/computers&tablets/ipad/tablet electronics/computers&tablets/ipad/tablet/ebookreaders     1\n",
       "men men/other men/other/other                                                                                                                                                     1\n",
       "sports&outdoors sports&outdoors/fanshop sports&outdoors/fanshop/nfl                                                                                                               1\n",
       "vintage&collectibles vintage&collectibles/supplies vintage&collectibles/supplies/bead                                                                                             1\n",
       "home home/kitchen&dining home/kitchen&dining/kitchen&tablelinens                                                                                                                  1\n",
       "beauty beauty/makeup beauty/makeup/makeupremover                                                                                                                                  1\n",
       "kids kids/bathing&skincare kids/bathing&skincare/skincare                                                                                                                         1\n",
       "men men/men'saccessories men/men'saccessories/sunglasses                                                                                                                          1\n",
       "kids kids/gear kids/gear/backpacks&carriers                                                                                                                                       1\n",
       "women women/shoes women/shoes/sandals                                                                                                                                             1\n",
       "Name: category_name, Length: 176, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[val_idx].loc[np.abs(y_true_val- y_pred_val) > 1.5].category_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = df_train.iloc[val_idx].loc[np.abs(y_true_val- y_pred_val) > 1.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp.loc[tmp.category_name == "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
