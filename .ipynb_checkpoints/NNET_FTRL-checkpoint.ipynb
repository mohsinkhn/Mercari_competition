{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO\n",
    "\n",
    "- Straighten script to do FM_FTRL and our nn\n",
    "- NN with wordbag features\n",
    "- Use FTRL optimizer for our NN\n",
    "- Exact features for expensive items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "np.random.seed(786)\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype, is_string_dtype\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "import math\n",
    "import gc\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(tqdm)\n",
    "\n",
    "import threading\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool, cpu_count\n",
    "num_partitions = 8\n",
    "num_cores = 4\n",
    "\n",
    "import wordbatch\n",
    "from  wordbatch.extractors import WordSeq, WordBag, WordHash\n",
    "from wordbatch.models import FTRL, FM_FTRL, NN_ReLU_H1, NN_ReLU_H2\n",
    "\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder, RobustScaler, MaxAbsScaler, QuantileTransformer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Reshape, concatenate, Dense, GlobalAveragePooling1D, BatchNormalization, Dropout, PReLU, LeakyReLU, Embedding\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.optimizers import Adam, RMSprop, Nadam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "rmse_sklearn = make_scorer(rmse, greater_is_better=False)\n",
    "\n",
    "\n",
    "stop_words = ['a', 'an', 'this', 'is', 'the', 'of', 'for']\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return  unicodedata.normalize('NFKC', s)\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"'\", r\"\", s)\n",
    "    s = re.sub(r\" one \", r\" 1 \", s)\n",
    "    s = re.sub(r\" two \", r\" 2 \", s)\n",
    "    s = re.sub(r\" three \", r\" 3 \", s)\n",
    "    s = re.sub(r\" four \", r\" 4 \", s)\n",
    "    s = re.sub(r\" five \", r\" 5 \", s)\n",
    "    s = re.sub(r\" six \", r\" 6 \", s)\n",
    "    s = re.sub(r\" seven \", r\" 7 \", s)\n",
    "    s = re.sub(r\"[.!?':;,]\", r\" \", s)\n",
    "    s = re.sub(r\"-\", r\"\", s)\n",
    "    s = re.sub(r\"[^0-9a-zA-Z]+\", r\" \", s)\n",
    "    s = re.sub(r\"lily jade\", r\"lilyjade\", s)\n",
    "    s = re.sub(r\"rae dunn cookie(s){0,1}\", r\"raedunncookie\", s)\n",
    "    s = re.sub(r\"hatchimals\", r\"hatchimal\", s)\n",
    "    s = re.sub(r\"virtual reality\", r\"vr\", s)\n",
    "    s = re.sub(r\" vs \", r\" victorias secret \", s)\n",
    "    s = re.sub(r\"google home\", r\"googlehome\", s)\n",
    "    s = re.sub(r\"16 gb\", r\"16gb \", s)\n",
    "    s = re.sub(r\"256 gb\", r\"256gb \", s)\n",
    "    s = re.sub(r\"32 gb\", r\"32gb \", s)\n",
    "    s = re.sub(r\"(?=\\w{1,2})iphone \", r\"iphone\", s)\n",
    "    s = re.sub(r\"(?=\\w{1,2})galaxy \", r\"galaxy\", s)\n",
    "    s = re.sub(\"14(k){0,1} gold\", '14kgold', s)\n",
    "    s = re.sub(\"lululemon bags\", 'lululemonbags', s)\n",
    "    s = re.sub(\"controller skin\", 'controllerskin', s)\n",
    "    s = re.sub(\"watch box\", 'watchbox', s)\n",
    "    s = re.sub(\"blaze band\", 'blazeband', s)\n",
    "    s = re.sub(\"vault boy\", 'vaultboy', s)\n",
    "    s = re.sub(\"lash boost\", 'lashboost', s)\n",
    "    s = re.sub(\"64 g \", '64gb ', s)\n",
    "    s = re.sub(\"32 g \", '32gb ', s)\n",
    "    s = re.sub(\"go(\\s){0,1}pro hero\", 'goprohero', s)\n",
    "    s = re.sub(\"nmd(s){0,1}(\\s){0,1}(r){0,1}(1){0,1}(\\s|$)\", 'nmdr ', s)\n",
    "    s = re.sub(\"private sale\", 'privatesale', s)\n",
    "    s = re.sub(\"vutton\", 'vuitton', s)\n",
    "    s = re.sub(\"louis vuitton eva\", 'louisvuittoneva', s)\n",
    "    s = re.sub(\"apple watch\", 'applewatch', s)\n",
    "    return s\n",
    "\n",
    "def _normalize_and_ngrams(sent, ngram):\n",
    "    input_list = normalizeString(sent).split()\n",
    "    input_list = [word for word in input_list if word not in stop_words]\n",
    "    #s = input_list.copy()\n",
    "    #for i in range(2, ngrams+1):\n",
    "    #    s += [' '.join(input_list[j:j+i]) for j in range(len(input_list)-i + 1)]\n",
    "        #s += list((zip(*[input_list[j:] for j in range(i)])))\n",
    "    s = [''.join(input_list[i:i+ngram]) for i in range(len(input_list))]\n",
    "    return ' '.join(s[:-1])\n",
    "\n",
    "def get_cat_1(x): return str(x).split('/')[0]\n",
    "def get_cat_2(x): return str(x).split('/')[1] if len(str(x).split('/')) > 1 else -1\n",
    "def get_cat_3(x): return ' '.join(str(x).split('/')[2:]) if len(str(x).split('/')) > 2 else -1\n",
    "\n",
    "def applycat1(df): \n",
    "    return df['category_name'].progress_apply(get_cat_1)\n",
    "    \n",
    "\n",
    "def applycat2(df): \n",
    "    return df['category_name'].progress_apply(get_cat_2)\n",
    "    \n",
    "\n",
    "def applycat3(df): \n",
    "    return df['category_name'].progress_apply(get_cat_3)\n",
    "\n",
    "def get_words(series): return series.progress_apply(lambda x: len(str(x).split()))\n",
    "\n",
    "def get_chars(series): return series.progress_apply(lambda x: len(str(x)))\n",
    "\n",
    "def get_tokens(series): return np.sum(np.array(series.tolist()) > 0, axis=1)\n",
    "\n",
    "def isphonecase(series): return series.str.contains(' case ', flags=re.IGNORECASE).astype(int)\n",
    "\n",
    "def isiphone6(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('6|six', flags=re.IGNORECASE) &\n",
    "                        ~(series.str.contains('plus|\\+', flags=re.IGNORECASE)) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) ).astype(int)\n",
    "\n",
    "def isiphone6p(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('6|six', flags=re.IGNORECASE) &\n",
    "                        series.str.contains('plus|\\+', flags=re.IGNORECASE) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) ).astype(int)\n",
    "\n",
    "def isiphone5(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('5|five', flags=re.IGNORECASE) &\n",
    "                        ~(series.str.contains('plus|\\+', flags=re.IGNORECASE)) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) ).astype(int)\n",
    "\n",
    "def isiphone5p(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('5|five', flags=re.IGNORECASE) &\n",
    "                        series.str.contains('plus|\\+', flags=re.IGNORECASE) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) ).astype(int)\n",
    "\n",
    "def isiphone7(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('7|seven', flags=re.IGNORECASE) &\n",
    "                        ~(series.str.contains('plus|\\+', flags=re.IGNORECASE)) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) ).astype(int)\n",
    "\n",
    "def isiphone7p(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('7|seven', flags=re.IGNORECASE) &\n",
    "                        series.str.contains('plus|\\+', flags=re.IGNORECASE) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) ).astype(int)\n",
    "\n",
    "def isunlocked(series): return series.str.contains('unlocked', flags=re.IGNORECASE).astype(int)\n",
    "\n",
    "def plussigns(series): return series.apply(lambda x: sum([(s == '+') | (s == '➕') for s in str(x)]))\n",
    "\n",
    "def andsigns(series): return series.apply(lambda x: sum([(s == '&') | (s == ' and ') for s in str(x)]))\n",
    "\n",
    "def commas(series): return series.apply(lambda x: sum([s == ',' for s in str(x)]))\n",
    "\n",
    "def add_ngrams(text, ngram=2):\n",
    "    word_list = normalizeString(text).split(' ')\n",
    "    out_list = [''.join(word_list[i:i+ngram]) for i in range(len(word_list))]\n",
    "    return ' '.join(out_list[:-1])\n",
    "\n",
    "def get_2grams(series): return series.apply(lambda x: _normalize_and_ngrams(str(x), 2))\n",
    "\n",
    "def norm3grams(s): return _normalize_and_ngrams(s, 3)\n",
    "\n",
    "def applyname(series): return series.progress_apply(norm3grams)\n",
    "\n",
    "def index2sent1(x, name_vocab): return indexesFromSentence(name_vocab, x, 3, 10)\n",
    "\n",
    "def name2index(series): return series.progress_apply(lambda x: index2sent1(x, name_vocab))\n",
    "\n",
    "def norm2grams(s): return _normalize_and_ngrams(s, 1)\n",
    "\n",
    "def applydesc(series):return series.progress_apply(norm2grams)\n",
    "\n",
    "def index2sent2(x, desc_vocab): return indexesFromSentence(desc_vocab, x, 1, 80)\n",
    "\n",
    "def desc2index(series): return series.progress_apply(lambda x: index2sent2(x, desc_vocab))\n",
    "\n",
    "def parallelize_dataframe(df, func):\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def cutting(dataset):\n",
    "    pop_brand = dataset['brand_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n",
    "    dataset.loc[~dataset['brand_name'].isin(pop_brand), 'brand_name'] = 'missing'\n",
    "    pop_category1 = dataset['category_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n",
    "    dataset.loc[~dataset['category_name'].isin(pop_category1), 'category_name'] = 'missing'\n",
    "\n",
    "    \n",
    "def generate_features(data):\n",
    "        data[\"item_description\"] = data[\"item_description\"].replace(\"No description yet\", \"missing\")\n",
    "        \n",
    "        data['cat1'] = parallelize_dataframe(data[['category_name']], applycat1)\n",
    "        data['cat2'] = parallelize_dataframe(data[['category_name']], applycat2)\n",
    "        data['cat3'] = parallelize_dataframe(data[['category_name']], applycat3)\n",
    "        data.fillna(-1, inplace=True)\n",
    "        \n",
    "        print(\"Getting word/char len features\")\n",
    "        data['desc_words'] = parallelize_dataframe(data['item_description'], get_words)\n",
    "        data['desc_chars'] = parallelize_dataframe(data['item_description'], get_chars)\n",
    "        data['name_words'] = parallelize_dataframe(data['name'], get_words)\n",
    "        data['name_chars'] = parallelize_dataframe(data['name'], get_chars)\n",
    "        \n",
    "        \n",
    "        print(\"Get iphone features\")\n",
    "        data['iphone_case'] = parallelize_dataframe(data['name'], isphonecase)\n",
    "        data['iphone6'] = parallelize_dataframe(data['name'], isiphone6)\n",
    "        data['iphone6p'] = parallelize_dataframe(data['name'], isiphone6p)\n",
    "        data['iphone5'] = parallelize_dataframe(data['name'], isiphone5)\n",
    "        data['iphone5p'] = parallelize_dataframe(data['name'], isiphone5p)\n",
    "        data['iphone7'] = parallelize_dataframe(data['name'], isiphone7)\n",
    "        data['iphone7p'] = parallelize_dataframe(data['name'], isiphone7p)\n",
    "        data['unlocked_phone'] = parallelize_dataframe(data['name'], isunlocked)\n",
    "        \n",
    "        all_brands = set(data['brand_name'].values)\n",
    "        def brandfinder(line):\n",
    "            brand = line[0]\n",
    "            name = line[1]\n",
    "            namesplit = name.split(' ')\n",
    "            if brand == 'missing':\n",
    "                for x in namesplit:\n",
    "                    if x in all_brands:\n",
    "                        return x\n",
    "            #else:\n",
    "            #    return brand\n",
    "            #if name in all_brands:\n",
    "            #    return name\n",
    "            return brand\n",
    "        data['brand_name2'] = data[['brand_name','name']].apply(brandfinder, axis = 1)   \n",
    "        \n",
    "        #print(\"Label encoding features\")\n",
    "        #cat_cols = ['category_name', 'brand_name', 'brand_name2', 'cat1', 'cat2', 'cat3', 'item_condition_id']\n",
    "        #for col in cat_cols:\n",
    "        #    data[col] = LabelEncoder().fit_transform(data[col].astype(str)) + 1\n",
    "            \n",
    "        print(\"Get count features\")\n",
    "        data['brand_counts'] = data.brand_name.map(data[\"brand_name\"].value_counts()).fillna(0).astype(int)\n",
    "\n",
    "        data['cat_counts'] = data.category_name.map(data[\"category_name\"].value_counts()).fillna(0).astype(int)\n",
    "        \n",
    "        data['cat1_counts'] = data.cat1.map(data[\"cat1\"].value_counts()).fillna(0).astype(int)\n",
    "\n",
    "        data['cat2_counts'] = data.cat2.map(data[\"cat2\"].value_counts()).fillna(0).astype(int)\n",
    "\n",
    "        data['cat3_counts'] = data.cat3.map(data[\"cat3\"].value_counts()).fillna(0).astype(int)\n",
    "  \n",
    "        \n",
    "        print(\"Getting punct related features\")\n",
    "        data[\"plus_counts\"] = parallelize_dataframe(data[\"item_description\"], plussigns)\n",
    "        data[\"ands_counts\"] = parallelize_dataframe(data[\"item_description\"], andsigns)\n",
    "        data[\"comma_counts\"] = parallelize_dataframe(data[\"item_description\"], commas)\n",
    "        data[\"all_counts\"] = data[\"plus_counts\"] + data[\"ands_counts\"] + data[\"comma_counts\"]\n",
    "        \n",
    "        #for col in [\"name\", \"item_description\"]:\n",
    "        #    data[col] = data[col].str.replace(\"'\", '').replace('-', '').progress_apply(unicodeToAscii)\n",
    "        #    data[col] = data[col].progress_apply(remove_puncts)\n",
    "        \n",
    "        data[\"brand_cat\"] = data[\"brand_name\"].astype(str) + ' ' + data[\"category_name\"].astype(str)\n",
    "        data[\"category_shipping\"] = data[\"category_name\"].astype(str) + ' ' + data[\"shipping\"].astype(str)\n",
    "        \n",
    "        data['brand_cat_counts'] = data.brand_cat.map(data[\"brand_cat\"].value_counts()).fillna(0).astype(int)\n",
    "        \n",
    "        num_cols =  [\"desc_words\", \"desc_chars\", \"name_words\", \"name_chars\", \"plus_counts\", \n",
    "                    \"ands_counts\", \"comma_counts\", \"all_counts\", \"brand_counts\", \"cat_counts\", \"cat1_counts\", \n",
    "                   \"cat2_counts\", \"cat3_counts\", \"brand_cat_counts\"]\n",
    "        data[num_cols]  = QuantileTransformer().fit_transform(data[num_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1482535, 8) (693359, 7)\n"
     ]
    }
   ],
   "source": [
    "in_path = \"../input/\"\n",
    "train_data = pd.read_table(os.path.join(in_path, 'train.tsv'))\n",
    "test_data  = pd.read_table(os.path.join(in_path, 'test.tsv'))\n",
    "\n",
    "train_rows = len(train_data)\n",
    "data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 271987/271987 [00:00<00:00, 660265.50it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 683249.35it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 681006.47it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 630651.82it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 649754.92it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 591550.75it/s]\n",
      "100%|██████████| 271986/271986 [00:00<00:00, 616528.00it/s]\n",
      "100%|██████████| 271986/271986 [00:00<00:00, 621740.90it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 529481.60it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 520537.13it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 533796.71it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 506607.58it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 506624.45it/s]\n",
      " 91%|█████████ | 246528/271986 [00:00<00:00, 492442.16it/s]\n",
      " 91%|█████████ | 247115/271986 [00:00<00:00, 495454.62it/s]\n",
      "100%|██████████| 271986/271986 [00:00<00:00, 483778.37it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 461888.28it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 478591.34it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 485444.56it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 479911.89it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 461320.28it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 477604.93it/s]\n",
      " 99%|█████████▉| 270301/271986 [00:00<00:00, 441098.62it/s]\n",
      "100%|██████████| 271986/271986 [00:00<00:00, 434426.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting word/char len features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 271987/271987 [00:00<00:00, 392074.54it/s]\n",
      " 57%|█████▋    | 154325/271987 [00:00<00:00, 387518.79it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 372031.10it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 386779.66it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 372177.96it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 387566.23it/s]\n",
      "100%|██████████| 271986/271986 [00:00<00:00, 389332.98it/s]\n",
      "100%|██████████| 271986/271986 [00:00<00:00, 401093.58it/s]\n",
      "  0%|          | 0/271987 [00:00<?, ?it/s]0, 708879.45it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 734432.47it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 703390.17it/s]\n",
      "  0%|          | 0/271986 [00:00<?, ?it/s]0, 742708.74it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 735664.81it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 673171.12it/s]\n",
      "100%|██████████| 271986/271986 [00:00<00:00, 696328.36it/s]\n",
      "100%|██████████| 271986/271986 [00:00<00:00, 750756.14it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 581681.84it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 544143.43it/s]\n",
      " 97%|█████████▋| 264579/271987 [00:00<00:00, 655445.65it/s]\n",
      " 22%|██▏       | 58543/271987 [00:00<00:00, 585429.16it/s]]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 565802.07it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 597607.76it/s]\n",
      "100%|██████████| 271986/271986 [00:00<00:00, 588575.92it/s]\n",
      "100%|██████████| 271986/271986 [00:00<00:00, 607020.25it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 710922.60it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 777947.99it/s]\n",
      " 84%|████████▍ | 228851/271987 [00:00<00:00, 777527.81it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 714864.74it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 697914.36it/s]\n",
      "100%|██████████| 271987/271987 [00:00<00:00, 737408.11it/s]\n",
      "100%|██████████| 271986/271986 [00:00<00:00, 772309.82it/s]\n",
      "100%|██████████| 271986/271986 [00:00<00:00, 733925.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get iphone features\n",
      "Get count features\n",
      "Getting punct related features\n",
      "(2175894, 37)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>item_description</th>\n",
       "      <th>name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>test_id</th>\n",
       "      <th>train_id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>...</th>\n",
       "      <th>cat1_counts</th>\n",
       "      <th>cat2_counts</th>\n",
       "      <th>cat3_counts</th>\n",
       "      <th>plus_counts</th>\n",
       "      <th>ands_counts</th>\n",
       "      <th>comma_counts</th>\n",
       "      <th>all_counts</th>\n",
       "      <th>brand_cat</th>\n",
       "      <th>category_shipping</th>\n",
       "      <th>brand_cat_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>missing</td>\n",
       "      <td>Men/Tops/T-shirts</td>\n",
       "      <td>3</td>\n",
       "      <td>missing</td>\n",
       "      <td>MLB Cincinnati Reds T Shirt Size XL</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Men</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183183</td>\n",
       "      <td>0.305806</td>\n",
       "      <td>0.573073</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>missing Men/Tops/T-shirts</td>\n",
       "      <td>Men/Tops/T-shirts 1</td>\n",
       "      <td>0.789790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Razer</td>\n",
       "      <td>Electronics/Computers &amp; Tablets/Components &amp; P...</td>\n",
       "      <td>3</td>\n",
       "      <td>This keyboard is in great condition and works ...</td>\n",
       "      <td>Razer BlackWidow Chroma Keyboard</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255756</td>\n",
       "      <td>0.092092</td>\n",
       "      <td>0.071987</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>Razer Electronics/Computers &amp; Tablets/Componen...</td>\n",
       "      <td>Electronics/Computers &amp; Tablets/Components &amp; P...</td>\n",
       "      <td>0.088088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Target</td>\n",
       "      <td>Women/Tops &amp; Blouses/Blouse</td>\n",
       "      <td>1</td>\n",
       "      <td>Adorable top with a hint of lace and a key hol...</td>\n",
       "      <td>AVA-VIV Blouse</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Women</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.788288</td>\n",
       "      <td>0.693193</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>7.597598e-01</td>\n",
       "      <td>7.352352e-01</td>\n",
       "      <td>Target Women/Tops &amp; Blouses/Blouse</td>\n",
       "      <td>Women/Tops &amp; Blouses/Blouse 1</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>missing</td>\n",
       "      <td>Home/Home Décor/Home Décor Accents</td>\n",
       "      <td>1</td>\n",
       "      <td>New with tags. Leather horses. Retail for [rm]...</td>\n",
       "      <td>Leather Horse Statues</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Home</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128629</td>\n",
       "      <td>0.353854</td>\n",
       "      <td>0.505005</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>missing Home/Home Décor/Home Décor Accents</td>\n",
       "      <td>Home/Home Décor/Home Décor Accents 1</td>\n",
       "      <td>0.915916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>missing</td>\n",
       "      <td>Women/Jewelry/Necklaces</td>\n",
       "      <td>1</td>\n",
       "      <td>Complete with certificate of authenticity</td>\n",
       "      <td>24K GOLD plated rose</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Women</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.663664</td>\n",
       "      <td>0.653654</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>missing Women/Jewelry/Necklaces</td>\n",
       "      <td>Women/Jewelry/Necklaces 0</td>\n",
       "      <td>0.958458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  brand_name                                      category_name  \\\n",
       "0    missing                                  Men/Tops/T-shirts   \n",
       "1      Razer  Electronics/Computers & Tablets/Components & P...   \n",
       "2     Target                        Women/Tops & Blouses/Blouse   \n",
       "3    missing                 Home/Home Décor/Home Décor Accents   \n",
       "4    missing                            Women/Jewelry/Necklaces   \n",
       "\n",
       "   item_condition_id                                   item_description  \\\n",
       "0                  3                                            missing   \n",
       "1                  3  This keyboard is in great condition and works ...   \n",
       "2                  1  Adorable top with a hint of lace and a key hol...   \n",
       "3                  1  New with tags. Leather horses. Retail for [rm]...   \n",
       "4                  1          Complete with certificate of authenticity   \n",
       "\n",
       "                                  name  price  shipping  test_id  train_id  \\\n",
       "0  MLB Cincinnati Reds T Shirt Size XL   10.0         1     -1.0       0.0   \n",
       "1     Razer BlackWidow Chroma Keyboard   52.0         0     -1.0       1.0   \n",
       "2                       AVA-VIV Blouse   10.0         1     -1.0       2.0   \n",
       "3                Leather Horse Statues   35.0         1     -1.0       3.0   \n",
       "4                 24K GOLD plated rose   44.0         0     -1.0       4.0   \n",
       "\n",
       "          cat1        ...        cat1_counts cat2_counts  cat3_counts  \\\n",
       "0          Men        ...           0.183183    0.305806     0.573073   \n",
       "1  Electronics        ...           0.255756    0.092092     0.071987   \n",
       "2        Women        ...           1.000000    0.788288     0.693193   \n",
       "3         Home        ...           0.128629    0.353854     0.505005   \n",
       "4        Women        ...           1.000000    0.663664     0.653654   \n",
       "\n",
       "    plus_counts   ands_counts  comma_counts    all_counts  \\\n",
       "0  1.000000e-07  1.000000e-07  1.000000e-07  1.000000e-07   \n",
       "1  1.000000e-07  1.000000e-07  1.000000e-07  1.000000e-07   \n",
       "2  1.000000e-07  1.000000e-07  7.597598e-01  7.352352e-01   \n",
       "3  1.000000e-07  1.000000e-07  1.000000e-07  1.000000e-07   \n",
       "4  1.000000e-07  1.000000e-07  1.000000e-07  1.000000e-07   \n",
       "\n",
       "                                           brand_cat  \\\n",
       "0                          missing Men/Tops/T-shirts   \n",
       "1  Razer Electronics/Computers & Tablets/Componen...   \n",
       "2                 Target Women/Tops & Blouses/Blouse   \n",
       "3         missing Home/Home Décor/Home Décor Accents   \n",
       "4                    missing Women/Jewelry/Necklaces   \n",
       "\n",
       "                                   category_shipping  brand_cat_counts  \n",
       "0                                Men/Tops/T-shirts 1          0.789790  \n",
       "1  Electronics/Computers & Tablets/Components & P...          0.088088  \n",
       "2                      Women/Tops & Blouses/Blouse 1          0.111111  \n",
       "3               Home/Home Décor/Home Décor Accents 1          0.915916  \n",
       "4                          Women/Jewelry/Necklaces 0          0.958458  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_BRANDS = 4500\n",
    "NUM_CATEGORIES = 1200\n",
    "\n",
    "cutting(data)\n",
    "generate_features(data)\n",
    "\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>test_id</th>\n",
       "      <th>train_id</th>\n",
       "      <th>desc_words</th>\n",
       "      <th>desc_chars</th>\n",
       "      <th>name_words</th>\n",
       "      <th>name_chars</th>\n",
       "      <th>iphone_case</th>\n",
       "      <th>...</th>\n",
       "      <th>brand_counts</th>\n",
       "      <th>cat_counts</th>\n",
       "      <th>cat1_counts</th>\n",
       "      <th>cat2_counts</th>\n",
       "      <th>cat3_counts</th>\n",
       "      <th>plus_counts</th>\n",
       "      <th>ands_counts</th>\n",
       "      <th>comma_counts</th>\n",
       "      <th>all_counts</th>\n",
       "      <th>brand_cat_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.175894e+06</td>\n",
       "      <td>2.175894e+06</td>\n",
       "      <td>2.175894e+06</td>\n",
       "      <td>2.175894e+06</td>\n",
       "      <td>2.175894e+06</td>\n",
       "      <td>2.175894e+06</td>\n",
       "      <td>2.175894e+06</td>\n",
       "      <td>2.175894e+06</td>\n",
       "      <td>2.175894e+06</td>\n",
       "      <td>2.175894e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.175894e+06</td>\n",
       "      <td>2.175894e+06</td>\n",
       "      <td>2.175894e+06</td>\n",
       "      <td>2.175894e+06</td>\n",
       "      <td>2.175894e+06</td>\n",
       "      <td>2.175894e+06</td>\n",
       "      <td>2.175894e+06</td>\n",
       "      <td>2.175894e+06</td>\n",
       "      <td>2.175894e+06</td>\n",
       "      <td>2.175894e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.906973e+00</td>\n",
       "      <td>1.789882e+01</td>\n",
       "      <td>4.474161e-01</td>\n",
       "      <td>1.104702e+05</td>\n",
       "      <td>5.050584e+05</td>\n",
       "      <td>4.974276e-01</td>\n",
       "      <td>4.993008e-01</td>\n",
       "      <td>5.008307e-01</td>\n",
       "      <td>5.012269e-01</td>\n",
       "      <td>4.991512e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>5.911850e-01</td>\n",
       "      <td>5.002783e-01</td>\n",
       "      <td>6.008656e-01</td>\n",
       "      <td>5.034970e-01</td>\n",
       "      <td>4.996568e-01</td>\n",
       "      <td>1.889075e-02</td>\n",
       "      <td>6.296192e-02</td>\n",
       "      <td>2.591100e-01</td>\n",
       "      <td>2.825355e-01</td>\n",
       "      <td>5.008439e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.032285e-01</td>\n",
       "      <td>3.437274e+01</td>\n",
       "      <td>4.972274e-01</td>\n",
       "      <td>1.971302e+05</td>\n",
       "      <td>4.940588e+05</td>\n",
       "      <td>2.917889e-01</td>\n",
       "      <td>2.884005e-01</td>\n",
       "      <td>2.854952e-01</td>\n",
       "      <td>2.887067e-01</td>\n",
       "      <td>7.047410e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>3.745884e-01</td>\n",
       "      <td>2.897944e-01</td>\n",
       "      <td>3.779273e-01</td>\n",
       "      <td>2.951679e-01</td>\n",
       "      <td>2.896321e-01</td>\n",
       "      <td>1.354933e-01</td>\n",
       "      <td>2.386104e-01</td>\n",
       "      <td>3.926311e-01</td>\n",
       "      <td>3.969298e-01</td>\n",
       "      <td>2.887984e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>2.492492e-01</td>\n",
       "      <td>2.517518e-01</td>\n",
       "      <td>2.272272e-01</td>\n",
       "      <td>2.647648e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.492492e-01</td>\n",
       "      <td>2.497497e-01</td>\n",
       "      <td>2.557558e-01</td>\n",
       "      <td>2.547548e-01</td>\n",
       "      <td>2.512513e-01</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>2.509176e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>3.945875e+05</td>\n",
       "      <td>4.959960e-01</td>\n",
       "      <td>5.025025e-01</td>\n",
       "      <td>4.309309e-01</td>\n",
       "      <td>5.030030e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>4.849850e-01</td>\n",
       "      <td>4.994995e-01</td>\n",
       "      <td>4.834835e-01</td>\n",
       "      <td>4.924925e-01</td>\n",
       "      <td>4.959960e-01</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>5.015015e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>2.200000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.493848e+05</td>\n",
       "      <td>9.385608e+05</td>\n",
       "      <td>7.472472e-01</td>\n",
       "      <td>7.482482e-01</td>\n",
       "      <td>8.133133e-01</td>\n",
       "      <td>7.567568e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>7.437437e-01</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>7.177177e-01</td>\n",
       "      <td>7.507508e-01</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>7.597598e-01</td>\n",
       "      <td>7.352352e-01</td>\n",
       "      <td>7.507508e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>2.009000e+03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.933580e+05</td>\n",
       "      <td>1.482534e+06</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>9.999999e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       item_condition_id         price      shipping       test_id  \\\n",
       "count       2.175894e+06  2.175894e+06  2.175894e+06  2.175894e+06   \n",
       "mean        1.906973e+00  1.789882e+01  4.474161e-01  1.104702e+05   \n",
       "std         9.032285e-01  3.437274e+01  4.972274e-01  1.971302e+05   \n",
       "min         1.000000e+00 -1.000000e+00  0.000000e+00 -1.000000e+00   \n",
       "25%         1.000000e+00 -1.000000e+00  0.000000e+00 -1.000000e+00   \n",
       "50%         2.000000e+00  1.100000e+01  0.000000e+00 -1.000000e+00   \n",
       "75%         3.000000e+00  2.200000e+01  1.000000e+00  1.493848e+05   \n",
       "max         5.000000e+00  2.009000e+03  1.000000e+00  6.933580e+05   \n",
       "\n",
       "           train_id    desc_words    desc_chars    name_words    name_chars  \\\n",
       "count  2.175894e+06  2.175894e+06  2.175894e+06  2.175894e+06  2.175894e+06   \n",
       "mean   5.050584e+05  4.974276e-01  4.993008e-01  5.008307e-01  5.012269e-01   \n",
       "std    4.940588e+05  2.917889e-01  2.884005e-01  2.854952e-01  2.887067e-01   \n",
       "min   -1.000000e+00  1.000000e-07  1.000000e-07  1.000000e-07  1.000000e-07   \n",
       "25%   -1.000000e+00  2.492492e-01  2.517518e-01  2.272272e-01  2.647648e-01   \n",
       "50%    3.945875e+05  4.959960e-01  5.025025e-01  4.309309e-01  5.030030e-01   \n",
       "75%    9.385608e+05  7.472472e-01  7.482482e-01  8.133133e-01  7.567568e-01   \n",
       "max    1.482534e+06  9.999999e-01  9.999999e-01  9.999999e-01  9.999999e-01   \n",
       "\n",
       "        iphone_case        ...         brand_counts    cat_counts  \\\n",
       "count  2.175894e+06        ...         2.175894e+06  2.175894e+06   \n",
       "mean   4.991512e-03        ...         5.911850e-01  5.002783e-01   \n",
       "std    7.047410e-02        ...         3.745884e-01  2.897944e-01   \n",
       "min    0.000000e+00        ...         1.000000e-07  1.000000e-07   \n",
       "25%    0.000000e+00        ...         2.492492e-01  2.497497e-01   \n",
       "50%    0.000000e+00        ...         4.849850e-01  4.994995e-01   \n",
       "75%    0.000000e+00        ...         9.999999e-01  7.437437e-01   \n",
       "max    1.000000e+00        ...         9.999999e-01  9.999999e-01   \n",
       "\n",
       "        cat1_counts   cat2_counts   cat3_counts   plus_counts   ands_counts  \\\n",
       "count  2.175894e+06  2.175894e+06  2.175894e+06  2.175894e+06  2.175894e+06   \n",
       "mean   6.008656e-01  5.034970e-01  4.996568e-01  1.889075e-02  6.296192e-02   \n",
       "std    3.779273e-01  2.951679e-01  2.896321e-01  1.354933e-01  2.386104e-01   \n",
       "min    1.000000e-07  1.000000e-07  1.000000e-07  1.000000e-07  1.000000e-07   \n",
       "25%    2.557558e-01  2.547548e-01  2.512513e-01  1.000000e-07  1.000000e-07   \n",
       "50%    4.834835e-01  4.924925e-01  4.959960e-01  1.000000e-07  1.000000e-07   \n",
       "75%    9.999999e-01  7.177177e-01  7.507508e-01  1.000000e-07  1.000000e-07   \n",
       "max    9.999999e-01  9.999999e-01  9.999999e-01  9.999999e-01  9.999999e-01   \n",
       "\n",
       "       comma_counts    all_counts  brand_cat_counts  \n",
       "count  2.175894e+06  2.175894e+06      2.175894e+06  \n",
       "mean   2.591100e-01  2.825355e-01      5.008439e-01  \n",
       "std    3.926311e-01  3.969298e-01      2.887984e-01  \n",
       "min    1.000000e-07  1.000000e-07      1.000000e-07  \n",
       "25%    1.000000e-07  1.000000e-07      2.509176e-01  \n",
       "50%    1.000000e-07  1.000000e-07      5.015015e-01  \n",
       "75%    7.597598e-01  7.352352e-01      7.507508e-01  \n",
       "max    9.999999e-01  9.999999e-01      9.999999e-01  \n",
       "\n",
       "[8 rows x 27 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wordbag_data(merge, nrow_train, nrow_test):\n",
    "    start_time = time.time()\n",
    "    print(\"Fit nane dictionary\")\n",
    "    wb_name = wordbatch.WordBatch(normalizeString,  procs=8)\n",
    "    wb_name.fit(merge[\"name\"])\n",
    "    wb_name.dictionary_freeze= True\n",
    "    \n",
    "    print(\"Get name bag of words\")\n",
    "    wb_name_bag = WordBag(wb_name, {\"hash_ngrams\": 2, \"hash_ngrams_weights\": [1.5, 1.0],\n",
    "                                                                  \"hash_size\": 2 ** 29, \"norm\": None, \"tf\": 'binary',\n",
    "                                                                  \"idf\": None,\n",
    "                                                                  })\n",
    "    X_name = wb_name_bag.transform(wb_name.transform(merge['name']))\n",
    "    del wb_name_bag\n",
    "    \n",
    "    X_name = X_name[:, np.array(np.clip(X_name.getnnz(axis=0) - 1, 0, 1), dtype=bool)]\n",
    "    print('[{}] Vectorize `name` completed.'.format(time.time() - start_time))\n",
    "\n",
    "    lb = LabelBinarizer(sparse_output=True)\n",
    "    X_category = lb.fit_transform(merge['category_name'].astype(str))\n",
    "    X_category1 = lb.fit_transform(merge['cat1'].astype(str))\n",
    "    X_category2 = lb.fit_transform(merge['cat2'].astype(str))\n",
    "    X_category3 = lb.fit_transform(merge['cat3'].astype(str))\n",
    "    print('[{}] Label binarize `categories` completed.'.format(time.time() - start_time))\n",
    "\n",
    "    # wb= wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 3, \"hash_ngrams_weights\": [1.0, 1.0, 0.5],\n",
    "    wb_desc = wordbatch.WordBatch(normalizeString  , procs=8)\n",
    "    wb_desc_bag = WordBag(wb_desc, {\"hash_ngrams\": 2, \"hash_ngrams_weights\": [1.0, 1.0],\n",
    "                                                                  \"hash_size\": 2 ** 28, \"norm\": \"l2\", \"tf\": 1.0,\n",
    "                                                                  \"idf\": None})\n",
    "    wb_desc.fit(merge['item_description'].astype(str))\n",
    "    wb_desc.dictionary_freeze= True\n",
    "    X_description = wb_desc_bag.transform(wb_desc.transform(merge['item_description'].astype(str)))\n",
    "    del wb_desc_bag\n",
    "    \n",
    "    X_description = X_description[:, np.array(np.clip(X_description.getnnz(axis=0) - 1, 0, 1), dtype=bool)]\n",
    "    print('[{}] Vectorize `item_description` completed.'.format(time.time() - start_time))\n",
    "\n",
    "    lb = LabelBinarizer(sparse_output=True)\n",
    "    X_brand = lb.fit_transform(merge['brand_name'].astype(str))\n",
    "    print('[{}] Label binarize `brand_name` completed.'.format(time.time() - start_time))\n",
    "\n",
    "    X_dummies = csr_matrix(pd.get_dummies(merge[['item_condition_id', 'shipping']],\n",
    "                                          sparse=True).values)\n",
    "    \n",
    "    num_cols= [\"desc_words\", \"desc_chars\", \"name_words\", \"name_chars\", \"plus_counts\", \n",
    "                    \"ands_counts\", \"comma_counts\", \"all_counts\", \"brand_counts\", \"cat_counts\", \"cat1_counts\", \n",
    "                   \"cat2_counts\", \"cat3_counts\", \"brand_cat_counts\"]\n",
    "    \n",
    "    X_extrafeats = merge[num_cols].values\n",
    "    print('[{}] Get dummies on `item_condition_id` and `shipping` completed.'.format(time.time() - start_time))\n",
    "    print(X_dummies.shape, X_description.shape, X_brand.shape, X_category.shape, X_category1.shape, \n",
    "          X_category2.shape, X_category3.shape, X_name.shape, X_extrafeats.shape)\n",
    "    sparse_merge = hstack((X_dummies, X_description, X_brand, X_category1, X_category2, X_category3, X_name, X_extrafeats)).tocsr()\n",
    "\n",
    "    print('[{}] Create sparse merge completed'.format(time.time() - start_time))\n",
    "\n",
    "    #    pd.to_pickle((sparse_merge, y), \"xy.pkl\")\n",
    "    # else:\n",
    "    #    nrow_train, nrow_test= 1481661, 1482535\n",
    "    #    sparse_merge, y = pd.read_pickle(\"xy.pkl\")\n",
    "\n",
    "    # Remove features with document frequency <=1\n",
    "    print(sparse_merge.shape)\n",
    "    mask = np.array(np.clip(sparse_merge.getnnz(axis=0) - 1, 0, 1), dtype=bool)\n",
    "    sparse_merge = sparse_merge[:, mask]\n",
    "    X = sparse_merge[:nrow_train]\n",
    "    X_test = sparse_merge[nrow_test:]\n",
    "    print(sparse_merge.shape)\n",
    "    \n",
    "    return X, X_test, wb_name, wb_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit nane dictionary\n",
      "Normalize text\n",
      "Get name bag of words\n",
      "Normalize text\n",
      "Extract wordbags\n",
      "[109.32775640487671] Vectorize `name` completed.\n",
      "[208.78752827644348] Label binarize `categories` completed.\n",
      "Normalize text\n",
      "Normalize text\n",
      "Extract wordbags\n",
      "[396.61797189712524] Vectorize `item_description` completed.\n",
      "[493.3160080909729] Label binarize `brand_name` completed.\n",
      "[493.5104818344116] Get dummies on `item_condition_id` and `shipping` completed.\n",
      "(2175894, 2) (2175894, 1987674) (2175894, 4501) (2175894, 1201) (2175894, 11) (2175894, 114) (2175894, 827) (2175894, 570025) (2175894, 14)\n",
      "[596.9384644031525] Create sparse merge completed\n",
      "(2175894, 2563168)\n",
      "(2175894, 2562677)\n"
     ]
    }
   ],
   "source": [
    "X, X_test, wb_name, wb_desc = wordbag_data(data, train_rows, train_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1482535, 2562677)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse for this fold 0.44450421592750194\n",
      "rmse for this fold 0.3547622566468773\n",
      "rmse for this fold 0.3100227578548293\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-26eb77343ecf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtr_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtr_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mval_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mpredsFM\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#ftrl model\n",
    "cvlist = list(KFold(6, random_state=786).split(X))\n",
    "y = np.log1p(train_data[\"price\"]).values\n",
    "predsFM = np.zeros(len(y))\n",
    "\n",
    "for tr_index, val_index in cvlist:\n",
    "    gc.collect()\n",
    "    X_tr, X_val = X[tr_index], X[val_index]\n",
    "    y_tr, y_val = y[tr_index], y[val_index]\n",
    "\n",
    "    model = FM_FTRL(alpha=0.01, beta=0.01, L1=0.00001, L2=0.1, D=X.shape[1], alpha_fm=0.01, L2_fm=0.0, init_fm=0.01,\n",
    "                D_fm=200, e_noise=0.0001, iters=17, inv_link=\"identity\", threads=16)\n",
    "    \n",
    "    model.fit(X_tr, y_tr)\n",
    "    val_preds = model.predict(X_val)\n",
    "    predsFM[val_index] = val_preds\n",
    "    print(\"rmse for this fold\", rmse(y_val, val_preds))\n",
    "\n",
    "print(rmse(y, predsFM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(in_path, out_path):\n",
    "    if False and os.path.exists(os.path.join(out_path, 'train_2.pkl')) and os.path.exists(os.path.join(out_path, 'test_2.pkl')):\n",
    "        train_data = pd.read_pickle(os.path.join(out_path, 'train_2.pkl'))\n",
    "        test_data  = pd.read_pickle(os.path.join(out_path, 'test_2.pkl'))\n",
    "        \n",
    "        return train_data, test_data\n",
    "    \n",
    "    else:\n",
    "        train_data = pd.read_table(os.path.join(in_path, 'train.tsv'))\n",
    "        test_data  = pd.read_table(os.path.join(in_path, 'test.tsv'))\n",
    "    \n",
    "        train_rows = len(train_data)\n",
    "        data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "        \n",
    "        data[\"item_description\"] = data[\"item_description\"].replace(\"No description yet\", \"missing\")\n",
    "        \n",
    "        data['cat1'] = parallelize_dataframe(data[['category_name']], applycat1)\n",
    "        data['cat2'] = parallelize_dataframe(data[['category_name']], applycat2)\n",
    "        data['cat3'] = parallelize_dataframe(data[['category_name']], applycat3)\n",
    "        data.fillna(-1, inplace=True)\n",
    "        \n",
    "        print(\"Getting word/char len features\")\n",
    "        data['desc_words'] = parallelize_dataframe(data['item_description'], get_words)\n",
    "        data['desc_chars'] = parallelize_dataframe(data['item_description'], get_chars)\n",
    "        data['name_words'] = parallelize_dataframe(data['name'], get_words)\n",
    "        data['name_chars'] = parallelize_dataframe(data['name'], get_chars)\n",
    "        \n",
    "        \n",
    "        print(\"Get iphone features\")\n",
    "        data['iphone_case'] = parallelize_dataframe(data['name'], isphonecase)\n",
    "        data['iphone6'] = parallelize_dataframe(data['name'], isiphone6)\n",
    "        data['iphone6p'] = parallelize_dataframe(data['name'], isiphone6p)\n",
    "        data['iphone5'] = parallelize_dataframe(data['name'], isiphone5)\n",
    "        data['iphone5p'] = parallelize_dataframe(data['name'], isiphone5p)\n",
    "        data['iphone7'] = parallelize_dataframe(data['name'], isiphone7)\n",
    "        data['iphone7p'] = parallelize_dataframe(data['name'], isiphone7p)\n",
    "        data['unlocked_phone'] = parallelize_dataframe(data['name'], isunlocked)\n",
    "        \n",
    "        print(\"Get brand words\")\n",
    "        wb_brands = wordbatch.WordBatch(normalizeString, n_words=3000)\n",
    "        wb_brands.fit(data[\"brand_name\"].fillna(\"missing\").astype(str))\n",
    "        \n",
    "        all_brands = set(data['brand_name'].values)\n",
    "        def brandfinder(line):\n",
    "            brand = line[0]\n",
    "            name = line[1]\n",
    "            namesplit = name.split(' ')\n",
    "            if brand == 'missing':\n",
    "                for x in namesplit:\n",
    "                    if x in all_brands:\n",
    "                        return name\n",
    "            else:\n",
    "                return brand\n",
    "            #if name in all_brands:\n",
    "            #    return name\n",
    "            return brand\n",
    "        data['brand_name2'] = data[['brand_name','name']].apply(brandfinder, axis = 1)   \n",
    "        \n",
    "        print(\"Label encoding features\")\n",
    "        cat_cols = ['category_name', 'brand_name', 'brand_name2', 'cat1', 'cat2', 'cat3', 'item_condition_id']\n",
    "        for col in cat_cols:\n",
    "            data[col] = LabelEncoder().fit_transform(data[col].astype(str)) + 1\n",
    "            \n",
    "        print(\"Get count features\")\n",
    "        data['brand_counts'] = data.brand_name.map(data[\"brand_name\"].value_counts()).fillna(0).astype(int)\n",
    "\n",
    "        data['cat_counts'] = data.brand_name.map(data[\"category_name\"].value_counts()).fillna(0).astype(int)\n",
    "        \n",
    "        data['cat1_counts'] = data.brand_name.map(data[\"cat1\"].value_counts()).fillna(0).astype(int)\n",
    "\n",
    "        data['cat2_counts'] = data.brand_name.map(data[\"cat2\"].value_counts()).fillna(0).astype(int)\n",
    "\n",
    "        data['cat3_counts'] = data.brand_name.map(data[\"cat3\"].value_counts()).fillna(0).astype(int)\n",
    "  \n",
    "        \n",
    "        print(\"Getting punct related features\")\n",
    "        data[\"plus_counts\"] = parallelize_dataframe(data[\"item_description\"], plussigns)\n",
    "        data[\"ands_counts\"] = parallelize_dataframe(data[\"item_description\"], andsigns)\n",
    "        data[\"comma_counts\"] = parallelize_dataframe(data[\"item_description\"], commas)\n",
    "        data[\"all_counts\"] = data[\"plus_counts\"] + data[\"ands_counts\"] + data[\"comma_counts\"]\n",
    "        \n",
    "        #for col in [\"name\", \"item_description\"]:\n",
    "        #    data[col] = data[col].str.replace(\"'\", '').replace('-', '').progress_apply(unicodeToAscii)\n",
    "        #    data[col] = data[col].progress_apply(remove_puncts)\n",
    "        \n",
    "        \n",
    "        num_cols =  [\"desc_words\", \"desc_chars\", \"name_words\", \"name_chars\", \"plus_counts\", \n",
    "                    \"ands_counts\", \"comma_counts\", \"all_counts\", \"brand_counts\", \"cat1_counts\", \n",
    "                   \"cat2_counts\", \"cat3_counts\"]\n",
    "        data[num_cols]  = MaxAbsScaler().fit_transform(data[num_cols])\n",
    "            \n",
    "        data[\"brand_cat\"] = data[\"brand_name\"].astype(str) + ' ' + data[\"category_name\"].astype(str)\n",
    "        data[\"category_shipping\"] = data[\"category_name\"].astype(str) + ' ' + data[\"shipping\"].astype(str)\n",
    "        \n",
    "        print(\"transform brand cat and category_shipping\")\n",
    "        data[\"brand_cat\"] = LabelEncoder().fit_transform(data[\"brand_cat\"])\n",
    "        data[\"category_shipping\"] = LabelEncoder().fit_transform(data[\"category_shipping\"])\n",
    "        data['item_desc2gram'] = parallelize_dataframe(data[\"item_description\"], get_2grams)\n",
    "        \n",
    "        print(\"Name to sequences\")\n",
    "        wb_name = wordbatch.WordBatch(normalizeString, n_words=20000)\n",
    "        wb_name.fit(data[\"name\"])\n",
    "        \n",
    "        seq_name = WordSeq(wb_name, {\"seq_maxlen\": 7,  \"seq_truncstart\":False, \"remove_oovs\":True})\n",
    "        #seq_name_desc = WordSeq(wb_name, {\"seq_maxlen\": 30,  \"seq_truncstart\":False, \"remove_oovs\":True})\n",
    "        seq_brands = WordSeq(wb_brands, {\"seq_maxlen\": 2,  \"seq_truncstart\":False, \"remove_oovs\":True})\n",
    "        \n",
    "        #data[\"item_name\"] = list(zip(seq_name_desc.transform(wb_name.transform(data[\"item_description\"].astype(str)))))\n",
    "        data[\"name_brand\"] = list(zip(seq_brands.transform(wb_brands.transform(data[\"name\"].astype(str)))))\n",
    "        data[\"name\"] = list(zip(seq_name.transform(wb_name.transform(data[\"name\"].astype(str)))))\n",
    "        \n",
    "        del wb_name, seq_name, \n",
    "        #seq_name_desc\n",
    "        \n",
    "        print(\"Desc to sequences\")\n",
    "        wb_desc = wordbatch.WordBatch(normalizeString, n_words=50000, extractor=(WordSeq, {\"seq_maxlen\": 70,\n",
    "                                                                                           \"seq_truncstart\":False,\n",
    "                                                                                           \"remove_oovs\":True\n",
    "                                                                            } ))\n",
    "        #wb_desc.fit(data[\"item_description\"].astype(str))\n",
    "        data[\"desc_brand\"] = list(zip(seq_brands.transform(wb_brands.transform(data[\"item_description\"].astype(str)))))\n",
    "        data[\"item_description\"] = list(zip(wb_desc.fit_transform(data[\"item_description\"].astype(str))))\n",
    "        del wb_desc\n",
    "        \n",
    "        print(\"Desc 2gram to sequences\")\n",
    "        #wb_desc2 = wordbatch.WordBatch(normalizeString, n_words=20000, extractor=(WordSeq, {\"seq_maxlen\": 30,\n",
    "        #                                                                                    \"seq_truncstart\":False,\n",
    "        #                                                                                    \"remove_oovs\":True\n",
    "        #                                                                    } ))\n",
    "        #wb_desc2.fit(data[\"item_desc2gram\"].astype(str))\n",
    "        tok_desc2 = Tokenizer(20000)\n",
    "        tok_desc2.fit_on_texts(data['item_desc2gram'].astype(str))\n",
    "        data[\"item_desc2gram\"] = list(zip(sequence.pad_sequences(tok_desc2.texts_to_sequences(data.item_desc2gram.astype(str)),\n",
    "                                         maxlen=20, padding='post', truncating='post')))\n",
    "        #del wb_desc2\n",
    "        \n",
    "        print(\"split train test\")\n",
    "        train_data = data.loc[: train_rows - 1, :].reset_index(drop=True)\n",
    "        train_data = train_data.loc[(train_data.price >= 3) & (train_data.price <= 2000), :].reset_index(drop=True)\n",
    "        test_data  = data.loc[train_rows: , :].reset_index(drop=True)\n",
    "        \n",
    "        print(\"Get target encodings for stuff\")\n",
    "        cvlist = list(KFold(6, random_state=100).split(train_data))\n",
    "        \n",
    "        #enc_1 = TargetEncoder(cols=['brand_name'])\n",
    "        #train_data[\"brand_mean\"] = cross_val_predict(enc_1, train_data[['brand_name']], train_data['price'], cv=cvlist, verbose=10, method='transform', n_jobs=1)\n",
    "        #test_data[\"brand_mean\"] = enc_1.fit(train_data[['brand_name']], train_data['price']).transform(test_data)\n",
    "        \n",
    "        \n",
    "        #enc_2 = TargetEncoder(cols=['category_name'])\n",
    "        #train_data[\"category_mean\"] = cross_val_predict(enc_2, train_data[['category_name']], train_data['price'], cv=cvlist, verbose=10, method='transform', n_jobs=1)\n",
    "        #test_data[\"category_mean\"] = enc_2.fit(train_data[['category_name']], train_data['price']).transform(test_data)\n",
    "        \n",
    "        #enc_3 = TargetEncoder(cols=[['brand_name', 'category_name']])\n",
    "        #train_data[\"brandcat_mean\"] = cross_val_predict(enc_3, train_data[['brand_name', 'category_name']], train_data['price'], cv=cvlist, verbose=10, method='transform', n_jobs=1)\n",
    "        #test_data[\"brandcat_mean\"] = enc_3.fit(train_data[['brand_name', 'category_name']], train_data['price']).transform(test_data)\n",
    "        \n",
    "        #train_data[\"brandcat_rat\"] = train_data[\"brandcat_mean\"]/(1 + train_data[\"category_mean\"])\n",
    "        #test_data[\"brandcat_rat\"] = test_data[\"brandcat_mean\"]/(1 + test_data[\"category_mean\"])\n",
    "        \n",
    "        #train_data[\"catbrand_rat\"] = train_data[\"brandcat_mean\"]/(1 + train_data[\"category_mean\"])\n",
    "        #test_data[\"catbrand_rat\"] = test_data[\"brandcat_mean\"]/(1 + test_data[\"brand_mean\"])\n",
    "        \n",
    "        #enc_4 = TargetEncoder(cols=['brand_name'])\n",
    "        #train_data[\"brandvalue\"] = cross_val_predict(enc_4, train_data[['brand_name']], train_data[\"brandcat_rat\"], cv=cvlist, verbose=10, method='transform', n_jobs=1)\n",
    "        #test_data[\"brandvalue\"] = enc_4.fit(train_data[['brand_name']], train_data[\"brandcat_rat\"]).transform(test_data)\n",
    "        \n",
    "        #extra_cols = [\"brand_mean\",\"category_mean\", \"brandcat_mean\",\"catbrand_rat\", \"brandvalue\"]\n",
    "        #scaler = QuantileTransformer()\n",
    "        #train_data[extra_cols]  = scaler.fit_transform(train_data[extra_cols])\n",
    "        #train_data[extra_cols]  = scaler.transform(train_data[extra_cols])\n",
    "        \n",
    "        print(train_data.head())\n",
    "        del train_data['test_id']\n",
    "        del test_data['train_id']\n",
    "        del data \n",
    "        test_data['test_id'] = test_data['test_id'].astype(int)\n",
    "        train_data.to_pickle(os.path.join(out_path, 'train_2.pkl'))\n",
    "        test_data.to_pickle(os.path.join(out_path, 'test_2.pkl'))\n",
    "        \n",
    "        return train_data, test_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
