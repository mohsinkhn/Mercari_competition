{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1\n",
    "* Tokenizer - Keras\n",
    "* Embeddings training from uniform; separate for each one\n",
    "* ** Optimizers and learning rate - test to check local cv vs lb **\n",
    "* Extra features - None for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohsin/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/mohsin/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import threading\n",
    "import multiprocessing\n",
    "\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "\n",
    "# from __future__ import print_function\n",
    "np.random.seed(786)  # for reproducibility\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D, ZeroPadding1D, AveragePooling1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier,  KerasRegressor\n",
    "#Some classes\n",
    "#Functions we need - Feature Selector, Fasttext_Estimator, Preprocessing Transformer, Binary_Encoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "from pandas.api.types import is_numeric_dtype, is_string_dtype\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    print(np.min(y_pred), np.max(y_pred))\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "rmse_sklearn = make_scorer(rmse, greater_is_better=False)\n",
    "\n",
    "def get_obj_cols(df):\n",
    "    \"\"\"Return columns with object dtypes\"\"\"\n",
    "    obj_cols = []\n",
    "    for idx, dt in enumerate(df.dtypes):\n",
    "        if dt == 'object':\n",
    "            obj_cols.append(df.columns.values[idx])\n",
    "\n",
    "    return obj_cols\n",
    "\n",
    "\n",
    "def convert_input(X):\n",
    "    \"\"\"if input not a dataframe convert it to one\"\"\"\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        if isinstance(X, list):\n",
    "            X = pd.DataFrame(np.array(X))\n",
    "        elif isinstance(X, (np.generic, np.ndarray)):\n",
    "            X = pd.DataFrame(X)\n",
    "        elif isinstance(X, csr_matrix):\n",
    "            X = pd.SparseDataFrame(X)\n",
    "        else:\n",
    "            raise ValueError('Unexpected input type: %s' % (str(type(X))))\n",
    "\n",
    "        #X = X.apply(lambda x: pd.to_numeric(x, errors='ignore'))\n",
    "    return X\n",
    "\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Class to do subset of features in sklearn pipeline\"\"\"\n",
    "    def __init__(self, cols=None, return_df=True, verbose=0):\n",
    "        self.cols = cols\n",
    "        self.return_df = return_df\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        #Do nothing\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        #if the input dataset isn't already a dataframe, convert it to one\n",
    "        X = X.copy(deep=True)\n",
    "        X = convert_input(X)\n",
    "        X = X.loc[:, self.col]\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Selecting columns are {}\".format(self.col))\n",
    "        if self.return_df:\n",
    "            return X\n",
    "        else:\n",
    "            return X.values\n",
    "    \n",
    "      \n",
    "#Data reading function\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "#(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.)\n",
    "\n",
    "def read_data(in_path, out_path):\n",
    "    if False and os.path.exists(os.path.join(out_path, 'train_2.pkl')) and os.path.exists(os.path.join(out_path, 'test_2.pkl')):\n",
    "        train_data = pd.read_pickle(os.path.join(out_path, 'train_2.pkl'))\n",
    "        test_data  = pd.read_pickle(os.path.join(out_path, 'test_2.pkl'))\n",
    "        \n",
    "        return train_data, test_data\n",
    "    \n",
    "    else:\n",
    "        train_data = pd.read_table(os.path.join(in_path, 'train.tsv'))\n",
    "        test_data  = pd.read_table(os.path.join(in_path, 'test.tsv'))\n",
    "    \n",
    "        train_rows = len(train_data)\n",
    "        data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "    \n",
    "        data['cat1'] = data['category_name'].apply(lambda x: str(x).split('/')[0])\n",
    "        data['cat2'] = data['category_name'].apply(lambda x: str(x).split('/')[1] \n",
    "                                                   if len(str(x).split('/')) > 1 else -1)\n",
    "        data['cat3'] = data['category_name'].apply(lambda x: ' '.join(str(x).split('/')[2:]) \n",
    "                                                   if len(str(x).split('/')) > 2 else -1)\n",
    "        data.fillna(-1, inplace=True)\n",
    "        cat_cols = ['category_name', 'brand_name', 'cat1', 'cat2', 'cat3', 'item_condition_id']\n",
    "        for col in cat_cols:\n",
    "            data[col] = LabelEncoder().fit_transform(data[col].astype(str))\n",
    "        \n",
    "        #tkn_desc = Tokenizer(50000)\n",
    "        #tkn_desc.fit_on_texts(data.item_description.astype(str))\n",
    "        #data['desc_seq'] = pad_sequences(tkn_desc.texts_to_sequences(data.item_description.astype(str)),\n",
    "        #                                 maxlen=100, padding='post', truncating='post')\n",
    "        \n",
    "        #tkn_name = Tokenizer(4000)\n",
    "        #tkn_name.fit_on_texts(data.name.astype(str))\n",
    "        #data['name_seq'] = pad_sequences(tkn_name.texts_to_sequences(data.name.astype(str)),\n",
    "        #                                 maxlen=6, padding='post', truncating='post')\n",
    "        \n",
    "        \n",
    "        train_data = data.loc[: train_rows - 1, :].reset_index(drop=True)\n",
    "        train_data = train_data.loc[(train_data.price >= 1) & (train_data.price <= 2000), :].reset_index(drop=True)\n",
    "        test_data  = data.loc[train_rows: , :].reset_index(drop=True)\n",
    "        \n",
    "        del train_data['test_id']\n",
    "        del test_data['train_id']\n",
    "        del data \n",
    "        test_data['test_id'] = test_data['test_id'].astype(int)\n",
    "        #train_data.to_pickle(os.path.join(out_path, 'train_2.pkl'))\n",
    "        #test_data.to_pickle(os.path.join(out_path, 'test_2.pkl'))\n",
    "        \n",
    "        return train_data, test_data\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EM_NNRegressor(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, embed_cols=None, dense_cols=None, embed_dims=None, \n",
    "                 text_embed_cols=None, text_embed_seq_lens=None, \n",
    "                 text_embed_dims=None, text_embed_tokenizers=None,\n",
    "                 num_layers=2, multiprocess=False,\n",
    "                layer_activations=None, layer_dims=None,layer_dropouts=None, epochs=20, batchsize=32,\n",
    "                optimizer_kwargs=None, val_size=0.1, verbose=1, seed=1):\n",
    "        \n",
    "        self.embed_cols = embed_cols\n",
    "        self.dense_cols = dense_cols\n",
    "        self.embed_dims = embed_dims\n",
    "        self.text_embed_cols = text_embed_cols\n",
    "        self.text_embed_dims = text_embed_dims\n",
    "        self.text_embed_tokenizers = text_embed_tokenizers\n",
    "        self.text_embed_seq_lens = text_embed_seq_lens\n",
    "        self.dense_dims = None\n",
    "        self.num_layers = num_layers\n",
    "        self.layer_dims = layer_dims\n",
    "        self.layer_activations = layer_activations\n",
    "        self.layer_dropouts = layer_dropouts\n",
    "        self.epochs = epochs\n",
    "        self.batchsize = batchsize\n",
    "        self.optimizer_kwargs = optimizer_kwargs\n",
    "        self.val_size = val_size\n",
    "        self.verbose = verbose\n",
    "        self.multiprocess = multiprocess\n",
    "        self.seed = seed\n",
    "        self.model = None\n",
    "        if self.dense_cols:\n",
    "            self.dense_dims = len(self.dense_cols)\n",
    "            \n",
    "    def _splitX(self, X):\n",
    "        X_splits = []\n",
    "        \n",
    "        if self.embed_cols:\n",
    "            for col in self.embed_cols :\n",
    "                X_splits.append(X[col].values.reshape(X.shape[0], -1))\n",
    "                \n",
    "        if self.text_embed_cols:\n",
    "            for i, (col, tok) in enumerate(zip(self.text_embed_cols, self.text_embed_tokenizers)):\n",
    "                max_features = self.text_embed_dims[i][0]\n",
    "                max_len = self.text_embed_seq_lens[i]\n",
    "                input_text = X[col].astype(str)\n",
    "                x_train = tok.texts_to_sequences(input_text)\n",
    "                print(np.mean([len(l) for l in x_train]))\n",
    "                x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "                X_splits.append(np.array(x_train).reshape(X.shape[0], -1))\n",
    "                \n",
    "        if self.dense_cols:\n",
    "            X_splits.append(X[self.dense_cols].values.reshape(X.shape[0], -1))\n",
    "            \n",
    "        return X_splits\n",
    "    \n",
    "    \n",
    "    def _build_model(self):\n",
    "        model_inputs = []\n",
    "        model_layers = []\n",
    "        \n",
    "        if self.embed_cols:\n",
    "            for col, dim in zip(self.embed_cols, self.embed_dims):\n",
    "                x1 = Input( shape=(1,), name=col)\n",
    "                model_inputs.append(x1)\n",
    "                x1 = Embedding(input_dim=dim[0], output_dim=dim[1], )(x1)\n",
    "                #x1 = Dropout(0.1)(x1)\n",
    "                x1 = Reshape(target_shape=(dim[1],))(x1)\n",
    "                model_layers.append(x1)\n",
    "                \n",
    "        if self.text_embed_cols:\n",
    "            for col, dim, seq_len in zip(self.text_embed_cols, \n",
    "                                                self.text_embed_dims, \n",
    "                                                self.text_embed_seq_lens):\n",
    "                x3 = Input( shape=(seq_len,))\n",
    "                model_inputs.append(x3)\n",
    "                x3 = Embedding(input_dim=dim[0], output_dim=dim[1], input_length=seq_len)(x3)\n",
    "                x3 = GlobalAveragePooling1D()(x3)\n",
    "                x3 = Reshape(target_shape=(dim[1],))(x3)\n",
    "                model_layers.append(x3)\n",
    "                \n",
    "        if self.dense_cols:\n",
    "            x2 = Input( shape=(self.dense_dims, ), name='dense_cols')\n",
    "            model_inputs.append(x2)\n",
    "            model_layers.append(x2)\n",
    "        print(model_layers)\n",
    "        x = concatenate(model_layers)\n",
    "        \n",
    "        if self.num_layers > 0:\n",
    "            for dim, drops in zip(self.layer_dims, self.layer_dropouts):\n",
    "                x = BatchNormalization()(x)\n",
    "                x = Dropout(rate=drops)(x)\n",
    "                x = Dense(dim, kernel_initializer='he_normal')(x)\n",
    "                x = LeakyReLU()(x)\n",
    "        \n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.05)(x)\n",
    "        output = Dense(1, activation='linear', kernel_initializer='normal')(x)\n",
    "        \n",
    "        model = Model(inputs=model_inputs, outputs=output)\n",
    "        #print(model.summary())\n",
    "        adam = Nadam(lr=0.0012, schedule_decay=0.001)\n",
    "        #adam = Adam(lr=0.003, decay=0.003)\n",
    "        #adam = SGD(lr=0.01, nesterov=True, momentum=0.9, decay=0.003)\n",
    "        #adam = RMSprop(lr=0.004, decay=0.004)\n",
    "        model.compile(optimizer=adam, loss='mean_squared_error' )\n",
    "        \n",
    "        return model \n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model = self._build_model()\n",
    "        if self.val_size > 0:\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.val_size, random_state=self.seed)\n",
    "            print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "            \n",
    "            callbacks= [ModelCheckpoint(\"embed_NN_\"+str(self.seed)+\".check\", save_best_only=True, verbose=1)]\n",
    "            if self.multiprocess == False:\n",
    "                self.model.fit(self._splitX(X_train), y_train, batch_size=self.batchsize, epochs=self.epochs,\n",
    "                               verbose=self.verbose,\n",
    "                              validation_data=(self._splitX(X_val), y_val), shuffle=True,\n",
    "                              callbacks=callbacks)\n",
    "            else:\n",
    "                X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.val_size, random_state=1)\n",
    "\n",
    "        else:\n",
    "            self.model.fit(self._splitX(X), y, batch_size=self.batchsize, epochs=self.epochs,\n",
    "               verbose=self.verbose, shuffle=True)\n",
    "\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        \n",
    "        if self.model:\n",
    "            model = load_model(\"embed_NN_\"+str(self.seed)+\".check\")\n",
    "            y_hat = model.predict(self._splitX(X))\n",
    "        else:\n",
    "            raise ValueError(\"Model not fit yet\")\n",
    "            \n",
    "        return y_hat\n",
    "        \n",
    "def add_ngrams(text, ngram=2):\n",
    "    word_list = str(text).lower().split(' ')\n",
    "    out_list = [''.join(word_list[i:i+ngram]) for i in range(len(word_list))]\n",
    "    return ' '.join(out_list[:-1])\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1481658, 11) (693359, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>item_description</th>\n",
       "      <th>name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>train_id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>830</td>\n",
       "      <td>2</td>\n",
       "      <td>No description yet</td>\n",
       "      <td>MLB Cincinnati Reds T Shirt Size XL</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>103</td>\n",
       "      <td>774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3890</td>\n",
       "      <td>87</td>\n",
       "      <td>2</td>\n",
       "      <td>This keyboard is in great condition and works ...</td>\n",
       "      <td>Razer BlackWidow Chroma Keyboard</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4589</td>\n",
       "      <td>1278</td>\n",
       "      <td>0</td>\n",
       "      <td>Adorable top with a hint of lace and a key hol...</td>\n",
       "      <td>AVA-VIV Blouse</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9</td>\n",
       "      <td>104</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>504</td>\n",
       "      <td>0</td>\n",
       "      <td>New with tags. Leather horses. Retail for [rm]...</td>\n",
       "      <td>Leather Horse Statues</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1205</td>\n",
       "      <td>0</td>\n",
       "      <td>Complete with certificate of authenticity</td>\n",
       "      <td>24K GOLD plated rose</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9</td>\n",
       "      <td>59</td>\n",
       "      <td>543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   brand_name  category_name  item_condition_id  \\\n",
       "0           2            830                  2   \n",
       "1        3890             87                  2   \n",
       "2        4589           1278                  0   \n",
       "3           2            504                  0   \n",
       "4           2           1205                  0   \n",
       "\n",
       "                                    item_description  \\\n",
       "0                                 No description yet   \n",
       "1  This keyboard is in great condition and works ...   \n",
       "2  Adorable top with a hint of lace and a key hol...   \n",
       "3  New with tags. Leather horses. Retail for [rm]...   \n",
       "4          Complete with certificate of authenticity   \n",
       "\n",
       "                                  name  price  shipping  train_id  cat1  cat2  \\\n",
       "0  MLB Cincinnati Reds T Shirt Size XL   10.0         1       0.0     5   103   \n",
       "1     Razer BlackWidow Chroma Keyboard   52.0         0       1.0     1    31   \n",
       "2                       AVA-VIV Blouse   10.0         1       2.0     9   104   \n",
       "3                Leather Horse Statues   35.0         1       3.0     3    56   \n",
       "4                 24K GOLD plated rose   44.0         0       4.0     9    59   \n",
       "\n",
       "   cat3  \n",
       "0   774  \n",
       "1   216  \n",
       "2    98  \n",
       "3   411  \n",
       "4   543  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read data\n",
    "train_data, test_data = read_data(\"../input\", \"./\")\n",
    "print(train_data.shape, test_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['item_desc2gram'] = train_data.item_description.apply(lambda x: add_ngrams(x, 2))\n",
    "test_data['item_desc2gram'] = test_data.item_description.apply(lambda x: add_ngrams(x, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1481658, 12) (693359, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>item_description</th>\n",
       "      <th>name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>train_id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>item_desc2gram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>830</td>\n",
       "      <td>2</td>\n",
       "      <td>No description yet</td>\n",
       "      <td>MLB Cincinnati Reds T Shirt Size XL</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>103</td>\n",
       "      <td>774</td>\n",
       "      <td>nodescription descriptionyet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3890</td>\n",
       "      <td>87</td>\n",
       "      <td>2</td>\n",
       "      <td>This keyboard is in great condition and works ...</td>\n",
       "      <td>Razer BlackWidow Chroma Keyboard</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>216</td>\n",
       "      <td>thiskeyboard keyboardis isin ingreat greatcond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4589</td>\n",
       "      <td>1278</td>\n",
       "      <td>0</td>\n",
       "      <td>Adorable top with a hint of lace and a key hol...</td>\n",
       "      <td>AVA-VIV Blouse</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9</td>\n",
       "      <td>104</td>\n",
       "      <td>98</td>\n",
       "      <td>adorabletop topwith witha ahint hintof oflace ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>504</td>\n",
       "      <td>0</td>\n",
       "      <td>New with tags. Leather horses. Retail for [rm]...</td>\n",
       "      <td>Leather Horse Statues</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>411</td>\n",
       "      <td>newwith withtags. tags.leather leatherhorses. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1205</td>\n",
       "      <td>0</td>\n",
       "      <td>Complete with certificate of authenticity</td>\n",
       "      <td>24K GOLD plated rose</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9</td>\n",
       "      <td>59</td>\n",
       "      <td>543</td>\n",
       "      <td>completewith withcertificate certificateof ofa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   brand_name  category_name  item_condition_id  \\\n",
       "0           2            830                  2   \n",
       "1        3890             87                  2   \n",
       "2        4589           1278                  0   \n",
       "3           2            504                  0   \n",
       "4           2           1205                  0   \n",
       "\n",
       "                                    item_description  \\\n",
       "0                                 No description yet   \n",
       "1  This keyboard is in great condition and works ...   \n",
       "2  Adorable top with a hint of lace and a key hol...   \n",
       "3  New with tags. Leather horses. Retail for [rm]...   \n",
       "4          Complete with certificate of authenticity   \n",
       "\n",
       "                                  name  price  shipping  train_id  cat1  cat2  \\\n",
       "0  MLB Cincinnati Reds T Shirt Size XL   10.0         1       0.0     5   103   \n",
       "1     Razer BlackWidow Chroma Keyboard   52.0         0       1.0     1    31   \n",
       "2                       AVA-VIV Blouse   10.0         1       2.0     9   104   \n",
       "3                Leather Horse Statues   35.0         1       3.0     3    56   \n",
       "4                 24K GOLD plated rose   44.0         0       4.0     9    59   \n",
       "\n",
       "   cat3                                     item_desc2gram  \n",
       "0   774                       nodescription descriptionyet  \n",
       "1   216  thiskeyboard keyboardis isin ingreat greatcond...  \n",
       "2    98  adorabletop topwith witha ahint hintof oflace ...  \n",
       "3   411  newwith withtags. tags.leather leatherhorses. ...  \n",
       "4   543  completewith withcertificate certificateof ofa...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_data.shape, test_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing data\")\n",
    "tok_name  = Tokenizer(20000)\n",
    "tok_name.fit_on_texts(pd.concat([train_data, test_data])['name'].astype(str))\n",
    "\n",
    "tok_desc= Tokenizer(50000)\n",
    "tok_desc.fit_on_texts(pd.concat([train_data, test_data])['item_description'].astype(str))\n",
    "\n",
    "tok_desc2 = Tokenizer(20000)\n",
    "tok_desc2.fit_on_texts(pd.concat([train_data, test_data])['item_desc2gram'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data\n",
    "y = np.log1p(train_data.price)\n",
    "\n",
    "cvlist= list(KFold(5, random_state=786).split(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_109/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_110/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_111/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_112/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_113/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_114/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_115/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_116/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_117/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_22:0' shape=(?, 1) dtype=float32>]\n",
      "(1161619, 12) (23707, 12) (1161619,) (23707,)\n",
      "4.334399661162567\n",
      "25.62508447261968\n",
      "19.236752325848666\n",
      "4.335765807567385\n",
      "25.815370987472054\n",
      "19.345889399755347\n",
      "Train on 1161619 samples, validate on 23707 samples\n",
      "Epoch 1/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.4262Epoch 00001: val_loss improved from inf to 0.34718, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 14s 12us/step - loss: 0.4258 - val_loss: 0.3472\n",
      "Epoch 2/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.3068Epoch 00002: val_loss improved from 0.34718 to 0.28262, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 9s 8us/step - loss: 0.3067 - val_loss: 0.2826\n",
      "Epoch 3/5\n",
      "1155072/1161619 [============================>.] - ETA: 0s - loss: 0.2968Epoch 00003: val_loss improved from 0.28262 to 0.27591, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 9s 8us/step - loss: 0.2968 - val_loss: 0.2759\n",
      "Epoch 4/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.2915Epoch 00004: val_loss improved from 0.27591 to 0.27135, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 9s 8us/step - loss: 0.2915 - val_loss: 0.2714\n",
      "Epoch 5/5\n",
      "1155072/1161619 [============================>.] - ETA: 0s - loss: 0.2878Epoch 00005: val_loss improved from 0.27135 to 0.26841, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 9s 8us/step - loss: 0.2878 - val_loss: 0.2684\n",
      "4.3303051982236145\n",
      "25.599698986272156\n",
      "19.212106691143717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_118/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_119/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_120/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_121/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_122/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_123/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_124/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_125/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_126/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_24:0' shape=(?, 1) dtype=float32>]\n",
      "(1161619, 12) (23707, 12) (1161619,) (23707,)\n",
      "4.332856986671189\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-320697b54593>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                  )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0moof_preds1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnnet1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcvlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moof_preds1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_predict\u001b[0;34m(estimator, X, y, groups, cv, n_jobs, verbose, fit_params, pre_dispatch, method)\u001b[0m\n\u001b[1;32m    678\u001b[0m     prediction_blocks = parallel(delayed(_fit_and_predict)(\n\u001b[1;32m    679\u001b[0m         clone(estimator), X, y, train, test, verbose, fit_params, method)\n\u001b[0;32m--> 680\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;31m# Concatenate the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_predict\u001b[0;34m(estimator, X, y, train, test, verbose, fit_params, method)\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-72aa5cf0b77b>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"embed_NN_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".check\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiprocess\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 self.model.fit(self._splitX(X_train), y_train, batch_size=self.batchsize, epochs=self.epochs,\n\u001b[0m\u001b[1;32m    118\u001b[0m                                \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_splitX\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-72aa5cf0b77b>\u001b[0m in \u001b[0;36m_splitX\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                 \u001b[0mX_splits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/preprocessing/sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# check `trunc` has expected shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mtrunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             raise ValueError('Shape of sample %s of sequence at position %s is different from expected shape %s' %\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nnet1 = EM_NNRegressor(embed_cols=['brand_name','category_name','item_condition_id', 'cat1', 'cat2', 'cat3'], \n",
    "                  embed_dims=[(6000, 40),(1500, 30), (5,4), (15,4), (120, 10), (900, 20)],\n",
    "                  text_embed_cols=['name', 'item_description', 'item_desc2gram'],\n",
    "                  text_embed_dims=[(20000, 50), (50000, 50), (20000, 50)],\n",
    "                  text_embed_seq_lens =[7, 60, 30],\n",
    "                  text_embed_tokenizers = [tok_name, tok_desc, tok_desc2],\n",
    "                  dense_cols=['shipping'],\n",
    "                  epochs=5,\n",
    "                  batchsize=2048 ,\n",
    "                  num_layers = 1,\n",
    "                  layer_dropouts=[0.2],\n",
    "                  layer_dims=[200],\n",
    "                  seed=1,\n",
    "                  val_size=0.02,\n",
    "                 )\n",
    "\n",
    "oof_preds1 = cross_val_predict(nnet1, X, y, verbose=10, cv=cvlist)\n",
    "score = rmse(y, oof_preds1)\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_496/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_497/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_498/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_499/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_500/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_501/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_502/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_503/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_504/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_110:0' shape=(?, 1) dtype=float32>]\n",
      "(1161619, 12) (23707, 12) (1161619,) (23707,)\n",
      "4.33439966116\n",
      "25.6250844726\n",
      "19.2367523258\n",
      "4.33576580757\n",
      "25.8153709875\n",
      "19.3458893998\n",
      "Train on 1161619 samples, validate on 23707 samples\n",
      "Epoch 1/4\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.5893Epoch 00001: val_loss improved from inf to 0.29713, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 36s 31us/step - loss: 0.5879 - val_loss: 0.2971\n",
      "Epoch 2/4\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.2035Epoch 00002: val_loss improved from 0.29713 to 0.19204, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 9us/step - loss: 0.2035 - val_loss: 0.1920\n",
      "Epoch 3/4\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1852Epoch 00003: val_loss did not improve\n",
      "1161619/1161619 [==============================] - 11s 9us/step - loss: 0.1852 - val_loss: 0.1962\n",
      "Epoch 4/4\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1755Epoch 00004: val_loss improved from 0.19204 to 0.18471, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.1755 - val_loss: 0.1847\n",
      "4.33030519822\n",
      "25.5996989863\n",
      "19.2121066911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_505/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_506/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_507/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_508/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_509/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_510/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_511/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_512/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_513/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_112:0' shape=(?, 1) dtype=float32>]\n",
      "(1161619, 12) (23707, 12) (1161619,) (23707,)\n",
      "4.33285698667\n",
      "25.6232783727\n",
      "19.2414698795\n",
      "4.33572362593\n",
      "25.7972328848\n",
      "19.3686674822\n",
      "Train on 1161619 samples, validate on 23707 samples\n",
      "Epoch 1/4\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.5945Epoch 00001: val_loss improved from inf to 0.32895, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 36s 31us/step - loss: 0.5930 - val_loss: 0.3290\n",
      "Epoch 2/4\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.2035Epoch 00002: val_loss improved from 0.32895 to 0.19660, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 9us/step - loss: 0.2034 - val_loss: 0.1966\n",
      "Epoch 3/4\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1853Epoch 00003: val_loss improved from 0.19660 to 0.18831, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.1853 - val_loss: 0.1883\n",
      "Epoch 4/4\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1758Epoch 00004: val_loss improved from 0.18831 to 0.18689, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.1758 - val_loss: 0.1869\n",
      "4.33635584412\n",
      "25.6082299583\n",
      "19.1917916391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  8.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_514/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_515/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_516/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_517/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_518/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_519/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_520/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_521/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_522/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_114:0' shape=(?, 1) dtype=float32>]\n",
      "(1161619, 12) (23707, 12) (1161619,) (23707,)\n",
      "4.33297148204\n",
      "25.6027363533\n",
      "19.2158289422\n",
      "4.32711857257\n",
      "25.9209937993\n",
      "19.5040705277\n",
      "Train on 1161619 samples, validate on 23707 samples\n",
      "Epoch 1/4\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.5976Epoch 00001: val_loss improved from inf to 0.30428, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 36s 31us/step - loss: 0.5962 - val_loss: 0.3043\n",
      "Epoch 2/4\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.2036Epoch 00002: val_loss improved from 0.30428 to 0.19889, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.2035 - val_loss: 0.1989\n",
      "Epoch 3/4\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1849Epoch 00003: val_loss improved from 0.19889 to 0.18879, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.1849 - val_loss: 0.1888\n",
      "Epoch 4/4\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1753Epoch 00004: val_loss improved from 0.18879 to 0.18642, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.1753 - val_loss: 0.1864\n",
      "4.33659544025\n",
      "25.6788534482\n",
      "19.2814714577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 12.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_523/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_524/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_525/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_526/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_527/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_528/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_529/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_530/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_531/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_116:0' shape=(?, 1) dtype=float32>]\n",
      "(1161620, 12) (23707, 12) (1161620,) (23707,)\n",
      "4.33419706961\n",
      "25.6193746664\n",
      "19.2258079234\n",
      "4.33880288522\n",
      "25.7503269077\n",
      "19.3687518454\n",
      "Train on 1161620 samples, validate on 23707 samples\n",
      "Epoch 1/4\n",
      "1161216/1161620 [============================>.] - ETA: 0s - loss: 0.6026Epoch 00001: val_loss improved from inf to 0.26973, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 38s 33us/step - loss: 0.6024 - val_loss: 0.2697\n",
      "Epoch 2/4\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.2033Epoch 00002: val_loss improved from 0.26973 to 0.19224, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 11s 10us/step - loss: 0.2032 - val_loss: 0.1922\n",
      "Epoch 3/4\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.1856Epoch 00003: val_loss improved from 0.19224 to 0.18860, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 11s 10us/step - loss: 0.1856 - val_loss: 0.1886\n",
      "Epoch 4/4\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.1754Epoch 00004: val_loss did not improve\n",
      "1161620/1161620 [==============================] - 11s 9us/step - loss: 0.1754 - val_loss: 0.1900\n",
      "4.33085637345\n",
      "25.6272850292\n",
      "19.2531797213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 17.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_532/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_533/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_534/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_535/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_536/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_537/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_538/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_539/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_540/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_118:0' shape=(?, 1) dtype=float32>]\n",
      "(1161620, 12) (23707, 12) (1161620,) (23707,)\n",
      "4.33361512371\n",
      "25.6282691414\n",
      "19.2341006525\n",
      "4.32926983591\n",
      "25.640654659\n",
      "19.2609355886\n",
      "Train on 1161620 samples, validate on 23707 samples\n",
      "Epoch 1/4\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.5891Epoch 00001: val_loss improved from inf to 0.25521, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 37s 32us/step - loss: 0.5876 - val_loss: 0.2552\n",
      "Epoch 2/4\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.2037Epoch 00002: val_loss improved from 0.25521 to 0.21770, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 11s 9us/step - loss: 0.2037 - val_loss: 0.2177\n",
      "Epoch 3/4\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.1857Epoch 00003: val_loss improved from 0.21770 to 0.18642, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 11s 9us/step - loss: 0.1857 - val_loss: 0.1864\n",
      "Epoch 4/4\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.1757Epoch 00004: val_loss improved from 0.18642 to 0.18569, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 11s 9us/step - loss: 0.1757 - val_loss: 0.1857\n",
      "4.33390026693\n",
      "25.6011925853\n",
      "19.2292976435\n",
      "0.429582845457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 21.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 21.4min finished\n"
     ]
    }
   ],
   "source": [
    "nnet2 = EM_NNRegressor(embed_cols=['brand_name','category_name','item_condition_id', 'cat1', 'cat2', 'cat3'], \n",
    "                  embed_dims=[(6000, 30),(1500, 25), (5,4), (15,4), (120, 10), (900, 20)],\n",
    "                  text_embed_cols=['name', 'item_description', 'item_desc2gram'],\n",
    "                  text_embed_dims=[(20000, 30), (50000, 30), (20000, 30)],\n",
    "                  text_embed_seq_lens =[7, 70, 30],\n",
    "                  text_embed_tokenizers = [tok_name, tok_desc, tok_desc2],\n",
    "                  dense_cols=['shipping'],\n",
    "                  epochs=4,\n",
    "                  batchsize=2048 ,\n",
    "                  num_layers = 1,\n",
    "                  layer_dropouts=[0.2],\n",
    "                  layer_dims=[100],\n",
    "                  seed=2,\n",
    "                  val_size=0.02\n",
    "                 )\n",
    "oof_preds2 = cross_val_predict(nnet1, X, y, verbose=10, cv=cvlist)\n",
    "score = rmse(y, oof_preds2)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_541/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_542/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_543/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_544/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_545/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_546/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_547/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_548/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_549/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_120:0' shape=(?, 1) dtype=float32>]\n",
      "(1161619, 12) (23707, 12) (1161619,) (23707,)\n",
      "4.33439966116\n",
      "25.6250844726\n",
      "19.2367523258\n",
      "4.33576580757\n",
      "25.8153709875\n",
      "19.3458893998\n",
      "Train on 1161619 samples, validate on 23707 samples\n",
      "Epoch 1/4\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.5807Epoch 00001: val_loss improved from inf to 0.27725, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 37s 32us/step - loss: 0.5793 - val_loss: 0.2773\n",
      "Epoch 2/4\n",
      "1159168/1161619 [============================>.] - ETA: 0s - loss: 0.2030Epoch 00002: val_loss improved from 0.27725 to 0.21691, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 9us/step - loss: 0.2030 - val_loss: 0.2169\n",
      "Epoch 3/4\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1848Epoch 00003: val_loss improved from 0.21691 to 0.19366, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 9us/step - loss: 0.1847 - val_loss: 0.1937\n",
      "Epoch 4/4\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1750Epoch 00004: val_loss improved from 0.19366 to 0.18370, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 9us/step - loss: 0.1750 - val_loss: 0.1837\n",
      "4.33030519822\n",
      "25.5996989863\n",
      "19.2121066911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_550/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_551/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_552/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_553/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_554/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_555/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_556/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_557/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_558/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_122:0' shape=(?, 1) dtype=float32>]\n",
      "(1161619, 12) (23707, 12) (1161619,) (23707,)\n",
      "4.33285698667\n",
      "25.6232783727\n",
      "19.2414698795\n",
      "4.33572362593\n",
      "25.7972328848\n",
      "19.3686674822\n",
      "Train on 1161619 samples, validate on 23707 samples\n",
      "Epoch 1/4\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.5911Epoch 00001: val_loss improved from inf to 0.26756, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 38s 32us/step - loss: 0.5896 - val_loss: 0.2676\n",
      "Epoch 2/4\n",
      "1159168/1161619 [============================>.] - ETA: 0s - loss: 0.2032Epoch 00002: val_loss improved from 0.26756 to 0.19279, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 9us/step - loss: 0.2032 - val_loss: 0.1928\n",
      "Epoch 3/4\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1852Epoch 00003: val_loss improved from 0.19279 to 0.19060, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 9us/step - loss: 0.1852 - val_loss: 0.1906\n",
      "Epoch 4/4\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1754Epoch 00004: val_loss improved from 0.19060 to 0.18496, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 9us/step - loss: 0.1754 - val_loss: 0.1850\n",
      "4.33635584412\n",
      "25.6082299583\n",
      "19.1917916391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  8.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_559/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_560/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_561/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_562/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_563/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_564/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_565/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_566/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_567/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_124:0' shape=(?, 1) dtype=float32>]\n",
      "(1161619, 12) (23707, 12) (1161619,) (23707,)\n",
      "4.33297148204\n",
      "25.6027363533\n",
      "19.2158289422\n",
      "4.32711857257\n",
      "25.9209937993\n",
      "19.5040705277\n",
      "Train on 1161619 samples, validate on 23707 samples\n",
      "Epoch 1/4\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.5893Epoch 00001: val_loss improved from inf to 0.27713, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 39s 33us/step - loss: 0.5879 - val_loss: 0.2771\n",
      "Epoch 2/4\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.2035Epoch 00002: val_loss improved from 0.27713 to 0.19449, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.2035 - val_loss: 0.1945\n",
      "Epoch 3/4\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1849Epoch 00003: val_loss improved from 0.19449 to 0.18611, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.1849 - val_loss: 0.1861\n",
      "Epoch 4/4\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1749Epoch 00004: val_loss did not improve\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.1749 - val_loss: 0.2004\n",
      "4.33659544025\n",
      "25.6788534482\n",
      "19.2814714577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 12.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_568/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_569/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_570/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_571/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_572/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_573/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_574/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_575/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_576/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_126:0' shape=(?, 1) dtype=float32>]\n",
      "(1161620, 12) (23707, 12) (1161620,) (23707,)\n",
      "4.33419706961\n",
      "25.6193746664\n",
      "19.2258079234\n",
      "4.33880288522\n",
      "25.7503269077\n",
      "19.3687518454\n",
      "Train on 1161620 samples, validate on 23707 samples\n",
      "Epoch 1/4\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.5954Epoch 00001: val_loss improved from inf to 0.26626, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 39s 34us/step - loss: 0.5939 - val_loss: 0.2663\n",
      "Epoch 2/4\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.2035Epoch 00002: val_loss improved from 0.26626 to 0.19951, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 11s 10us/step - loss: 0.2035 - val_loss: 0.1995\n",
      "Epoch 3/4\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.1850Epoch 00003: val_loss improved from 0.19951 to 0.19285, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 11s 10us/step - loss: 0.1850 - val_loss: 0.1929\n",
      "Epoch 4/4\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.1756Epoch 00004: val_loss improved from 0.19285 to 0.18593, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 11s 10us/step - loss: 0.1756 - val_loss: 0.1859\n",
      "4.33085637345\n",
      "25.6272850292\n",
      "19.2531797213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 17.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_577/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_578/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_579/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_580/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_581/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_582/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_583/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_584/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_585/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_128:0' shape=(?, 1) dtype=float32>]\n",
      "(1161620, 12) (23707, 12) (1161620,) (23707,)\n",
      "4.33361512371\n",
      "25.6282691414\n",
      "19.2341006525\n",
      "4.32926983591\n",
      "25.640654659\n",
      "19.2609355886\n",
      "Train on 1161620 samples, validate on 23707 samples\n",
      "Epoch 1/4\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.5868Epoch 00001: val_loss improved from inf to 0.27031, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 39s 34us/step - loss: 0.5855 - val_loss: 0.2703\n",
      "Epoch 2/4\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.2034Epoch 00002: val_loss improved from 0.27031 to 0.19772, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 11s 10us/step - loss: 0.2034 - val_loss: 0.1977\n",
      "Epoch 3/4\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.1850Epoch 00003: val_loss improved from 0.19772 to 0.18492, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 11s 10us/step - loss: 0.1850 - val_loss: 0.1849\n",
      "Epoch 4/4\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.1752Epoch 00004: val_loss improved from 0.18492 to 0.18116, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 11s 10us/step - loss: 0.1751 - val_loss: 0.1812\n",
      "4.33390026693\n",
      "25.6011925853\n",
      "19.2292976435\n",
      "0.427506540433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 21.5min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 21.5min finished\n"
     ]
    }
   ],
   "source": [
    "nnet3 = EM_NNRegressor(embed_cols=['brand_name','category_name','item_condition_id', 'cat1', 'cat2', 'cat3'], \n",
    "                  embed_dims=[(6000, 30),(1500, 20), (5,4), (15,4), (120, 10), (900, 20)],\n",
    "                  text_embed_cols=['name', 'item_description', 'item_desc2gram'],\n",
    "                  text_embed_dims=[(20000, 50), (50000, 50), (20000, 50)],\n",
    "                  text_embed_seq_lens =[7, 60, 30],\n",
    "                  text_embed_tokenizers = [tok_name, tok_desc, tok_desc2],\n",
    "                  dense_cols=['shipping'],\n",
    "                  epochs=4,\n",
    "                  batchsize=2048 ,\n",
    "                  num_layers = 1,\n",
    "                  layer_dropouts=[0.2],\n",
    "                  layer_dims=[200],\n",
    "                  seed=3,\n",
    "                  val_size=0.02,\n",
    "                 )\n",
    "oof_preds3 = cross_val_predict(nnet1, X, y, verbose=10, cv=cvlist)\n",
    "score = rmse(y, oof_preds3)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import hmean, gmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1481658,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.42114728017492642"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_preds = np.mean(np.hstack((oof_preds1, oof_preds2, oof_preds3)), axis=1)\n",
    "print(oof_preds.shape)\n",
    "rmse(y, oof_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With Nadam lr=0.015, decay=0.005 - mean of 3 models is 0.4185 (0.429, 0.429, 0.428)\n",
    "#RMSProp lr=0.003, decay=0.005 - mean 0.420 (0.425, 0.426, 0.424)\n",
    "#RMSProp lr=0.01, decay=0.005 - mean 0.425 (0.434, 0.431, 0.430)\n",
    "#RMSProp lr=0.004, decay=0.004 - mean 0.421 (0.428, 0.429, 0.427) \n",
    "#Adam lr=0.003, decay=0.003 - mean -- (0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet1.fit(train_data, np.log1p(train_data.price) )\n",
    "print(\"Predicting on test data\")\n",
    "test_preds1 = nnet1.predict(test_data)\n",
    "\n",
    "nnet2.fit(train_data, np.log1p(train_data.price) )\n",
    "print(\"Predicting on test data\")\n",
    "test_preds2 = nnet2.predict(test_data)\n",
    "\n",
    "nnet3.fit(train_data, np.log1p(train_data.price) )\n",
    "print(\"Predicting on test data\")\n",
    "test_preds3 = nnet3.predict(test_data)\n",
    "\n",
    "test_preds = (1/3)*(test_preds1 + test_preds2 + test_preds3)\n",
    "print(\"Write out submission\")\n",
    "submission: pd.DataFrame = test_data[['test_id']]\n",
    "submission['price'] = np.expm1(test_preds)\n",
    "submission.price = submission.price.clip(3, 2000)\n",
    "submission.to_csv(\"embedding_nn_v2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
