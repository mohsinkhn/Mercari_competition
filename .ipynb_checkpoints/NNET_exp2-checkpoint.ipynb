{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2\n",
    "* Initialize with pretrained weights\n",
    "    - Try different pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohsin/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import threading\n",
    "import multiprocessing\n",
    "\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "\n",
    "# from __future__ import print_function\n",
    "np.random.seed(786)  # for reproducibility\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D, ZeroPadding1D, AveragePooling1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier,  KerasRegressor\n",
    "#Some classes\n",
    "#Functions we need - Feature Selector, Fasttext_Estimator, Preprocessing Transformer, Binary_Encoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "from pandas.api.types import is_numeric_dtype, is_string_dtype\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    print(np.min(y_pred), np.max(y_pred))\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "rmse_sklearn = make_scorer(rmse, greater_is_better=False)\n",
    "\n",
    "def get_obj_cols(df):\n",
    "    \"\"\"Return columns with object dtypes\"\"\"\n",
    "    obj_cols = []\n",
    "    for idx, dt in enumerate(df.dtypes):\n",
    "        if dt == 'object':\n",
    "            obj_cols.append(df.columns.values[idx])\n",
    "\n",
    "    return obj_cols\n",
    "\n",
    "\n",
    "def convert_input(X):\n",
    "    \"\"\"if input not a dataframe convert it to one\"\"\"\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        if isinstance(X, list):\n",
    "            X = pd.DataFrame(np.array(X))\n",
    "        elif isinstance(X, (np.generic, np.ndarray)):\n",
    "            X = pd.DataFrame(X)\n",
    "        elif isinstance(X, csr_matrix):\n",
    "            X = pd.SparseDataFrame(X)\n",
    "        else:\n",
    "            raise ValueError('Unexpected input type: %s' % (str(type(X))))\n",
    "\n",
    "        #X = X.apply(lambda x: pd.to_numeric(x, errors='ignore'))\n",
    "    return X\n",
    "\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Class to do subset of features in sklearn pipeline\"\"\"\n",
    "    def __init__(self, cols=None, return_df=True, verbose=0):\n",
    "        self.cols = cols\n",
    "        self.return_df = return_df\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        #Do nothing\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        #if the input dataset isn't already a dataframe, convert it to one\n",
    "        X = X.copy(deep=True)\n",
    "        X = convert_input(X)\n",
    "        X = X.loc[:, self.col]\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Selecting columns are {}\".format(self.col))\n",
    "        if self.return_df:\n",
    "            return X\n",
    "        else:\n",
    "            return X.values\n",
    "        \n",
    "class TargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols=None, thresh=0, func=np.mean, add_to_orig=False):\n",
    "        self.cols = cols\n",
    "        self.thresh = thresh\n",
    "        self.func = func\n",
    "        self.add_to_orig = add_to_orig\n",
    "    \n",
    "    #@numba.jit        \n",
    "    def fit(self, X, y):\n",
    "        self.prior = self.func(y)\n",
    "        self._dict = {}\n",
    "        for col in self.cols:\n",
    "            if isinstance(col, (list, tuple)):\n",
    "                print('here')\n",
    "                tmp_df = X.loc[: ,col]\n",
    "                col = tuple(col)\n",
    "            else:\n",
    "                tmp_df = X.loc[: ,[col]]\n",
    "            tmp_df['y'] = y\n",
    "            print(tmp_df.columns)\n",
    "            #tmp_df = pd.DataFrame({'eval_col':X[col].values, 'y':y})\n",
    "            if isinstance(col, (list, tuple)):\n",
    "                print('here')\n",
    "                col = tuple(col)\n",
    "            self._dict[col] = tmp_df.groupby(col)['y'].apply(lambda x: \n",
    "                                self.func(x) if len(x) >= self.thresh  else self.prior).to_dict()\n",
    "                                \n",
    "            del tmp_df\n",
    "        return self\n",
    "    #@numba.jit\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for col in self.cols:\n",
    "            \n",
    "            if isinstance(col, (list, tuple)):\n",
    "                tmp_df = X.loc[:, col]\n",
    "                enc = tmp_df[col].apply(lambda x: self._dict[tuple(col)][tuple(x)]\n",
    "                                                                     if tuple(x) in self._dict[tuple(col)]\n",
    "                                                                     else self.prior, axis=1).values\n",
    "            else:\n",
    "                tmp_df = X.loc[:, [col]]\n",
    "                enc = tmp_df[col].apply(lambda x: self._dict[col][x]\n",
    "                                                                     if x in self._dict[col]\n",
    "                                                                     else self.prior).values\n",
    "            del tmp_df\n",
    "            X_transformed.append(enc)\n",
    "        \n",
    "        X_transformed = np.vstack(X_transformed).T\n",
    "        \n",
    "        if self.add_to_orig:\n",
    "            return np.concatenate((X.values, X_transformed), axis=1)\n",
    "            \n",
    "        else:\n",
    "            return X_transformed\n",
    "        \n",
    "def isiphonecase(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                                (series.str.contains('case', flags=re.IGNORECASE)) )\n",
    "def isiphone6(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('6|six', flags=re.IGNORECASE) &\n",
    "                        ~(series.str.contains('plus|\\+', flags=re.IGNORECASE)) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) )\n",
    "\n",
    "def isiphone6p(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('6|six', flags=re.IGNORECASE) &\n",
    "                        series.str.contains('plus|\\+', flags=re.IGNORECASE) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) )\n",
    "\n",
    "def isiphone5(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('5|five', flags=re.IGNORECASE) &\n",
    "                        ~(series.str.contains('plus|\\+', flags=re.IGNORECASE)) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) )\n",
    "\n",
    "def isiphone5p(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('5|five', flags=re.IGNORECASE) &\n",
    "                        series.str.contains('plus|\\+', flags=re.IGNORECASE) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) )\n",
    "\n",
    "def isiphone7(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('7|seven', flags=re.IGNORECASE) &\n",
    "                        ~(series.str.contains('plus|\\+', flags=re.IGNORECASE)) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) )\n",
    "\n",
    "def isiphone7p(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('7|seven', flags=re.IGNORECASE) &\n",
    "                        series.str.contains('plus|\\+', flags=re.IGNORECASE) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) )\n",
    "\n",
    "#Data reading function\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "#(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.)\n",
    "\n",
    "def read_data(in_path, out_path):\n",
    "    if False and os.path.exists(os.path.join(out_path, 'train_2.pkl')) and os.path.exists(os.path.join(out_path, 'test_2.pkl')):\n",
    "        train_data = pd.read_pickle(os.path.join(out_path, 'train_2.pkl'))\n",
    "        test_data  = pd.read_pickle(os.path.join(out_path, 'test_2.pkl'))\n",
    "        \n",
    "        return train_data, test_data\n",
    "    \n",
    "    else:\n",
    "        train_data = pd.read_table(os.path.join(in_path, 'train.tsv'))\n",
    "        test_data  = pd.read_table(os.path.join(in_path, 'test.tsv'))\n",
    "    \n",
    "        train_rows = len(train_data)\n",
    "        data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "    \n",
    "        data['cat1'] = data['category_name'].apply(lambda x: str(x).split('/')[0])\n",
    "        data['cat2'] = data['category_name'].apply(lambda x: str(x).split('/')[1] \n",
    "                                                   if len(str(x).split('/')) > 1 else -1)\n",
    "        data['cat3'] = data['category_name'].apply(lambda x: ' '.join(str(x).split('/')[2:]) \n",
    "                                                   if len(str(x).split('/')) > 2 else -1)\n",
    "        data.fillna(-1, inplace=True)\n",
    "        \n",
    "        print(\"Getting word/char len features\")\n",
    "        data['desc_words'] = data['item_description'].apply(lambda x: len(str(x).split()))\n",
    "        data['desc_chars'] = data['item_description'].apply(lambda x: len(str(x)))\n",
    "        data['name_words'] = data['name'].apply(lambda x: len(str(x).split()))\n",
    "        data['name_chars'] = data['name'].apply(len)\n",
    "        \n",
    "        for col in [\"desc_words\", \"desc_chars\", \"name_words\", \"name_chars\"]:\n",
    "            data[col]  = data[col]/ data[col].max()\n",
    "        \n",
    "        print(\"Get iphone features\")\n",
    "        data['iphone_case'] = isiphonecase(data['name'])\n",
    "        data['iphone6'] = isiphone6(data['name'])\n",
    "        data['iphone6p'] = isiphone6p(data['name'])\n",
    "        data['iphone5'] = isiphone5(data['name'])\n",
    "        data['iphone5p'] = isiphone5p(data['name'])\n",
    "        data['iphone7'] = isiphone7(data['name'])\n",
    "        data['iphone7p'] = isiphone7p(data['name'])\n",
    "        data['unlocked_phone'] = data.name.str.contains('unlocked', flags=re.IGNORECASE)\n",
    "        cat_cols = ['category_name', 'brand_name', 'cat1', 'cat2', 'cat3', 'item_condition_id']\n",
    "        for col in cat_cols:\n",
    "            data[col] = LabelEncoder().fit_transform(data[col].astype(str)) + 1\n",
    "            \n",
    "        print(\"Get count features\")\n",
    "        target_enc1 = TargetEncoder(cols=['brand_name'], func=len)\n",
    "        data['brand_counts'] = target_enc1.fit_transform(data[['brand_name']], data.price)\n",
    "        data['brand_counts'] = data['brand_counts']/data['brand_counts'].max()\n",
    "\n",
    "        target_enc2 = TargetEncoder(cols=['category_name'], func=len)\n",
    "        data['cat_counts'] = target_enc2.fit_transform(data[['category_name']], data.price)\n",
    "        data['cat_counts'] = data['cat_counts']/data['cat_counts'].max()\n",
    "        \n",
    "        target_enc3 = TargetEncoder(cols=['cat1'], func=len)\n",
    "        data['cat1_counts'] = target_enc3.fit_transform(data[['cat1']], data.price)\n",
    "        data['cat1_counts'] = data['cat1_counts']/data['cat1_counts'].max()\n",
    "        \n",
    "        target_enc4 = TargetEncoder(cols=['cat2'], func=len)\n",
    "        data['cat2_counts'] = target_enc4.fit_transform(data[['cat2']], data.price)\n",
    "        data['cat2_counts'] = data['cat2_counts']/data['cat2_counts'].max()\n",
    "        \n",
    "        target_enc5 = TargetEncoder(cols=['cat3'], func=len)\n",
    "        data['cat3_counts'] = target_enc5.fit_transform(data[['cat3']], data.price)\n",
    "        data['cat3_counts'] = data['cat3_counts']/data['cat3_counts'].max()\n",
    "        #tkn_desc = Tokenizer(50000)   \n",
    "        \n",
    "        data['item_desc2gram'] = data.item_description.apply(lambda x: add_ngrams(x, 2))\n",
    "        print(\"Tokenizing data\")\n",
    "        tok_name  = Tokenizer(20000)\n",
    "        tok_name.fit_on_texts(data['name'].astype(str))\n",
    "        \n",
    "        tok_desc= Tokenizer(100000)\n",
    "        tok_desc.fit_on_texts(data['item_description'].astype(str))\n",
    "\n",
    "        tok_desc2 = Tokenizer(20000)\n",
    "        tok_desc2.fit_on_texts(data['item_desc2gram'].astype(str))\n",
    "        \n",
    "        data[\"name\"] = list(zip(sequence.pad_sequences(tok_name.texts_to_sequences(data.name.astype(str)),\n",
    "                                         maxlen=7, padding='post', truncating='post')))\n",
    "        \n",
    "        data[\"item_description\"] = list(zip(sequence.pad_sequences(tok_desc.texts_to_sequences(data.item_description.astype(str)),\n",
    "                                         maxlen=70, padding='post', truncating='post')))\n",
    "        \n",
    "        data[\"item_desc2gram\"] = list(zip(sequence.pad_sequences(tok_desc2.texts_to_sequences(data.item_desc2gram.astype(str)),\n",
    "                                         maxlen=30, padding='post', truncating='post')))\n",
    "        #tkn_desc = Tokenizer(50000)\n",
    "        #tkn_desc.fit_on_texts(data.item_description.astype(str))\n",
    "        #data['desc_seq'] = pad_sequences(tkn_desc.texts_to_sequences(data.item_description.astype(str)),\n",
    "        #                                 maxlen=100, padding='post', truncating='post')\n",
    "        \n",
    "        #tkn_name = Tokenizer(4000)\n",
    "        #tkn_name.fit_on_texts(data.name.astype(str))\n",
    "        #data['name_seq'] = pad_sequences(tkn_name.texts_to_sequences(data.name.astype(str)),\n",
    "        #                                 maxlen=6, padding='post', truncating='post')\n",
    "        \n",
    "        \n",
    "        train_data = data.loc[: train_rows - 1, :].reset_index(drop=True)\n",
    "        train_data = train_data.loc[(train_data.price >= 1) & (train_data.price <= 2000), :].reset_index(drop=True)\n",
    "        test_data  = data.loc[train_rows: , :].reset_index(drop=True)\n",
    "        \n",
    "        del train_data['test_id']\n",
    "        del test_data['train_id']\n",
    "        del data \n",
    "        test_data['test_id'] = test_data['test_id'].astype(int)\n",
    "        #train_data.to_pickle(os.path.join(out_path, 'train_2.pkl'))\n",
    "        #test_data.to_pickle(os.path.join(out_path, 'test_2.pkl'))\n",
    "        \n",
    "        return train_data, test_data, tok_name, tok_desc, tok_desc2\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EM_NNRegressor(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, embed_cols=None, dense_cols=None, embed_dims=None, \n",
    "                 text_embed_cols=None, text_embed_seq_lens=None, \n",
    "                 text_embed_dims=None, \n",
    "                 text_embed_tokenizers=None,\n",
    "                 num_layers=2, multiprocess=False,\n",
    "                layer_activations=None, layer_dims=None,layer_dropouts=None, epochs=20, batchsize=32,\n",
    "                optimizer_kwargs=None, val_size=0.1, verbose=1, seed=1,):\n",
    "        \n",
    "        self.embed_cols = embed_cols\n",
    "        self.dense_cols = dense_cols\n",
    "        self.embed_dims = embed_dims\n",
    "        self.text_embed_cols = text_embed_cols\n",
    "        self.text_embed_dims = text_embed_dims\n",
    "        self.text_embed_tokenizers = text_embed_tokenizers\n",
    "        self.text_embed_seq_lens = text_embed_seq_lens\n",
    "        self.dense_dims = None\n",
    "        self.num_layers = num_layers\n",
    "        self.layer_dims = layer_dims\n",
    "        self.layer_activations = layer_activations\n",
    "        self.layer_dropouts = layer_dropouts\n",
    "        self.epochs = epochs\n",
    "        self.batchsize = batchsize\n",
    "        self.optimizer_kwargs = optimizer_kwargs\n",
    "        self.val_size = val_size\n",
    "        self.verbose = verbose\n",
    "        self.multiprocess = multiprocess\n",
    "        self.seed = seed\n",
    "        #self.optim = optim\n",
    "        self.model = None\n",
    "        if self.dense_cols:\n",
    "            self.dense_dims = len(self.dense_cols)\n",
    "            \n",
    "    def _splitX(self, X):\n",
    "        X_splits = []\n",
    "        \n",
    "        if self.embed_cols:\n",
    "            for col in self.embed_cols :\n",
    "                X_splits.append(X[col].values.reshape(X.shape[0], -1))\n",
    "                \n",
    "        if self.text_embed_cols:\n",
    "            for i, col in enumerate(self.text_embed_cols):\n",
    "                #max_features = self.text_embed_dims[i][0]\n",
    "                #max_len = self.text_embed_seq_lens[i]\n",
    "                #input_text = X[col].astype(str)\n",
    "                #x_train = tok.texts_to_sequences(input_text)\n",
    "                #print(np.mean([len(l) for l in x_train]))\n",
    "                #x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "                #X_splits.append(np.array(x_train).reshape(X.shape[0], -1))\n",
    "                X_splits.append(np.concatenate(X[col].values))\n",
    "                \n",
    "        if self.dense_cols:\n",
    "            X_splits.append(X[self.dense_cols].values.reshape(X.shape[0], -1))\n",
    "            \n",
    "        return X_splits\n",
    "    \n",
    "    def get_pretrained_embeddings(self, tok):\n",
    "        embeddings_index = {}\n",
    "        f = open(os.path.join('/home/mohsin/Downloads/', 'glove.6B.100d.txt'))\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "        \n",
    "        embedding_matrix = np.zeros(self.text_embed_dims[0])\n",
    "        for word, i in tok.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        print('Found %s word vectors.' % len(embeddings_index))\n",
    "        return embedding_matrix\n",
    "    \n",
    "    \n",
    "    def _build_model(self):\n",
    "        model_inputs = []\n",
    "        model_layers = []\n",
    "        \n",
    "        if self.embed_cols:\n",
    "            for col, dim in zip(self.embed_cols, self.embed_dims):\n",
    "                x1 = Input( shape=(1,), name=col)\n",
    "                model_inputs.append(x1)\n",
    "                x1 = Embedding(input_dim=dim[0], output_dim=dim[1], )(x1)\n",
    "                #x1 = Dropout(0.1)(x1)\n",
    "                x1 = Reshape(target_shape=(dim[1],))(x1)\n",
    "                model_layers.append(x1)\n",
    "                \n",
    "        if self.text_embed_cols:\n",
    "            for col, dim, seq_len in zip(self.text_embed_cols, \n",
    "                                                self.text_embed_dims, \n",
    "                                                self.text_embed_seq_lens):\n",
    "                x3 = Input( shape=(seq_len,))\n",
    "                model_inputs.append(x3)\n",
    "                x3 = Embedding(input_dim=dim[0], output_dim=dim[1], input_length=seq_len)(x3)\n",
    "                x3 = GlobalAveragePooling1D()(x3)\n",
    "                x3 = Reshape(target_shape=(dim[1],))(x3)\n",
    "                model_layers.append(x3)\n",
    "                \n",
    "        if self.dense_cols:\n",
    "            x2 = Input( shape=(self.dense_dims, ), name='dense_cols')\n",
    "            model_inputs.append(x2)\n",
    "            model_layers.append(x2)\n",
    "        print(model_layers)\n",
    "        x = concatenate(model_layers)\n",
    "        \n",
    "        if self.num_layers > 0:\n",
    "            for dim, drops in zip(self.layer_dims, self.layer_dropouts):\n",
    "                x = BatchNormalization()(x)\n",
    "                x = Dropout(rate=drops)(x)\n",
    "                x = Dense(dim, kernel_initializer='he_normal')(x)\n",
    "                x = LeakyReLU()(x)\n",
    "        \n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.05)(x)\n",
    "        output = Dense(1, activation='linear', kernel_initializer='he_normal')(x)\n",
    "        \n",
    "        model = Model(inputs=model_inputs, outputs=output)\n",
    "        #print(model.summary())\n",
    "        #adam = Nadam(lr=0.002, schedule_decay=0.02)\n",
    "        adam = Adam(lr=0.005, decay=0.001)\n",
    "        #adam = SGD(lr=0.01, nesterov=True, momentum=0.9, decay=0.003)\n",
    "        #adam = RMSprop(lr=0.01, decay=0.006)\n",
    "        #adam = self.optim\n",
    "        model.compile(optimizer=adam, loss='mean_squared_error' )\n",
    "        \n",
    "        return model \n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model = self._build_model()\n",
    "        if self.val_size > 0:\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.val_size, random_state=self.seed)\n",
    "            print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "            \n",
    "            callbacks= [ModelCheckpoint(\"embed_NN_\"+str(self.seed)+\".check\", save_best_only=True, verbose=1)]\n",
    "            if self.multiprocess == False:\n",
    "                self.model.fit(self._splitX(X_train), y_train, batch_size=self.batchsize, epochs=self.epochs,\n",
    "                               verbose=self.verbose,\n",
    "                              validation_data=(self._splitX(X_val), y_val), shuffle=True,\n",
    "                              callbacks=callbacks)\n",
    "            else:\n",
    "                X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.val_size, random_state=1)\n",
    "\n",
    "        else:\n",
    "            self.model.fit(self._splitX(X), y, batch_size=self.batchsize, epochs=self.epochs,\n",
    "               verbose=self.verbose, shuffle=True)\n",
    "\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        \n",
    "        if self.model:\n",
    "            model = load_model(\"embed_NN_\"+str(self.seed)+\".check\")\n",
    "            y_hat = model.predict(self._splitX(X))\n",
    "        else:\n",
    "            raise ValueError(\"Model not fit yet\")\n",
    "            \n",
    "        return y_hat\n",
    "        \n",
    "def add_ngrams(text, ngram=2):\n",
    "    word_list = str(text).lower().split(' ')\n",
    "    out_list = [''.join(word_list[i:i+ngram]) for i in range(len(word_list))]\n",
    "    return ' '.join(out_list[:-1])\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting word/char len features\n",
      "Get iphone features\n",
      "Get count features\n",
      "Index(['brand_name', 'y'], dtype='object')\n",
      "Index(['category_name', 'y'], dtype='object')\n",
      "Index(['cat1', 'y'], dtype='object')\n",
      "Index(['cat2', 'y'], dtype='object')\n",
      "Index(['cat3', 'y'], dtype='object')\n",
      "Tokenizing data\n"
     ]
    }
   ],
   "source": [
    "#Read data\n",
    "train_data, test_data = read_data(\"../input\", \"./\")\n",
    "print(train_data.shape, test_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data\n",
    "y = np.log1p(train_data.price)\n",
    "\n",
    "cvlist= list(KFold(5, random_state=786).split(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet1 = EM_NNRegressor(embed_cols=['brand_name','category_name','item_condition_id', 'cat1', 'cat2', 'cat3'], \n",
    "                  embed_dims=[(6000, 40),(1500, 30), (5,4), (15,4), (120, 10), (900, 20)],\n",
    "                  text_embed_cols=['name', 'item_description', 'item_desc2gram'],\n",
    "                  text_embed_dims=[(20000, 50), (100000, 100), (20000, 50)],\n",
    "                  text_embed_seq_lens =[7, 70, 30],\n",
    "                  #text_embed_tokenizers = [tok_name, tok_desc, tok_desc2],\n",
    "                  dense_cols=['shipping', 'desc_words', 'desc_chars', 'name_chars', 'name_words',\n",
    "                                'iphone_case', 'iphone6', 'iphone6p',\n",
    "                                'iphone5', 'iphone5p', 'iphone7', 'iphone7p', 'unlocked_phone',\n",
    "                              'brand_counts', 'cat_counts',\n",
    "                                   'cat1_counts', 'cat2_counts', 'cat3_counts'\n",
    "                                  ],\n",
    "                  epochs=5,\n",
    "                  batchsize=2048 ,\n",
    "                  num_layers = 1,\n",
    "                  layer_dropouts=[0.22],\n",
    "                  layer_dims=[200],\n",
    "                  seed=1,\n",
    "                  val_size=0.025,\n",
    "                 )\n",
    "\n",
    "oof_preds1 = cross_val_predict(nnet1, X, y, verbose=10, cv=cvlist)\n",
    "score = rmse(y, oof_preds1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19063"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_298/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_299/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_300/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_301/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_302/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_303/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_304/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_305/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_306/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_59:0' shape=(?, 17) dtype=float32>]\n",
      "(1155692, 29) (29634, 29) (1155692,) (29634,)\n",
      "Train on 1155692 samples, validate on 29634 samples\n",
      "Epoch 1/5\n",
      "1150976/1155692 [============================>.] - ETA: 0s - loss: 0.5649Epoch 00001: val_loss improved from inf to 0.26376, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 28s 24us/step - loss: 0.5636 - val_loss: 0.2638\n",
      "Epoch 2/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.2023Epoch 00002: val_loss improved from 0.26376 to 0.18651, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.2023 - val_loss: 0.1865\n",
      "Epoch 3/5\n",
      "1153024/1155692 [============================>.] - ETA: 0s - loss: 0.1777Epoch 00003: val_loss improved from 0.18651 to 0.18236, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1777 - val_loss: 0.1824\n",
      "Epoch 4/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.1635Epoch 00004: val_loss improved from 0.18236 to 0.18088, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1635 - val_loss: 0.1809\n",
      "Epoch 5/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.1538Epoch 00005: val_loss improved from 0.18088 to 0.17902, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1539 - val_loss: 0.1790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_307/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_308/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_309/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_310/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_311/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_312/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_313/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_314/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_315/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_61:0' shape=(?, 17) dtype=float32>]\n",
      "(1155692, 29) (29634, 29) (1155692,) (29634,)\n",
      "Train on 1155692 samples, validate on 29634 samples\n",
      "Epoch 1/5\n",
      "1150976/1155692 [============================>.] - ETA: 0s - loss: 0.5806Epoch 00001: val_loss improved from inf to 0.24525, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 28s 25us/step - loss: 0.5792 - val_loss: 0.2453\n",
      "Epoch 2/5\n",
      "1153024/1155692 [============================>.] - ETA: 0s - loss: 0.2021Epoch 00002: val_loss improved from 0.24525 to 0.18385, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.2021 - val_loss: 0.1839\n",
      "Epoch 3/5\n",
      "1153024/1155692 [============================>.] - ETA: 0s - loss: 0.1781Epoch 00003: val_loss improved from 0.18385 to 0.18114, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1781 - val_loss: 0.1811\n",
      "Epoch 4/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.1645Epoch 00004: val_loss improved from 0.18114 to 0.17804, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1645 - val_loss: 0.1780\n",
      "Epoch 5/5\n",
      "1153024/1155692 [============================>.] - ETA: 0s - loss: 0.1548Epoch 00005: val_loss did not improve\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1548 - val_loss: 0.1789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  4.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_316/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_317/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_318/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_319/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_320/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_321/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_322/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_323/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_324/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_63:0' shape=(?, 17) dtype=float32>]\n",
      "(1155692, 29) (29634, 29) (1155692,) (29634,)\n",
      "Train on 1155692 samples, validate on 29634 samples\n",
      "Epoch 1/5\n",
      "1150976/1155692 [============================>.] - ETA: 0s - loss: 0.5647Epoch 00001: val_loss improved from inf to 0.25191, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 29s 25us/step - loss: 0.5633 - val_loss: 0.2519\n",
      "Epoch 2/5\n",
      "1153024/1155692 [============================>.] - ETA: 0s - loss: 0.2011Epoch 00002: val_loss improved from 0.25191 to 0.18897, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.2011 - val_loss: 0.1890\n",
      "Epoch 3/5\n",
      "1150976/1155692 [============================>.] - ETA: 0s - loss: 0.1775Epoch 00003: val_loss improved from 0.18897 to 0.18056, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1775 - val_loss: 0.1806\n",
      "Epoch 4/5\n",
      "1150976/1155692 [============================>.] - ETA: 0s - loss: 0.1633Epoch 00004: val_loss improved from 0.18056 to 0.17862, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1633 - val_loss: 0.1786\n",
      "Epoch 5/5\n",
      "1153024/1155692 [============================>.] - ETA: 0s - loss: 0.1534Epoch 00005: val_loss did not improve\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1534 - val_loss: 0.1786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  7.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_325/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_326/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_327/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_328/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_329/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_330/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_331/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_332/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_333/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_65:0' shape=(?, 17) dtype=float32>]\n",
      "(1155693, 29) (29634, 29) (1155693,) (29634,)\n",
      "Train on 1155693 samples, validate on 29634 samples\n",
      "Epoch 1/5\n",
      "1155072/1155693 [============================>.] - ETA: 0s - loss: 0.5760Epoch 00001: val_loss improved from inf to 0.25223, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 30s 26us/step - loss: 0.5758 - val_loss: 0.2522\n",
      "Epoch 2/5\n",
      "1155072/1155693 [============================>.] - ETA: 0s - loss: 0.2019Epoch 00002: val_loss improved from 0.25223 to 0.18721, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.2018 - val_loss: 0.1872\n",
      "Epoch 3/5\n",
      "1155072/1155693 [============================>.] - ETA: 0s - loss: 0.1779Epoch 00003: val_loss improved from 0.18721 to 0.18023, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.1779 - val_loss: 0.1802\n",
      "Epoch 4/5\n",
      "1155072/1155693 [============================>.] - ETA: 0s - loss: 0.1637Epoch 00004: val_loss improved from 0.18023 to 0.17916, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.1637 - val_loss: 0.1792\n",
      "Epoch 5/5\n",
      "1153024/1155693 [============================>.] - ETA: 0s - loss: 0.1537Epoch 00005: val_loss improved from 0.17916 to 0.17838, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.1537 - val_loss: 0.1784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  9.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_334/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_335/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_336/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_337/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_338/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_339/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_340/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_341/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_342/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_67:0' shape=(?, 17) dtype=float32>]\n",
      "(1155693, 29) (29634, 29) (1155693,) (29634,)\n",
      "Train on 1155693 samples, validate on 29634 samples\n",
      "Epoch 1/5\n",
      "1153024/1155693 [============================>.] - ETA: 0s - loss: 0.5633Epoch 00001: val_loss improved from inf to 0.24940, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 30s 26us/step - loss: 0.5626 - val_loss: 0.2494\n",
      "Epoch 2/5\n",
      "1155072/1155693 [============================>.] - ETA: 0s - loss: 0.2015Epoch 00002: val_loss improved from 0.24940 to 0.18373, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.2015 - val_loss: 0.1837\n",
      "Epoch 3/5\n",
      "1150976/1155693 [============================>.] - ETA: 0s - loss: 0.1779Epoch 00003: val_loss improved from 0.18373 to 0.17886, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.1779 - val_loss: 0.1789\n",
      "Epoch 4/5\n",
      "1153024/1155693 [============================>.] - ETA: 0s - loss: 0.1640Epoch 00004: val_loss improved from 0.17886 to 0.17707, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.1640 - val_loss: 0.1771\n",
      "Epoch 5/5\n",
      "1155072/1155693 [============================>.] - ETA: 0s - loss: 0.1540Epoch 00005: val_loss improved from 0.17707 to 0.17607, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.1540 - val_loss: 0.1761\n",
      "0.47740754 9.648284\n",
      "0.4204833150009789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 12.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 12.3min finished\n"
     ]
    }
   ],
   "source": [
    "nnet2 = EM_NNRegressor(embed_cols=['brand_name','category_name','item_condition_id', 'cat1', 'cat2', 'cat3'], \n",
    "                  embed_dims=[(6000, 30),(1500, 25), (5,4), (15,4), (120, 10), (900, 20)],\n",
    "                  text_embed_cols=['name', 'item_description', 'item_desc2gram'],\n",
    "                  text_embed_dims=[(20000, 30), (50000, 30), (20000, 30)],\n",
    "                  text_embed_seq_lens =[7, 70, 30],\n",
    "                  #text_embed_tokenizers = [tok_name, tok_desc, tok_desc2],\n",
    "                  dense_cols=['shipping', 'desc_words', 'desc_chars', 'name_chars',\n",
    "                                'iphone_case', 'iphone6', 'iphone6p',\n",
    "                                'iphone5', 'iphone5p', 'iphone7', 'iphone7p', 'unlocked_phone',\n",
    "                              'brand_counts', 'cat_counts',\n",
    "                                   'cat1_counts', 'cat2_counts', 'cat3_counts'],\n",
    "                  epochs=4,\n",
    "                  batchsize=2048 ,\n",
    "                  num_layers = 1,\n",
    "                  layer_dropouts=[0.2],\n",
    "                  layer_dims=[100],\n",
    "                  seed=2,\n",
    "                  val_size=0.02\n",
    "                 )\n",
    "oof_preds2 = cross_val_predict(nnet1, X, y, verbose=10, cv=cvlist)\n",
    "score = rmse(y, oof_preds2)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_343/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_344/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_345/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_346/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_347/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_348/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_349/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_350/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_351/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_69:0' shape=(?, 17) dtype=float32>]\n",
      "(1155692, 29) (29634, 29) (1155692,) (29634,)\n",
      "Train on 1155692 samples, validate on 29634 samples\n",
      "Epoch 1/5\n",
      "1153024/1155692 [============================>.] - ETA: 0s - loss: 0.5652Epoch 00001: val_loss improved from inf to 0.25011, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 31s 27us/step - loss: 0.5644 - val_loss: 0.2501\n",
      "Epoch 2/5\n",
      "1153024/1155692 [============================>.] - ETA: 0s - loss: 0.2023Epoch 00002: val_loss improved from 0.25011 to 0.18530, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.2023 - val_loss: 0.1853\n",
      "Epoch 3/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.1781Epoch 00003: val_loss improved from 0.18530 to 0.18093, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1781 - val_loss: 0.1809\n",
      "Epoch 4/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.1643Epoch 00004: val_loss improved from 0.18093 to 0.17861, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1643 - val_loss: 0.1786\n",
      "Epoch 5/5\n",
      "1153024/1155692 [============================>.] - ETA: 0s - loss: 0.1540Epoch 00005: val_loss did not improve\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1540 - val_loss: 0.1789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_352/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_353/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_354/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_355/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_356/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_357/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_358/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_359/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_360/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_71:0' shape=(?, 17) dtype=float32>]\n",
      "(1155692, 29) (29634, 29) (1155692,) (29634,)\n",
      "Train on 1155692 samples, validate on 29634 samples\n",
      "Epoch 1/5\n",
      "1153024/1155692 [============================>.] - ETA: 0s - loss: 0.5729Epoch 00001: val_loss improved from inf to 0.24662, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 31s 27us/step - loss: 0.5721 - val_loss: 0.2466\n",
      "Epoch 2/5\n",
      "1150976/1155692 [============================>.] - ETA: 0s - loss: 0.2020Epoch 00002: val_loss improved from 0.24662 to 0.18736, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.2020 - val_loss: 0.1874\n",
      "Epoch 3/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.1785Epoch 00003: val_loss improved from 0.18736 to 0.18088, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1785 - val_loss: 0.1809\n",
      "Epoch 4/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.1646Epoch 00004: val_loss improved from 0.18088 to 0.17897, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1646 - val_loss: 0.1790\n",
      "Epoch 5/5\n",
      "1153024/1155692 [============================>.] - ETA: 0s - loss: 0.1546Epoch 00005: val_loss did not improve\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1546 - val_loss: 0.1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  5.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_361/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_362/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_363/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_364/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_365/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_366/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_367/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_368/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_369/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_73:0' shape=(?, 17) dtype=float32>]\n",
      "(1155692, 29) (29634, 29) (1155692,) (29634,)\n",
      "Train on 1155692 samples, validate on 29634 samples\n",
      "Epoch 1/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.5759Epoch 00001: val_loss improved from inf to 0.24836, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 31s 27us/step - loss: 0.5757 - val_loss: 0.2484\n",
      "Epoch 2/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.2010Epoch 00002: val_loss improved from 0.24836 to 0.18542, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.2010 - val_loss: 0.1854\n",
      "Epoch 3/5\n",
      "1153024/1155692 [============================>.] - ETA: 0s - loss: 0.1778Epoch 00003: val_loss improved from 0.18542 to 0.18066, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1778 - val_loss: 0.1807\n",
      "Epoch 4/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.1640Epoch 00004: val_loss improved from 0.18066 to 0.17752, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1640 - val_loss: 0.1775\n",
      "Epoch 5/5\n",
      "1153024/1155692 [============================>.] - ETA: 0s - loss: 0.1542Epoch 00005: val_loss did not improve\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1542 - val_loss: 0.1776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  7.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_370/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_371/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_372/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_373/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_374/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_375/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_376/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_377/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_378/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_75:0' shape=(?, 17) dtype=float32>]\n",
      "(1155693, 29) (29634, 29) (1155693,) (29634,)\n",
      "Train on 1155693 samples, validate on 29634 samples\n",
      "Epoch 1/5\n",
      "1153024/1155693 [============================>.] - ETA: 0s - loss: 0.5573Epoch 00001: val_loss improved from inf to 0.26231, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 32s 27us/step - loss: 0.5566 - val_loss: 0.2623\n",
      "Epoch 2/5\n",
      "1155072/1155693 [============================>.] - ETA: 0s - loss: 0.2072Epoch 00002: val_loss improved from 0.26231 to 0.18558, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.2072 - val_loss: 0.1856\n",
      "Epoch 3/5\n",
      "1153024/1155693 [============================>.] - ETA: 0s - loss: 0.1818Epoch 00003: val_loss improved from 0.18558 to 0.18380, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.1818 - val_loss: 0.1838\n",
      "Epoch 4/5\n",
      "1153024/1155693 [============================>.] - ETA: 0s - loss: 0.1667Epoch 00004: val_loss improved from 0.18380 to 0.17916, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.1667 - val_loss: 0.1792\n",
      "Epoch 5/5\n",
      "1153024/1155693 [============================>.] - ETA: 0s - loss: 0.1566Epoch 00005: val_loss did not improve\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.1566 - val_loss: 0.1802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 10.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_379/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_380/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_381/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_382/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_383/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_384/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_385/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_386/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_387/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_77:0' shape=(?, 17) dtype=float32>]\n",
      "(1155693, 29) (29634, 29) (1155693,) (29634,)\n",
      "Train on 1155693 samples, validate on 29634 samples\n",
      "Epoch 1/5\n",
      "1155072/1155693 [============================>.] - ETA: 0s - loss: 0.5663Epoch 00001: val_loss improved from inf to 0.26489, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 32s 28us/step - loss: 0.5661 - val_loss: 0.2649\n",
      "Epoch 2/5\n",
      "1155072/1155693 [============================>.] - ETA: 0s - loss: 0.2042Epoch 00002: val_loss improved from 0.26489 to 0.18569, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.2042 - val_loss: 0.1857\n",
      "Epoch 3/5\n",
      "1153024/1155693 [============================>.] - ETA: 0s - loss: 0.1798Epoch 00003: val_loss improved from 0.18569 to 0.18255, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.1798 - val_loss: 0.1826\n",
      "Epoch 4/5\n",
      "1155072/1155693 [============================>.] - ETA: 0s - loss: 0.1657Epoch 00004: val_loss improved from 0.18255 to 0.18069, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.1657 - val_loss: 0.1807\n",
      "Epoch 5/5\n",
      "1153024/1155693 [============================>.] - ETA: 0s - loss: 0.1555Epoch 00005: val_loss improved from 0.18069 to 0.17996, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.1555 - val_loss: 0.1800\n",
      "0.5372175 8.981724\n",
      "0.42130786201521814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 12.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 12.7min finished\n"
     ]
    }
   ],
   "source": [
    "nnet3 = EM_NNRegressor(embed_cols=['brand_name','category_name','item_condition_id', 'cat1', 'cat2', 'cat3'], \n",
    "                  embed_dims=[(6000, 30),(1500, 20), (5,4), (15,4), (120, 10), (900, 20)],\n",
    "                  text_embed_cols=['name', 'item_description', 'item_desc2gram'],\n",
    "                  text_embed_dims=[(20000, 50), (50000, 50), (20000, 50)],\n",
    "                  text_embed_seq_lens =[7, 70, 30],\n",
    "                  #text_embed_tokenizers = [tok_name, tok_desc, tok_desc2],\n",
    "                  dense_cols=['shipping', 'desc_words', 'desc_chars', 'name_chars',\n",
    "                                'iphone_case', 'iphone6', 'iphone6p',\n",
    "                                'iphone5', 'iphone5p', 'iphone7', 'iphone7p', 'unlocked_phone',\n",
    "                              'brand_counts', 'cat_counts',\n",
    "                                   'cat1_counts', 'cat2_counts', 'cat3_counts'],\n",
    "                  epochs=4,\n",
    "                  batchsize=2048 ,\n",
    "                  num_layers = 1,\n",
    "                  layer_dropouts=[0.2],\n",
    "                  layer_dims=[200],\n",
    "                  seed=3,\n",
    "                  val_size=0.02,\n",
    "                 )\n",
    "oof_preds3 = cross_val_predict(nnet1, X, y, verbose=10, cv=cvlist)\n",
    "score = rmse(y, oof_preds3)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import hmean, gmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1481658,)\n",
      "0.4934621 9.394966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4137029815802479"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_preds = np.mean(np.hstack((oof_preds1, oof_preds2, oof_preds3)), axis=1)\n",
    "print(oof_preds.shape)\n",
    "rmse(y, oof_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet1.fit(train_data, np.log1p(train_data.price) )\n",
    "print(\"Predicting on test data\")\n",
    "test_preds1 = nnet1.predict(test_data)\n",
    "\n",
    "nnet2.fit(train_data, np.log1p(train_data.price) )\n",
    "print(\"Predicting on test data\")\n",
    "test_preds2 = nnet2.predict(test_data)\n",
    "\n",
    "nnet3.fit(train_data, np.log1p(train_data.price) )\n",
    "print(\"Predicting on test data\")\n",
    "test_preds3 = nnet3.predict(test_data)\n",
    "\n",
    "test_preds = (1/3)*(test_preds1 + test_preds2 + test_preds3)\n",
    "print(\"Write out submission\")\n",
    "submission: pd.DataFrame = test_data[['test_id']]\n",
    "submission['price'] = np.expm1(test_preds)\n",
    "submission.price = submission.price.clip(3, 2000)\n",
    "submission.to_csv(\"embedding_nn_v2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
