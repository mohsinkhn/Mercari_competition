{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment 5\n",
    "* Use gensim for phrase based tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohsin/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import threading\n",
    "import multiprocessing\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm.pandas(tqdm_notebook)\n",
    "# from __future__ import print_function\n",
    "np.random.seed(786)  # for reproducibility\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D, ZeroPadding1D, AveragePooling1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier,  KerasRegressor\n",
    "#Some classes\n",
    "#Functions we need - Feature Selector, Fasttext_Estimator, Preprocessing Transformer, Binary_Encoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "from pandas.api.types import is_numeric_dtype, is_string_dtype\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    print(np.min(y_pred), np.max(y_pred))\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "rmse_sklearn = make_scorer(rmse, greater_is_better=False)\n",
    "\n",
    "def get_obj_cols(df):\n",
    "    \"\"\"Return columns with object dtypes\"\"\"\n",
    "    obj_cols = []\n",
    "    for idx, dt in enumerate(df.dtypes):\n",
    "        if dt == 'object':\n",
    "            obj_cols.append(df.columns.values[idx])\n",
    "\n",
    "    return obj_cols\n",
    "\n",
    "\n",
    "def convert_input(X):\n",
    "    \"\"\"if input not a dataframe convert it to one\"\"\"\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        if isinstance(X, list):\n",
    "            X = pd.DataFrame(np.array(X))\n",
    "        elif isinstance(X, (np.generic, np.ndarray)):\n",
    "            X = pd.DataFrame(X)\n",
    "        elif isinstance(X, csr_matrix):\n",
    "            X = pd.SparseDataFrame(X)\n",
    "        else:\n",
    "            raise ValueError('Unexpected input type: %s' % (str(type(X))))\n",
    "\n",
    "        #X = X.apply(lambda x: pd.to_numeric(x, errors='ignore'))\n",
    "    return X\n",
    "\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Class to do subset of features in sklearn pipeline\"\"\"\n",
    "    def __init__(self, cols=None, return_df=True, verbose=0):\n",
    "        self.cols = cols\n",
    "        self.return_df = return_df\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        #Do nothing\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        #if the input dataset isn't already a dataframe, convert it to one\n",
    "        X = X.copy(deep=True)\n",
    "        X = convert_input(X)\n",
    "        X = X.loc[:, self.col]\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Selecting columns are {}\".format(self.col))\n",
    "        if self.return_df:\n",
    "            return X\n",
    "        else:\n",
    "            return X.values\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "class TargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols=None, thresh=0, func=np.mean, add_to_orig=False):\n",
    "        self.cols = cols\n",
    "        self.thresh = thresh\n",
    "        self.func = func\n",
    "        self.add_to_orig = add_to_orig\n",
    "    \n",
    "    #@numba.jit        \n",
    "    def fit(self, X, y):\n",
    "        self.prior = self.func(y)\n",
    "        self._dict = {}\n",
    "        for col in self.cols:\n",
    "            if isinstance(col, (list, tuple)):\n",
    "                print('here')\n",
    "                tmp_df = X.loc[: ,col]\n",
    "                col = tuple(col)\n",
    "            else:\n",
    "                tmp_df = X.loc[: ,[col]]\n",
    "            tmp_df['y'] = y\n",
    "            print(tmp_df.columns)\n",
    "            #tmp_df = pd.DataFrame({'eval_col':X[col].values, 'y':y})\n",
    "            if isinstance(col, (list, tuple)):\n",
    "                print('here')\n",
    "                col = tuple(col)\n",
    "            self._dict[col] = tmp_df.groupby(col)['y'].apply(lambda x: \n",
    "                                self.func(x) if len(x) >= self.thresh  else self.prior).to_dict()\n",
    "                                \n",
    "            del tmp_df\n",
    "        return self\n",
    "    #@numba.jit\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for col in self.cols:\n",
    "            \n",
    "            if isinstance(col, (list, tuple)):\n",
    "                tmp_df = X.loc[:, col]\n",
    "                enc = tmp_df[col].apply(lambda x: self._dict[tuple(col)][tuple(x)]\n",
    "                                                                     if tuple(x) in self._dict[tuple(col)]\n",
    "                                                                     else self.prior, axis=1).values\n",
    "            else:\n",
    "                tmp_df = X.loc[:, [col]]\n",
    "                enc = tmp_df[col].apply(lambda x: self._dict[col][x]\n",
    "                                                                     if x in self._dict[col]\n",
    "                                                                     else self.prior).values\n",
    "            del tmp_df\n",
    "            X_transformed.append(enc)\n",
    "        \n",
    "        X_transformed = np.vstack(X_transformed).T\n",
    "        \n",
    "        if self.add_to_orig:\n",
    "            return np.concatenate((X.values, X_transformed), axis=1)\n",
    "            \n",
    "        else:\n",
    "            return X_transformed\n",
    "        \n",
    "def isiphonecase(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                                (series.str.contains('case', flags=re.IGNORECASE)) )\n",
    "def isiphone6(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('6|six', flags=re.IGNORECASE) &\n",
    "                        ~(series.str.contains('plus|\\+', flags=re.IGNORECASE)) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) )\n",
    "\n",
    "def isiphone6p(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('6|six', flags=re.IGNORECASE) &\n",
    "                        series.str.contains('plus|\\+', flags=re.IGNORECASE) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) )\n",
    "\n",
    "def isiphone5(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('5|five', flags=re.IGNORECASE) &\n",
    "                        ~(series.str.contains('plus|\\+', flags=re.IGNORECASE)) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) )\n",
    "\n",
    "def isiphone5p(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('5|five', flags=re.IGNORECASE) &\n",
    "                        series.str.contains('plus|\\+', flags=re.IGNORECASE) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) )\n",
    "\n",
    "def isiphone7(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('7|seven', flags=re.IGNORECASE) &\n",
    "                        ~(series.str.contains('plus|\\+', flags=re.IGNORECASE)) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) )\n",
    "\n",
    "def isiphone7p(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('7|seven', flags=re.IGNORECASE) &\n",
    "                        series.str.contains('plus|\\+', flags=re.IGNORECASE) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) )\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c if unicodedata.category(c) not in ['So', 'Sm', 'Lo', 'Sc']\n",
    "        else ' '\n",
    "        for c in unicodedata.normalize('NFD', str(s))\n",
    "    )\n",
    "\n",
    "def remove_puncts(s):\n",
    "    trans_table = str.maketrans({s:' ' for s in string.punctuation})\n",
    "    return str(s).translate(trans_table)\n",
    "\n",
    "#Data reading function\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "#(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.)\n",
    "\n",
    "        \n",
    "def tokenize(text):\n",
    "    return text.lower().strip(string.punctuation).split()\n",
    "\n",
    "\n",
    "def read_data(in_path, out_path):\n",
    "    if False and os.path.exists(os.path.join(out_path, 'train_2.pkl')) and os.path.exists(os.path.join(out_path, 'test_2.pkl')):\n",
    "        train_data = pd.read_pickle(os.path.join(out_path, 'train_2.pkl'))\n",
    "        test_data  = pd.read_pickle(os.path.join(out_path, 'test_2.pkl'))\n",
    "        \n",
    "        return train_data, test_data\n",
    "    \n",
    "    else:\n",
    "        train_data = pd.read_table(os.path.join(in_path, 'train.tsv'))\n",
    "        test_data  = pd.read_table(os.path.join(in_path, 'test.tsv'))\n",
    "    \n",
    "        train_rows = len(train_data)\n",
    "        data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "    \n",
    "        data['cat1'] = data['category_name'].apply(lambda x: str(x).split('/')[0])\n",
    "        data['cat2'] = data['category_name'].apply(lambda x: str(x).split('/')[1] \n",
    "                                                   if len(str(x).split('/')) > 1 else -1)\n",
    "        data['cat3'] = data['category_name'].apply(lambda x: ' '.join(str(x).split('/')[2:]) \n",
    "                                                   if len(str(x).split('/')) > 2 else -1)\n",
    "        data.fillna(-1, inplace=True)\n",
    "        \n",
    "        print(\"Getting word/char len features\")\n",
    "        data['desc_words'] = data['item_description'].apply(lambda x: len(str(x).split()))\n",
    "        data['desc_chars'] = data['item_description'].apply(lambda x: len(str(x)))\n",
    "        data['name_words'] = data['name'].apply(lambda x: len(str(x).split()))\n",
    "        data['name_chars'] = data['name'].apply(len)\n",
    "        \n",
    "        \n",
    "        print(\"Get iphone features\")\n",
    "        data['iphone_case'] = isiphonecase(data['name'])\n",
    "        data['iphone6'] = isiphone6(data['name'])\n",
    "        data['iphone6p'] = isiphone6p(data['name'])\n",
    "        data['iphone5'] = isiphone5(data['name'])\n",
    "        data['iphone5p'] = isiphone5p(data['name'])\n",
    "        data['iphone7'] = isiphone7(data['name'])\n",
    "        data['iphone7p'] = isiphone7p(data['name'])\n",
    "        data['unlocked_phone'] = data.name.str.contains('unlocked', flags=re.IGNORECASE)\n",
    "        cat_cols = ['category_name', 'brand_name', 'cat1', 'cat2', 'cat3', 'item_condition_id']\n",
    "        for col in cat_cols:\n",
    "            data[col] = LabelEncoder().fit_transform(data[col].astype(str)) + 1\n",
    "            \n",
    "        print(\"Get count features\")\n",
    "        target_enc1 = TargetEncoder(cols=['brand_name'], func=len)\n",
    "        data['brand_counts'] = target_enc1.fit_transform(data[['brand_name']], data.price)\n",
    "        data['brand_counts'] = data['brand_counts']/data['brand_counts'].max()\n",
    "\n",
    "        target_enc2 = TargetEncoder(cols=['category_name'], func=len)\n",
    "        data['cat_counts'] = target_enc2.fit_transform(data[['category_name']], data.price)\n",
    "        data['cat_counts'] = data['cat_counts']/data['cat_counts'].max()\n",
    "        \n",
    "        target_enc3 = TargetEncoder(cols=['cat1'], func=len)\n",
    "        data['cat1_counts'] = target_enc3.fit_transform(data[['cat1']], data.price)\n",
    "        data['cat1_counts'] = data['cat1_counts']/data['cat1_counts'].max()\n",
    "        \n",
    "        target_enc4 = TargetEncoder(cols=['cat2'], func=len)\n",
    "        data['cat2_counts'] = target_enc4.fit_transform(data[['cat2']], data.price)\n",
    "        data['cat2_counts'] = data['cat2_counts']/data['cat2_counts'].max()\n",
    "        \n",
    "        target_enc5 = TargetEncoder(cols=['cat3'], func=len)\n",
    "        data['cat3_counts'] = target_enc5.fit_transform(data[['cat3']], data.price)\n",
    "        data['cat3_counts'] = data['cat3_counts']/data['cat3_counts'].max()\n",
    "        #tkn_desc = Tokenizer(50000)   \n",
    "        \n",
    "        #reg = re.compile('[^a-zA-Z0-9 ]')\n",
    "        data[\"plus_counts\"] = data[\"name\"].apply(lambda x: sum([(s == '+') | (s == '➕') for s in str(x)]))\n",
    "        data[\"ands_counts\"] = data[\"name\"].apply(lambda x: sum([(s == '&') | (s == ' and ') for s in str(x)]))\n",
    "        data[\"comma_counts\"] = data[\"name\"].apply(lambda x: sum([s == ',' for s in str(x)]))\n",
    "        data[\"all_counts\"] = data[\"plus_counts\"] + data[\"ands_counts\"] + data[\"comma_counts\"]\n",
    "        \n",
    "        for col in [\"name\", \"item_description\"]:\n",
    "            data[col] = data[col].str.replace(\"'\", '').replace('-', '').progress_apply(unicodeToAscii)\n",
    "            data[col] = data[col].progress_apply(remove_puncts)\n",
    "        \n",
    "        \n",
    "        for col in [\"desc_words\", \"desc_chars\", \"name_words\", \"name_chars\", \"plus_counts\", \n",
    "                    \"ands_counts\", \"comma_counts\", \"all_counts\"]:\n",
    "            data[col]  = data[col]/ data[col].max()\n",
    "            \n",
    "        data[\"name_description\"] = data[\"name\"] + ' ' + data[\"item_description\"]\n",
    "        #stoplist = set('for a of the and to in on yet it'.split())\n",
    "        #dictionary = corpora.Dictionary(line.lower().split() for line in data[\"name_description\"].values)\n",
    "        \n",
    "        #stop_ids = [dictionary.token2id[stopword] for stopword in stoplist\n",
    "        #         if stopword in dictionary.token2id]\n",
    "        \n",
    "        #dictionary.filter_tokens(stop_ids)  # remove stop words and words that appear only once\n",
    "        #dictionary.compactify()\n",
    "        #print(dictionary)\n",
    "        #sentence_stream = [doc.lower().split(\" \") for doc in data[\"name_description\"].tolist()]\n",
    "        #del data[\"name_description\"]\n",
    "        \n",
    "        #bigram = Phraser(Phrases(sentence_stream, min_count=10, threshold=10, common_terms=stoplist))\n",
    "        #tokenized_stream = [bigram[sent] for sent in sentence_stream]\n",
    "        #data[\"name\"] = data['name'].progress_apply(lambda x: str(x).lower().split())\n",
    "        #dictionary_name = corpora.Dictionary([bigram[sent] for sent in tqdm(data['name'].values)])\n",
    "        #print(dictionary_name)\n",
    "        #dictionary_name.filter_extremes(keep_n=25000)\n",
    "        ##dictionary_name.compactify()\n",
    "        #data[\"name\"] = list(zip(pad_sequences(data[\"name\"].progress_apply(lambda x: \n",
    "        #    dictionary_name.doc2idx(bigram[x])).values, maxlen=7, value=-1.0, padding='post', truncating='post') + 1))\n",
    "\n",
    "        #del dictionary_name, sentence_stream\n",
    "        \n",
    "        #data[\"item_description\"] = data['item_description'].progress_apply(lambda x: str(x).lower().split())\n",
    "        #dictionary_desc = corpora.Dictionary([bigram[sent] for sent in tqdm(data['item_description'].values)])\n",
    "        #print(dictionary_desc)\n",
    "        #dictionary_desc.filter_extremes(keep_n=80000)\n",
    "        #dictionary_desc.compactify()\n",
    "        #data[\"item_description\"] = list(zip(pad_sequences(data[\"item_description\"].progress_apply(lambda x: \n",
    "        #    dictionary_desc.doc2idx(bigram[x])).values, maxlen=70, value=-1.0, padding='post', truncating='post') + 1))\n",
    "        \n",
    "        #del dictionary_desc\n",
    "        data['item_desc2gram'] = data.item_description.apply(lambda x: add_ngrams(x, 2))\n",
    "        print(\"Tokenizing data\")\n",
    "        tok_name  = Tokenizer(20000)\n",
    "        tok_name.fit_on_texts(data['name'].astype(str))\n",
    "        print(len(tok_name.word_index))\n",
    "        \n",
    "        tok_desc= Tokenizer(100000)\n",
    "        tok_desc.fit_on_texts(data['item_description'].astype(str))\n",
    "        print(len(tok_desc.word_index))\n",
    "        \n",
    "        tok_desc2 = Tokenizer(10000)\n",
    "        tok_desc2.fit_on_texts(data['item_desc2gram'].astype(str))\n",
    "        print(len(tok_desc2.word_index))\n",
    "        \n",
    "        data[\"item_name\"] = list(zip(sequence.pad_sequences(tok_name.texts_to_sequences(data.item_description.astype(str)),\n",
    "                                         maxlen=20, padding='post', truncating='post')))\n",
    "        \n",
    "        data[\"name\"] = list(zip(sequence.pad_sequences(tok_name.texts_to_sequences(data.name.astype(str)),\n",
    "                                         maxlen=7, padding='post', truncating='post')))\n",
    "        \n",
    "        data[\"item_description\"] = list(zip(sequence.pad_sequences(tok_desc.texts_to_sequences(data.item_description.astype(str)),\n",
    "                                         maxlen=80, padding='post', truncating='post')))\n",
    "        \n",
    "        \n",
    "        data[\"item_desc2gram\"] = list(zip(sequence.pad_sequences(tok_desc2.texts_to_sequences(data.item_desc2gram.astype(str)),\n",
    "                                         maxlen=20, padding='post', truncating='post')))\n",
    "        #tkn_desc = Tokenizer(50000)\n",
    "        #tkn_desc.fit_on_texts(data.item_description.astype(str))\n",
    "        #data['desc_seq'] = pad_sequences(tkn_desc.texts_to_sequences(data.item_description.astype(str)),\n",
    "        #                                 maxlen=100, padding='post', truncating='post')\n",
    "        \n",
    "        #tkn_name = Tokenizer(4000)\n",
    "        #tkn_name.fit_on_texts(data.name.astype(str))\n",
    "        #data['name_seq'] = pad_sequences(tkn_name.texts_to_sequences(data.name.astype(str)),\n",
    "        #                                 maxlen=6, padding='post', truncating='post')\n",
    "        \n",
    "        \n",
    "        train_data = data.loc[: train_rows - 1, :].reset_index(drop=True)\n",
    "        train_data = train_data.loc[(train_data.price >= 3) & (train_data.price <= 2000), :].reset_index(drop=True)\n",
    "        test_data  = data.loc[train_rows: , :].reset_index(drop=True)\n",
    "        \n",
    "        del train_data['test_id']\n",
    "        del test_data['train_id']\n",
    "        del data \n",
    "        test_data['test_id'] = test_data['test_id'].astype(int)\n",
    "        #train_data.to_pickle(os.path.join(out_path, 'train_2.pkl'))\n",
    "        #test_data.to_pickle(os.path.join(out_path, 'test_2.pkl'))\n",
    "        \n",
    "        return train_data, test_data\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train  =pd.read_table(\"../input/train.tsv\")\n",
    "#train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train[\"name_description\"] = train[\"name\"].astype(str) + ' ' + train[\"item_description\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence_stream = [doc.lower().split(\" \") for doc in tqdm(train[\"name_description\"].tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#stoplist = set('for a of the and to in'.split())\n",
    "#bigram = Phrases(sentence_stream, min_count=5, threshold=10, common_terms=stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#dictionary = corpora.Dictionary([bigram[sent] for sent in sentence_stream])\n",
    "#print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del sentence_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#dictionary.filter_extremes(keep_n=200000)\n",
    "#print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary.doc2idx(bigram[train.name.iloc[0].lower().split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#pad_sequences(train[\"name\"].progress_apply(lambda x: dictionary.doc2idx(bigram[x.lower().split()])).values, maxlen=7, value=-1.0, padding='post') + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.engine.topology import Layer\n",
    "class ZeroMaskedEntries(Layer):\n",
    "    \"\"\"\n",
    "    This layer is called after an Embedding layer.\n",
    "    It zeros out all of the masked-out embeddings.\n",
    "    It also swallows the mask without passing it on.\n",
    "    You can change this to default pass-on behavior as follows:\n",
    "\n",
    "    def compute_mask(self, x, mask=None):\n",
    "        if not self.mask_zero:\n",
    "            return None\n",
    "        else:\n",
    "            return K.not_equal(x, 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.support_mask = True\n",
    "        super(ZeroMaskedEntries, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.output_dim = input_shape[1]\n",
    "        self.repeat_dim = input_shape[2]\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        #print(mask.shape)\n",
    "        mask = K.cast(mask, 'float32')\n",
    "        mask = K.repeat(mask, self.repeat_dim)\n",
    "        #print(mask.shape)\n",
    "        mask = K.permute_dimensions(mask, (0, 2, 1))\n",
    "        return x * mask\n",
    "\n",
    "    def compute_mask(self, input_shape, input_mask=None):\n",
    "        return None\n",
    "    \n",
    "def mask_aware_mean(x):\n",
    "    # recreate the masks - all zero rows have been masked\n",
    "    #mask = K.not_equal(K.sum(K.abs(x), axis=2, keepdims=True), 0)\n",
    "\n",
    "    # number of that rows are not all zeros\n",
    "    #n = K.sum(K.cast(mask, 'float32'), axis=1, keepdims=False)\n",
    "    # compute mask-aware mean of x\n",
    "    x_mean = K.sum(x, axis=1, keepdims=False)\n",
    "    #print(x_mean.shape)\n",
    "    return x_mean\n",
    "\n",
    "def mask_aware_mean_output_shape(input_shape):\n",
    "    shape = list(input_shape)\n",
    "    assert len(shape) == 3 \n",
    "    return (shape[0], shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EM_NNRegressor(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, embed_cols=None, dense_cols=None, embed_dims=None, \n",
    "                 text_embed_cols=None, text_embed_seq_lens=None, \n",
    "                 text_embed_dims=None, \n",
    "                 #text_embed_tokenizers=None,\n",
    "                 num_layers=2, multiprocess=False,\n",
    "                layer_activations=None, layer_dims=None,layer_dropouts=None, epochs=20, batchsize=32,\n",
    "                optimizer_kwargs=None, val_size=0.1, verbose=1, seed=1, lr_lr=0.005, lr_decay=0.001):\n",
    "        \n",
    "        self.embed_cols = embed_cols\n",
    "        self.dense_cols = dense_cols\n",
    "        self.embed_dims = embed_dims\n",
    "        self.text_embed_cols = text_embed_cols\n",
    "        self.text_embed_dims = text_embed_dims\n",
    "        #self.text_embed_tokenizers = text_embed_tokenizers\n",
    "        self.text_embed_seq_lens = text_embed_seq_lens\n",
    "        self.dense_dims = None\n",
    "        self.num_layers = num_layers\n",
    "        self.layer_dims = layer_dims\n",
    "        self.layer_activations = layer_activations\n",
    "        self.layer_dropouts = layer_dropouts\n",
    "        self.epochs = epochs\n",
    "        self.batchsize = batchsize\n",
    "        self.optimizer_kwargs = optimizer_kwargs\n",
    "        self.val_size = val_size\n",
    "        self.verbose = verbose\n",
    "        self.multiprocess = multiprocess\n",
    "        self.seed = seed\n",
    "        self.lr_lr=lr_lr\n",
    "        self.lr_decay=lr_decay\n",
    "        #self.optim = optim\n",
    "        self.model = None\n",
    "        if self.dense_cols:\n",
    "            self.dense_dims = len(self.dense_cols)\n",
    "            \n",
    "    def _splitX(self, X):\n",
    "        X_splits = []\n",
    "        \n",
    "        if self.embed_cols:\n",
    "            for col in self.embed_cols :\n",
    "                X_splits.append(X[col].values.reshape(X.shape[0], -1))\n",
    "                \n",
    "        if self.text_embed_cols:\n",
    "            for i, col in enumerate(self.text_embed_cols):\n",
    "                #max_features = self.text_embed_dims[i][0]\n",
    "                #max_len = self.text_embed_seq_lens[i]\n",
    "                #input_text = X[col].astype(str)\n",
    "                #x_train = tok.texts_to_sequences(input_text)\n",
    "                #print(np.mean([len(l) for l in x_train]))\n",
    "                #x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "                #X_splits.append(np.array(x_train).reshape(X.shape[0], -1))\n",
    "                X_splits.append(np.concatenate(X[col].values))\n",
    "                \n",
    "        if self.dense_cols:\n",
    "            X_splits.append(X[self.dense_cols].values.reshape(X.shape[0], -1))\n",
    "            \n",
    "        return X_splits\n",
    "    \n",
    "    \n",
    "    def _build_model(self):\n",
    "        model_inputs = []\n",
    "        model_layers = []\n",
    "        \n",
    "        if self.embed_cols:\n",
    "            for col, dim in zip(self.embed_cols, self.embed_dims):\n",
    "                x1 = Input( shape=(1,), name=col)\n",
    "                model_inputs.append(x1)\n",
    "                x1 = Embedding(input_dim=dim[0], output_dim=dim[1], )(x1)\n",
    "                #x1 = Dropout(0.1)(x1)\n",
    "                x1 = Reshape(target_shape=(dim[1],))(x1)\n",
    "                model_layers.append(x1)\n",
    "                \n",
    "        if self.text_embed_cols:\n",
    "            for col, dim, seq_len in zip(self.text_embed_cols, \n",
    "                                                self.text_embed_dims, \n",
    "                                                self.text_embed_seq_lens):\n",
    "                x3 = Input( shape=(seq_len,))\n",
    "                model_inputs.append(x3)\n",
    "                x3 = Embedding(input_dim=dim[0], output_dim=dim[1], input_length=seq_len,\n",
    "                               )(x3)\n",
    "                #x3 = Bidirectional(GRU(32, return_sequences=True))(x3)\n",
    "                x3 = GlobalAveragePooling1D()(x3)\n",
    "                #x3 = GlobalAvgPool1D()(x3)\n",
    "                #x3 = Reshape(target_shape=(dim[1],))(x3)\n",
    "                #x3 = Flatten()(x3)\n",
    "                #x3 = ZeroMaskedEntries()(x3)\n",
    "                #x3 = Lambda(mask_aware_mean, mask_aware_mean_output_shape)(x3)\n",
    "                model_layers.append(x3)\n",
    "                \n",
    "        if self.dense_cols:\n",
    "            x2 = Input( shape=(self.dense_dims, ), name='dense_cols')\n",
    "            model_inputs.append(x2)\n",
    "            model_layers.append(x2)\n",
    "        print(model_layers)\n",
    "        x = concatenate(model_layers)\n",
    "        \n",
    "        if self.num_layers > 0:\n",
    "            for dim, drops in zip(self.layer_dims, self.layer_dropouts):\n",
    "                x = BatchNormalization()(x)\n",
    "                x = Dropout(rate=drops)(x)\n",
    "                x = Dense(dim, activation='selu',kernel_initializer='he_normal')(x)\n",
    "                x = PReLU()(x)\n",
    "        \n",
    "        #x = concatenate([x, *model_layers[:3]])\n",
    "        #x = BatchNormalization()(x)\n",
    "        #x = Dropout(0.1)(x)\n",
    "        #x = Dense(100, kernel_initializer='he_normal')(x)\n",
    "        #x = LeakyReLU()(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.03)(x)\n",
    "        output = Dense(1,  kernel_initializer='he_normal')(x)\n",
    "        \n",
    "        model = Model(inputs=model_inputs, outputs=output)\n",
    "        #print(model.summary())\n",
    "        #adam = Nadam(lr=0.002, schedule_decay=0.02)\n",
    "        adam = Adam(lr=self.lr_lr, decay=self.lr_decay)\n",
    "        #adam = SGD(lr=0.01, nesterov=True, momentum=0.9, decay=0.003)\n",
    "        #adam = RMSprop(lr=0.008, decay=0.006)\n",
    "        #adam = self.optim\n",
    "        model.compile(optimizer=adam, loss='mean_squared_error' )\n",
    "        \n",
    "        return model \n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model = self._build_model()\n",
    "        if self.val_size > 0:\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.val_size, random_state=self.seed)\n",
    "            print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "            \n",
    "            callbacks= [ModelCheckpoint(\"embed_NN_\"+str(self.seed)+\".check\", save_best_only=True, verbose=1)]\n",
    "            if self.multiprocess == False:\n",
    "                self.model.fit(self._splitX(X_train), y_train, batch_size=self.batchsize, epochs=self.epochs,\n",
    "                               verbose=self.verbose,\n",
    "                              validation_data=(self._splitX(X_val), y_val), shuffle=True,\n",
    "                              callbacks=callbacks)\n",
    "            else:\n",
    "                X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.val_size, random_state=1)\n",
    "\n",
    "        else:\n",
    "            self.model.fit(self._splitX(X), y, batch_size=self.batchsize, epochs=self.epochs,\n",
    "               verbose=self.verbose, shuffle=True)\n",
    "\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        \n",
    "        if self.model:\n",
    "            if self.val_size > 0:\n",
    "                model = load_model(\"embed_NN_\"+str(self.seed)+\".check\")\n",
    "                y_hat = model.predict(self._splitX(X))\n",
    "            else:\n",
    "                y_hat = model.predict(self._splitX(X))\n",
    "        else:\n",
    "            raise ValueError(\"Model not fit yet\")\n",
    "            \n",
    "        return y_hat\n",
    "        \n",
    "def add_ngrams(text, ngram=2):\n",
    "    word_list = str(text).lower().split(' ')\n",
    "    out_list = [''.join(word_list[i:i+ngram]) for i in range(len(word_list))]\n",
    "    return ' '.join(out_list[:-1])\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting word/char len features\n",
      "Get iphone features\n",
      "Get count features\n",
      "Index(['brand_name', 'y'], dtype='object')\n",
      "Index(['category_name', 'y'], dtype='object')\n",
      "Index(['cat1', 'y'], dtype='object')\n",
      "Index(['cat2', 'y'], dtype='object')\n",
      "Index(['cat3', 'y'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2175894/2175894 [00:17<00:00, 125178.84it/s]\n",
      "100%|██████████| 2175894/2175894 [00:12<00:00, 178444.19it/s]\n",
      "100%|██████████| 2175894/2175894 [01:19<00:00, 27400.67it/s]\n",
      "100%|██████████| 2175894/2175894 [00:17<00:00, 125543.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data\n",
      "134841\n",
      "224064\n",
      "3720109\n",
      "(1481658, 35) (693359, 35)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>item_description</th>\n",
       "      <th>name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>train_id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>...</th>\n",
       "      <th>cat1_counts</th>\n",
       "      <th>cat2_counts</th>\n",
       "      <th>cat3_counts</th>\n",
       "      <th>plus_counts</th>\n",
       "      <th>ands_counts</th>\n",
       "      <th>comma_counts</th>\n",
       "      <th>all_counts</th>\n",
       "      <th>name_description</th>\n",
       "      <th>item_desc2gram</th>\n",
       "      <th>item_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>831</td>\n",
       "      <td>3</td>\n",
       "      <td>([12, 63, 68, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>([2641, 4717, 5005, 57, 15, 4, 56],)</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141360</td>\n",
       "      <td>0.154083</td>\n",
       "      <td>0.252631</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MLB Cincinnati Reds T Shirt Size XL No descrip...</td>\n",
       "      <td>([14, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>([453, 1683, 13268, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3891</td>\n",
       "      <td>88</td>\n",
       "      <td>3</td>\n",
       "      <td>([24, 2955, 10, 5, 34, 17, 1, 196, 51, 19, 904...</td>\n",
       "      <td>([5721, 12851, 10190, 1572, 0, 0, 0],)</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185101</td>\n",
       "      <td>0.055355</td>\n",
       "      <td>0.016111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Razer BlackWidow Chroma Keyboard This keyboard...</td>\n",
       "      <td>([195, 77, 27, 768, 2445, 1968, 3183, 8740, 33...</td>\n",
       "      <td>([1093, 1572, 972, 88, 819, 955, 8, 244, 252, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4590</td>\n",
       "      <td>1279</td>\n",
       "      <td>1</td>\n",
       "      <td>([532, 88, 8, 3, 4601, 11, 251, 1, 3, 954, 107...</td>\n",
       "      <td>([3866, 6142, 197, 0, 0, 0, 0],)</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10</td>\n",
       "      <td>105</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.798175</td>\n",
       "      <td>0.338567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AVA VIV Blouse Adorable top with a hint of lac...</td>\n",
       "      <td>([1318, 68, 9390, 6607, 184, 5206, 35, 199, 15...</td>\n",
       "      <td>([1364, 19, 77, 156, 25, 84, 8, 156, 641, 4683...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>505</td>\n",
       "      <td>1</td>\n",
       "      <td>([6, 8, 59, 195, 6630, 189, 4, 21, 142, 1056, ...</td>\n",
       "      <td>([119, 1603, 11469, 0, 0, 0, 0],)</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102457</td>\n",
       "      <td>0.187833</td>\n",
       "      <td>0.217378</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Leather Horse Statues New with tags  Leather h...</td>\n",
       "      <td>([30, 45, 24, 525, 124, 1898, 10, 1, 1, 132, 1...</td>\n",
       "      <td>([3, 77, 769, 119, 5179, 2586, 6, 136, 2134, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1206</td>\n",
       "      <td>1</td>\n",
       "      <td>([746, 8, 6145, 11, 1689, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>([3528, 45, 913, 138, 0, 0, 0],)</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.458721</td>\n",
       "      <td>0.328417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24K GOLD plated rose Complete with certificate...</td>\n",
       "      <td>([4337, 6842, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>([1058, 77, 16091, 25, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   brand_name  category_name  item_condition_id  \\\n",
       "0           3            831                  3   \n",
       "1        3891             88                  3   \n",
       "2        4590           1279                  1   \n",
       "3           3            505                  1   \n",
       "4           3           1206                  1   \n",
       "\n",
       "                                    item_description  \\\n",
       "0  ([12, 63, 68, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1  ([24, 2955, 10, 5, 34, 17, 1, 196, 51, 19, 904...   \n",
       "2  ([532, 88, 8, 3, 4601, 11, 251, 1, 3, 954, 107...   \n",
       "3  ([6, 8, 59, 195, 6630, 189, 4, 21, 142, 1056, ...   \n",
       "4  ([746, 8, 6145, 11, 1689, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "\n",
       "                                     name  price  shipping  train_id  cat1  \\\n",
       "0    ([2641, 4717, 5005, 57, 15, 4, 56],)   10.0         1       0.0     6   \n",
       "1  ([5721, 12851, 10190, 1572, 0, 0, 0],)   52.0         0       1.0     2   \n",
       "2        ([3866, 6142, 197, 0, 0, 0, 0],)   10.0         1       2.0    10   \n",
       "3       ([119, 1603, 11469, 0, 0, 0, 0],)   35.0         1       3.0     4   \n",
       "4        ([3528, 45, 913, 138, 0, 0, 0],)   44.0         0       4.0    10   \n",
       "\n",
       "   cat2                        ...                          cat1_counts  \\\n",
       "0   104                        ...                             0.141360   \n",
       "1    32                        ...                             0.185101   \n",
       "2   105                        ...                             1.000000   \n",
       "3    57                        ...                             0.102457   \n",
       "4    60                        ...                             1.000000   \n",
       "\n",
       "   cat2_counts  cat3_counts  plus_counts  ands_counts  comma_counts  \\\n",
       "0     0.154083     0.252631          0.0          0.0           0.0   \n",
       "1     0.055355     0.016111          0.0          0.0           0.0   \n",
       "2     0.798175     0.338567          0.0          0.0           0.0   \n",
       "3     0.187833     0.217378          0.0          0.0           0.0   \n",
       "4     0.458721     0.328417          0.0          0.0           0.0   \n",
       "\n",
       "   all_counts                                   name_description  \\\n",
       "0         0.0  MLB Cincinnati Reds T Shirt Size XL No descrip...   \n",
       "1         0.0  Razer BlackWidow Chroma Keyboard This keyboard...   \n",
       "2         0.0  AVA VIV Blouse Adorable top with a hint of lac...   \n",
       "3         0.0  Leather Horse Statues New with tags  Leather h...   \n",
       "4         0.0  24K GOLD plated rose Complete with certificate...   \n",
       "\n",
       "                                      item_desc2gram  \\\n",
       "0  ([14, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  ([195, 77, 27, 768, 2445, 1968, 3183, 8740, 33...   \n",
       "2  ([1318, 68, 9390, 6607, 184, 5206, 35, 199, 15...   \n",
       "3  ([30, 45, 24, 525, 124, 1898, 10, 1, 1, 132, 1...   \n",
       "4  ([4337, 6842, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "\n",
       "                                           item_name  \n",
       "0  ([453, 1683, 13268, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "1  ([1093, 1572, 972, 88, 819, 955, 8, 244, 252, ...  \n",
       "2  ([1364, 19, 77, 156, 25, 84, 8, 156, 641, 4683...  \n",
       "3  ([3, 77, 769, 119, 5179, 2586, 6, 136, 2134, 9...  \n",
       "4  ([1058, 77, 16091, 25, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read data\n",
    "train_data, test_data = read_data(\"../input\", \"./\")\n",
    "print(train_data.shape, test_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#desc2len = train_data.item_desc2gram.apply(lambda x: sum([el > 0 for el in x]))\n",
    "#plt.hist(desc2len)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data\n",
    "y = np.log1p(train_data.price)\n",
    "\n",
    "cvlist= list(KFold(5, random_state=786).split(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet1 = EM_NNRegressor(embed_cols=['brand_name','category_name','item_condition_id', 'cat1', 'cat2', 'cat3'], \n",
    "                  embed_dims=[(6000, 50),(1500, 50), (6,4), (16,4), (121, 10), (900, 20)],\n",
    "                  text_embed_cols=['name', 'item_description', \n",
    "                                   'item_desc2gram', 'item_name'\n",
    "                                  ],\n",
    "                  text_embed_dims=[(20000, 40), (100000, 30), \n",
    "                                   (10000, 20), (20000, 20)\n",
    "                                  ],\n",
    "                  text_embed_seq_lens =[7, 80, \n",
    "                                       20, 20\n",
    "                                       ],\n",
    "                  #text_embed_tokenizers = [tok_name, tok_desc, tok_desc2],\n",
    "                  dense_cols=['shipping', 'desc_words', 'desc_chars', 'name_chars','name_words',\n",
    "                                'iphone_case', 'iphone6', 'iphone6p',\n",
    "                                'iphone5', 'iphone5p', 'iphone7', 'iphone7p', 'unlocked_phone',\n",
    "                              'brand_counts', 'cat_counts',\n",
    "                                   'cat1_counts', 'cat2_counts', 'cat3_counts',\n",
    "                              \"plus_counts\", \"comma_counts\", \"ands_counts\", 'all_counts',\n",
    "                                  ],\n",
    "                  epochs=5,\n",
    "                  batchsize=2048 ,\n",
    "                  num_layers = 1,\n",
    "                  layer_dropouts=[0.1],\n",
    "                  layer_dims=[300],\n",
    "                  seed=1,\n",
    "                  val_size=0.025,\n",
    "                 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_157/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_158/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_159/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_160/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_161/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_162/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'global_average_pooling1d_105/Mean:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'global_average_pooling1d_106/Mean:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'global_average_pooling1d_107/Mean:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'global_average_pooling1d_108/Mean:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'dense_cols_49:0' shape=(?, 22) dtype=float32>]\n",
      "(1155692, 35) (29634, 35) (1155692,) (29634,)\n",
      "Train on 1155692 samples, validate on 29634 samples\n",
      "Epoch 1/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.5467Epoch 00001: val_loss improved from inf to 0.27394, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 23s 20us/step - loss: 0.5465 - val_loss: 0.2739\n",
      "Epoch 2/5\n",
      "1150976/1155692 [============================>.] - ETA: 0s - loss: 0.1817Epoch 00002: val_loss improved from 0.27394 to 0.18198, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 13s 11us/step - loss: 0.1816 - val_loss: 0.1820\n",
      "Epoch 3/5\n",
      "1153024/1155692 [============================>.] - ETA: 0s - loss: 0.1624Epoch 00003: val_loss did not improve\n",
      "1155692/1155692 [==============================] - 13s 11us/step - loss: 0.1624 - val_loss: 0.1958\n",
      "Epoch 4/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.1517Epoch 00004: val_loss improved from 0.18198 to 0.17477, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 13s 12us/step - loss: 0.1516 - val_loss: 0.1748\n",
      "Epoch 5/5\n",
      "1150976/1155692 [============================>.] - ETA: 0s - loss: 0.1448Epoch 00005: val_loss did not improve\n",
      "1155692/1155692 [==============================] - 13s 11us/step - loss: 0.1448 - val_loss: 0.1752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_163/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_164/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_165/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_166/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_167/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_168/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'global_average_pooling1d_109/Mean:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'global_average_pooling1d_110/Mean:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'global_average_pooling1d_111/Mean:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'global_average_pooling1d_112/Mean:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'dense_cols_51:0' shape=(?, 22) dtype=float32>]\n",
      "(1155692, 35) (29634, 35) (1155692,) (29634,)\n",
      "Train on 1155692 samples, validate on 29634 samples\n",
      "Epoch 1/5\n",
      "1150976/1155692 [============================>.] - ETA: 0s - loss: 0.5572Epoch 00001: val_loss improved from inf to 0.25247, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 24s 21us/step - loss: 0.5558 - val_loss: 0.2525\n",
      "Epoch 2/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.1831Epoch 00002: val_loss improved from 0.25247 to 0.20854, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 13s 11us/step - loss: 0.1831 - val_loss: 0.2085\n",
      "Epoch 3/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.1632Epoch 00003: val_loss improved from 0.20854 to 0.17995, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1632 - val_loss: 0.1800\n",
      "Epoch 4/5\n",
      "1150976/1155692 [============================>.] - ETA: 0s - loss: 0.1524Epoch 00004: val_loss improved from 0.17995 to 0.17580, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 13s 11us/step - loss: 0.1524 - val_loss: 0.1758\n",
      "Epoch 5/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.1451Epoch 00005: val_loss improved from 0.17580 to 0.17418, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 13s 11us/step - loss: 0.1451 - val_loss: 0.1742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  4.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_169/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_170/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_171/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_172/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_173/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_174/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'global_average_pooling1d_113/Mean:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'global_average_pooling1d_114/Mean:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'global_average_pooling1d_115/Mean:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'global_average_pooling1d_116/Mean:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'dense_cols_53:0' shape=(?, 22) dtype=float32>]\n",
      "(1155692, 35) (29634, 35) (1155692,) (29634,)\n",
      "Train on 1155692 samples, validate on 29634 samples\n",
      "Epoch 1/5\n",
      "1150976/1155692 [============================>.] - ETA: 0s - loss: 0.5773Epoch 00001: val_loss improved from inf to 0.25062, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 24s 21us/step - loss: 0.5757 - val_loss: 0.2506\n",
      "Epoch 2/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.1818Epoch 00002: val_loss improved from 0.25062 to 0.18066, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1818 - val_loss: 0.1807\n",
      "Epoch 3/5\n",
      "1150976/1155692 [============================>.] - ETA: 0s - loss: 0.1623Epoch 00003: val_loss improved from 0.18066 to 0.17638, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 13s 12us/step - loss: 0.1622 - val_loss: 0.1764\n",
      "Epoch 4/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.1515Epoch 00004: val_loss improved from 0.17638 to 0.17400, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 13s 11us/step - loss: 0.1515 - val_loss: 0.1740\n",
      "Epoch 5/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.1445Epoch 00005: val_loss improved from 0.17400 to 0.17340, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 13s 11us/step - loss: 0.1445 - val_loss: 0.1734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  6.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_175/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_176/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_177/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_178/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_179/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_180/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'global_average_pooling1d_117/Mean:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'global_average_pooling1d_118/Mean:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'global_average_pooling1d_119/Mean:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'global_average_pooling1d_120/Mean:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'dense_cols_55:0' shape=(?, 22) dtype=float32>]\n",
      "(1155693, 35) (29634, 35) (1155693,) (29634,)\n",
      "Train on 1155693 samples, validate on 29634 samples\n",
      "Epoch 1/5\n",
      "1153024/1155693 [============================>.] - ETA: 0s - loss: 0.5346Epoch 00001: val_loss improved from inf to 0.28374, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 25s 21us/step - loss: 0.5339 - val_loss: 0.2837\n",
      "Epoch 2/5\n",
      "1150976/1155693 [============================>.] - ETA: 0s - loss: 0.1816Epoch 00002: val_loss improved from 0.28374 to 0.18344, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.1816 - val_loss: 0.1834\n",
      "Epoch 3/5\n",
      "1155072/1155693 [============================>.] - ETA: 0s - loss: 0.1623Epoch 00003: val_loss improved from 0.18344 to 0.18173, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 13s 12us/step - loss: 0.1623 - val_loss: 0.1817\n",
      "Epoch 4/5\n",
      "1150976/1155693 [============================>.] - ETA: 0s - loss: 0.1516Epoch 00004: val_loss improved from 0.18173 to 0.17261, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.1516 - val_loss: 0.1726\n",
      "Epoch 5/5\n",
      "1153024/1155693 [============================>.] - ETA: 0s - loss: 0.1447Epoch 00005: val_loss did not improve\n",
      "1155693/1155693 [==============================] - 13s 11us/step - loss: 0.1447 - val_loss: 0.1731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  8.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_181/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_182/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_183/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_184/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_185/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_186/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'global_average_pooling1d_121/Mean:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'global_average_pooling1d_122/Mean:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'global_average_pooling1d_123/Mean:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'global_average_pooling1d_124/Mean:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'dense_cols_57:0' shape=(?, 22) dtype=float32>]\n",
      "(1155693, 35) (29634, 35) (1155693,) (29634,)\n",
      "Train on 1155693 samples, validate on 29634 samples\n",
      "Epoch 1/5\n",
      "1155072/1155693 [============================>.] - ETA: 0s - loss: 0.5159Epoch 00001: val_loss improved from inf to 0.24684, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 26s 22us/step - loss: 0.5158 - val_loss: 0.2468\n",
      "Epoch 2/5\n",
      "1153024/1155693 [============================>.] - ETA: 0s - loss: 0.1806Epoch 00002: val_loss improved from 0.24684 to 0.17845, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 13s 11us/step - loss: 0.1806 - val_loss: 0.1785\n",
      "Epoch 3/5\n",
      "1155072/1155693 [============================>.] - ETA: 0s - loss: 0.1616Epoch 00003: val_loss improved from 0.17845 to 0.17518, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 13s 12us/step - loss: 0.1616 - val_loss: 0.1752\n",
      "Epoch 4/5\n",
      "1153024/1155693 [============================>.] - ETA: 0s - loss: 0.1509Epoch 00004: val_loss did not improve\n",
      "1155693/1155693 [==============================] - 13s 12us/step - loss: 0.1509 - val_loss: 0.1774\n",
      "Epoch 5/5\n",
      "1155072/1155693 [============================>.] - ETA: 0s - loss: 0.1439Epoch 00005: val_loss improved from 0.17518 to 0.17317, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 13s 12us/step - loss: 0.1439 - val_loss: 0.1732\n",
      "0.7477284 8.297203\n",
      "0.4153918218540626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 10.9min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 10.9min finished\n"
     ]
    }
   ],
   "source": [
    "oof_preds1 = cross_val_predict(nnet1, X, y, verbose=10, cv=cvlist)\n",
    "score = rmse(y, oof_preds1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV, gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt import dump\n",
    "\n",
    "def optimize_nn( X, y, cvlist, save_path=\"../input/\"):\n",
    "    space  = [(5, 80),                           # brand embeddings\n",
    "              (5, 80),                              # category embedding\n",
    "              (2, 10),                            # item condition embedding\n",
    "              (2, 10),                            # cat1 embedding\n",
    "              (2, 20),                             # cat2 embedding\n",
    "              (2, 80),                              # cat3 embedding\n",
    "              (20, 200),                         #name embedding\n",
    "              (20, 200),                         #desc embedding\n",
    "              (20, 100),                         #desc2 embedding\n",
    "              (10, 100),                         #name desc\n",
    "              (50, 500),                         #dense layer\n",
    "              (0.01, 0.5),                        #dense dropout \n",
    "              (0.0005, 0.01),                      #lr\n",
    "              (0.0001, 0.01) ]                     #lr decay\n",
    "              \n",
    "    def objective(params):\n",
    "        gc.collect()\n",
    "        brand_dim, category_dim, condition_dim, cat1_dim, cat2_dim, cat3_dim, \\\n",
    "        name_dim, desc_dim, desc2_dim, namedesc_dim, dense_dim, dense_drop, lr, lr_decay = params\n",
    "        nnet2 = EM_NNRegressor(embed_cols=['brand_name','category_name','item_condition_id', 'cat1', 'cat2', 'cat3'], \n",
    "                      embed_dims=[(6000, brand_dim),\n",
    "                                  (1500, category_dim), \n",
    "                                  (6, condition_dim), \n",
    "                                  (16,cat1_dim), \n",
    "                                  (121, cat2_dim), \n",
    "                                  (900, cat3_dim)],\n",
    "                      text_embed_cols=['name', 'item_description', 'item_desc2gram', 'item_name'],\n",
    "                      text_embed_dims=[(20000, name_dim), (100000, desc_dim), (10000, desc2_dim), (20000, namedesc_dim)],\n",
    "                      text_embed_seq_lens =[7, 80, 20, 20],\n",
    "                      #text_embed_cols=['name'],\n",
    "                      #text_embed_dims=[(50000, 80)],\n",
    "                      #text_embed_seq_lens =[10], \n",
    "                      dense_cols=['shipping', 'desc_words', 'desc_chars', 'name_chars',\n",
    "                                'iphone_case', 'iphone6', 'iphone6p',\n",
    "                                'iphone5', 'iphone5p', 'iphone7', 'iphone7p', 'unlocked_phone',\n",
    "                              'brand_counts', 'cat_counts',\n",
    "                                   'cat1_counts', 'cat2_counts', 'cat3_counts'],\n",
    "                      epochs=5,\n",
    "                      batchsize=2048,\n",
    "                      num_layers = 1,\n",
    "                      layer_dropouts=[dense_drop],\n",
    "                      layer_dims=[dense_dim],\n",
    "                      val_size=0,\n",
    "                      seed=786,\n",
    "                      lr_lr=lr,\n",
    "                      lr_decay=lr_decay,\n",
    "                     )\n",
    "        print(\"parameters...................\",params)\n",
    "        #scores = cross_val_score(nnet2, X, y, cv=cvlist, verbose=1, scoring=rmse_sklearn, n_jobs=1,\n",
    "        #                         pre_dispatch=None)\n",
    "        preds = cross_val_predict(nnet2, X, y, cv=cvlist, verbose=1, n_jobs=1)\n",
    "        gc.collect()\n",
    "        #score = np.mean(scores)\n",
    "        score = rmse(y,preds)\n",
    "        del nnet2, preds\n",
    "        print(\"score........................\", score)\n",
    "        #preds_test = est.set_params(**lgb_params).fit(X, y).predict_proba_corr(X_test)\n",
    "        #preds_dict = {'params':lgb_params, 'train_preds': preds, 'test_preds':preds_test}\n",
    "        #filepath = os.path.join(save_path, 'nn_'+str(score))\n",
    "        #with open(filepath, \"wb\") as f:\n",
    "        #    pickle.dump(preds_dict, f)\n",
    "        #print(\"Saved..............:\", filepath)\n",
    "        return score\n",
    "    gc.collect()\n",
    "    res = gp_minimize(objective,                  # the function to minimize\n",
    "                      space,                          # the bounds on each dimension of x\n",
    "                      acq_func=\"EI\",                  # the acquisition function\n",
    "                      n_calls=30,                     # the number of evaluations of f \n",
    "                      n_random_starts=5,             # the number of random initialization points\n",
    "                      random_state=786,\n",
    "                     verbose=True) \n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "parameters................... [62, 28, 9, 5, 18, 8, 88, 79, 95, 53, 300, 0.17400593951308432, 0.0031915051527402006, 0.005159879136097237]\n",
      "[<tf.Tensor 'reshape_13/Reshape:0' shape=(?, 62) dtype=float32>, <tf.Tensor 'reshape_14/Reshape:0' shape=(?, 28) dtype=float32>, <tf.Tensor 'reshape_15/Reshape:0' shape=(?, 9) dtype=float32>, <tf.Tensor 'reshape_16/Reshape:0' shape=(?, 5) dtype=float32>, <tf.Tensor 'reshape_17/Reshape:0' shape=(?, 18) dtype=float32>, <tf.Tensor 'reshape_18/Reshape:0' shape=(?, 8) dtype=float32>, <tf.Tensor 'global_average_pooling1d_9/Mean:0' shape=(?, 88) dtype=float32>, <tf.Tensor 'global_average_pooling1d_10/Mean:0' shape=(?, 79) dtype=float32>, <tf.Tensor 'global_average_pooling1d_11/Mean:0' shape=(?, 95) dtype=float32>, <tf.Tensor 'global_average_pooling1d_12/Mean:0' shape=(?, 53) dtype=float32>, <tf.Tensor 'dense_cols_2:0' shape=(?, 17) dtype=float32>]\n",
      "Epoch 1/5\n",
      "1185326/1185326 [==============================] - 18s 15us/step - loss: 0.6916\n",
      "Epoch 2/5\n",
      "1185326/1185326 [==============================] - 17s 14us/step - loss: 0.2224\n",
      "Epoch 3/5\n",
      "1185326/1185326 [==============================] - 17s 14us/step - loss: 0.1963\n",
      "Epoch 4/5\n",
      "1185326/1185326 [==============================] - 17s 14us/step - loss: 0.1804\n",
      "Epoch 5/5\n",
      "1185326/1185326 [==============================] - 17s 14us/step - loss: 0.1692\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown layer: ZeroMaskedEntries",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-fd38d06598ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcvlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog1p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcvlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-1f5fd724544d>\u001b[0m in \u001b[0;36moptimize_nn\u001b[0;34m(X, y, cvlist, save_path)\u001b[0m\n\u001b[1;32m     73\u001b[0m                       \u001b[0mn_random_starts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m             \u001b[0;31m# the number of random initialization points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                       \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m786\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                      verbose=True) \n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/skopt/optimizer/gp.py\u001b[0m in \u001b[0;36mgp_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         callback=callback, n_jobs=n_jobs)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/skopt/optimizer/base.py\u001b[0m in \u001b[0;36mbase_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# no need to fit a model on the last iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mfit_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn_calls\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mnext_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-1f5fd724544d>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m#scores = cross_val_score(nnet2, X, y, cv=cvlist, verbose=1, scoring=rmse_sklearn, n_jobs=1,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m#                         pre_dispatch=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnnet2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcvlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m#score = np.mean(scores)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_predict\u001b[0;34m(estimator, X, y, groups, cv, n_jobs, verbose, fit_params, pre_dispatch, method)\u001b[0m\n\u001b[1;32m    678\u001b[0m     prediction_blocks = parallel(delayed(_fit_and_predict)(\n\u001b[1;32m    679\u001b[0m         clone(estimator), X, y, train, test, verbose, fit_params, method)\n\u001b[0;32m--> 680\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;31m# Concatenate the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_predict\u001b[0;34m(estimator, X, y, train, test, verbose, fit_params, method)\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'decision_function'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'predict_proba'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'predict_log_proba'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-2363e0f95448>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"embed_NN_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".check\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m             \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_splitX\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No model found in config file.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;31m# set weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    312\u001b[0m                         \u001b[0;34m'Maybe you meant to use '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                         '`Sequential.from_config(config)`?')\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 return cls.from_config(config['config'],\n\u001b[1;32m    139\u001b[0m                                        custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n\u001b[0;32m--> 140\u001b[0;31m                                                            list(custom_objects.items())))\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m   2488\u001b[0m         \u001b[0;31m# First, we create all layers and enqueue nodes to be processed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2489\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2490\u001b[0;31m             \u001b[0mprocess_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2491\u001b[0m         \u001b[0;31m# Then we process nodes in order of layer depth.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2492\u001b[0m         \u001b[0;31m# Nodes that cannot yet be processed (if the inbound node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mprocess_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m   2474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m             layer = deserialize_layer(layer_data,\n\u001b[0;32m-> 2476\u001b[0;31m                                       custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m   2477\u001b[0m             \u001b[0mcreated_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 raise ValueError('Unknown ' + printable_module_name +\n\u001b[0;32m--> 134\u001b[0;31m                                  ': ' + class_name)\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'from_config'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mcustom_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_objects\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown layer: ZeroMaskedEntries"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "cvlist = list(KFold(5, random_state=1).split(train_data))\n",
    "res = optimize_nn(train_data, np.log1p(train_data.price), cvlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EM_NNRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-227c0d825127>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m nnet2 = EM_NNRegressor(embed_cols=['brand_name','category_name','item_condition_id', 'cat1', 'cat2', 'cat3'], \n\u001b[0m\u001b[1;32m      2\u001b[0m                   \u001b[0membed_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                   \u001b[0mtext_embed_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item_description'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item_desc2gram'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                   \u001b[0mtext_embed_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                   \u001b[0mtext_embed_seq_lens\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EM_NNRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "nnet2 = EM_NNRegressor(embed_cols=['brand_name','category_name','item_condition_id', 'cat1', 'cat2', 'cat3'], \n",
    "                  embed_dims=[(6000, 30),(1500, 25), (5,4), (15,4), (120, 10), (900, 20)],\n",
    "                  text_embed_cols=['name', 'item_description', 'item_desc2gram', 'item_name'],\n",
    "                  text_embed_dims=[(20000, 30), (100000, 30), (10000, 30), (20000, 30)],\n",
    "                  text_embed_seq_lens =[7, 80, 20, 20],\n",
    "                  #text_embed_tokenizers = [tok_name, tok_desc, tok_desc2],\n",
    "                  dense_cols=['shipping', 'desc_words', 'desc_chars', 'name_chars',\n",
    "                                'iphone_case', 'iphone6', 'iphone6p',\n",
    "                                'iphone5', 'iphone5p', 'iphone7', 'iphone7p', 'unlocked_phone',\n",
    "                              'brand_counts', 'cat_counts',\n",
    "                                   'cat1_counts', 'cat2_counts', 'cat3_counts'],\n",
    "                  epochs=5,\n",
    "                  batchsize=2048 ,\n",
    "                  num_layers = 1,\n",
    "                  layer_dropouts=[0.1],\n",
    "                  layer_dims=[100],\n",
    "                  seed=2,\n",
    "                  val_size=0.02\n",
    "                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_211/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_212/Reshape:0' shape=(?, 25) dtype=float32>, <tf.Tensor 'reshape_213/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_214/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_215/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_216/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'global_average_pooling1d_141/Mean:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'global_average_pooling1d_142/Mean:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'global_average_pooling1d_143/Mean:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'global_average_pooling1d_144/Mean:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'dense_cols_66:0' shape=(?, 17) dtype=float32>]\n",
      "(1161619, 35) (23707, 35) (1161619,) (23707,)\n",
      "Train on 1161619 samples, validate on 23707 samples\n",
      "Epoch 1/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.5079Epoch 00001: val_loss improved from inf to 0.22992, saving model to embed_NN_2.check\n",
      "1161619/1161619 [==============================] - 26s 22us/step - loss: 0.5068 - val_loss: 0.2299\n",
      "Epoch 2/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1933Epoch 00002: val_loss improved from 0.22992 to 0.18494, saving model to embed_NN_2.check\n",
      "1161619/1161619 [==============================] - 12s 10us/step - loss: 0.1933 - val_loss: 0.1849\n",
      "Epoch 3/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1724Epoch 00003: val_loss improved from 0.18494 to 0.18494, saving model to embed_NN_2.check\n",
      "1161619/1161619 [==============================] - 12s 10us/step - loss: 0.1725 - val_loss: 0.1849\n",
      "Epoch 4/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1601Epoch 00004: val_loss improved from 0.18494 to 0.18203, saving model to embed_NN_2.check\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.1601 - val_loss: 0.1820\n",
      "Epoch 5/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1512Epoch 00005: val_loss improved from 0.18203 to 0.18160, saving model to embed_NN_2.check\n",
      "1161619/1161619 [==============================] - 12s 10us/step - loss: 0.1512 - val_loss: 0.1816\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-1b392cb55e74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moof_preds2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnnet2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcvlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moof_preds2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_predict\u001b[0;34m(estimator, X, y, groups, cv, n_jobs, verbose, fit_params, pre_dispatch, method)\u001b[0m\n\u001b[1;32m    678\u001b[0m     prediction_blocks = parallel(delayed(_fit_and_predict)(\n\u001b[1;32m    679\u001b[0m         clone(estimator), X, y, train, test, verbose, fit_params, method)\n\u001b[0;32m--> 680\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;31m# Concatenate the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_predict\u001b[0;34m(estimator, X, y, train, test, verbose, fit_params, method)\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'decision_function'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'predict_proba'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'predict_log_proba'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-876e2a9385df>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"embed_NN_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".check\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_splitX\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;31m# set weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mtopology\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;31m# Early return if compilation is not required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m   3141\u001b[0m                              ' elements.')\n\u001b[1;32m   3142\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3143\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2250\u001b[0m             \u001b[0massign_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2251\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2252\u001b[0;31m         \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "oof_preds2 = cross_val_predict(nnet2, X, y, verbose=10, cv=cvlist)\n",
    "score = rmse(y, oof_preds2)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_85/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_86/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_87/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_88/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_89/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_90/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'global_average_pooling1d_57/Mean:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'global_average_pooling1d_58/Mean:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'global_average_pooling1d_59/Mean:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'global_average_pooling1d_60/Mean:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'dense_cols_25:0' shape=(?, 17) dtype=float32>]\n",
      "(1161619, 35) (23707, 35) (1161619,) (23707,)\n",
      "Train on 1161619 samples, validate on 23707 samples\n",
      "Epoch 1/4\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.5970Epoch 00001: val_loss improved from inf to 0.22496, saving model to embed_NN_3.check\n",
      "1161619/1161619 [==============================] - 19s 17us/step - loss: 0.5955 - val_loss: 0.2250\n",
      "Epoch 2/4\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1755Epoch 00002: val_loss improved from 0.22496 to 0.17857, saving model to embed_NN_3.check\n",
      "1161619/1161619 [==============================] - 14s 12us/step - loss: 0.1755 - val_loss: 0.1786\n",
      "Epoch 3/4\n",
      "1161216/1161619 [============================>.] - ETA: 0s - loss: 0.1538Epoch 00003: val_loss did not improve\n",
      "1161619/1161619 [==============================] - 14s 12us/step - loss: 0.1538 - val_loss: 0.1814\n",
      "Epoch 4/4\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1413Epoch 00004: val_loss improved from 0.17857 to 0.17097, saving model to embed_NN_3.check\n",
      "1161619/1161619 [==============================] - 14s 12us/step - loss: 0.1413 - val_loss: 0.1710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_91/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_92/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_93/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_94/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_95/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_96/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'global_average_pooling1d_61/Mean:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'global_average_pooling1d_62/Mean:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'global_average_pooling1d_63/Mean:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'global_average_pooling1d_64/Mean:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'dense_cols_27:0' shape=(?, 17) dtype=float32>]\n",
      "(1161619, 35) (23707, 35) (1161619,) (23707,)\n",
      "Train on 1161619 samples, validate on 23707 samples\n",
      "Epoch 1/4\n",
      "1161216/1161619 [============================>.] - ETA: 0s - loss: 0.5604Epoch 00001: val_loss improved from inf to 0.27157, saving model to embed_NN_3.check\n",
      "1161619/1161619 [==============================] - 19s 17us/step - loss: 0.5603 - val_loss: 0.2716\n",
      "Epoch 2/4\n",
      "1159168/1161619 [============================>.] - ETA: 0s - loss: 0.1754Epoch 00002: val_loss improved from 0.27157 to 0.19065, saving model to embed_NN_3.check\n",
      "1161619/1161619 [==============================] - 14s 12us/step - loss: 0.1753 - val_loss: 0.1907\n",
      "Epoch 3/4\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1533Epoch 00003: val_loss improved from 0.19065 to 0.17058, saving model to embed_NN_3.check\n",
      "1161619/1161619 [==============================] - 14s 12us/step - loss: 0.1533 - val_loss: 0.1706\n",
      "Epoch 4/4\n",
      "1161216/1161619 [============================>.] - ETA: 0s - loss: 0.1407Epoch 00004: val_loss did not improve\n",
      "1161619/1161619 [==============================] - 14s 12us/step - loss: 0.1407 - val_loss: 0.1723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  3.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_97/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_98/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_99/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_100/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_101/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_102/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'global_average_pooling1d_65/Mean:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'global_average_pooling1d_66/Mean:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'global_average_pooling1d_67/Mean:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'global_average_pooling1d_68/Mean:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'dense_cols_29:0' shape=(?, 17) dtype=float32>]\n",
      "(1161619, 35) (23707, 35) (1161619,) (23707,)\n",
      "Train on 1161619 samples, validate on 23707 samples\n",
      "Epoch 1/4\n",
      "1161216/1161619 [============================>.] - ETA: 0s - loss: 0.5594Epoch 00001: val_loss improved from inf to 0.22372, saving model to embed_NN_3.check\n",
      "1161619/1161619 [==============================] - 20s 17us/step - loss: 0.5592 - val_loss: 0.2237\n",
      "Epoch 2/4\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1748Epoch 00002: val_loss improved from 0.22372 to 0.18285, saving model to embed_NN_3.check\n",
      "1161619/1161619 [==============================] - 14s 12us/step - loss: 0.1747 - val_loss: 0.1829\n",
      "Epoch 3/4\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1528Epoch 00003: val_loss improved from 0.18285 to 0.17608, saving model to embed_NN_3.check\n",
      "1161619/1161619 [==============================] - 14s 12us/step - loss: 0.1528 - val_loss: 0.1761\n",
      "Epoch 4/4\n",
      "1161216/1161619 [============================>.] - ETA: 0s - loss: 0.1403Epoch 00004: val_loss improved from 0.17608 to 0.17089, saving model to embed_NN_3.check\n",
      "1161619/1161619 [==============================] - 14s 12us/step - loss: 0.1403 - val_loss: 0.1709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  5.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_103/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_104/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_105/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_106/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_107/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_108/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'global_average_pooling1d_69/Mean:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'global_average_pooling1d_70/Mean:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'global_average_pooling1d_71/Mean:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'global_average_pooling1d_72/Mean:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'dense_cols_31:0' shape=(?, 17) dtype=float32>]\n",
      "(1161620, 35) (23707, 35) (1161620,) (23707,)\n",
      "Train on 1161620 samples, validate on 23707 samples\n",
      "Epoch 1/4\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.5585Epoch 00001: val_loss improved from inf to 0.22848, saving model to embed_NN_3.check\n",
      "1161620/1161620 [==============================] - 20s 18us/step - loss: 0.5571 - val_loss: 0.2285\n",
      "Epoch 2/4\n",
      "1159168/1161620 [============================>.] - ETA: 0s - loss: 0.1747Epoch 00002: val_loss improved from 0.22848 to 0.17922, saving model to embed_NN_3.check\n",
      "1161620/1161620 [==============================] - 14s 12us/step - loss: 0.1747 - val_loss: 0.1792\n",
      "Epoch 3/4\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.1532Epoch 00003: val_loss improved from 0.17922 to 0.17278, saving model to embed_NN_3.check\n",
      "1161620/1161620 [==============================] - 13s 12us/step - loss: 0.1532 - val_loss: 0.1728\n",
      "Epoch 4/4\n",
      "1161216/1161620 [============================>.] - ETA: 0s - loss: 0.1409Epoch 00004: val_loss did not improve\n",
      "1161620/1161620 [==============================] - 13s 12us/step - loss: 0.1409 - val_loss: 0.1740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  6.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_109/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_110/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_111/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_112/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_113/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_114/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'global_average_pooling1d_73/Mean:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'global_average_pooling1d_74/Mean:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'global_average_pooling1d_75/Mean:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'global_average_pooling1d_76/Mean:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'dense_cols_33:0' shape=(?, 17) dtype=float32>]\n",
      "(1161620, 35) (23707, 35) (1161620,) (23707,)\n",
      "Train on 1161620 samples, validate on 23707 samples\n",
      "Epoch 1/4\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.5359Epoch 00001: val_loss improved from inf to 0.24006, saving model to embed_NN_3.check\n",
      "1161620/1161620 [==============================] - 21s 18us/step - loss: 0.5346 - val_loss: 0.2401\n",
      "Epoch 2/4\n",
      "1159168/1161620 [============================>.] - ETA: 0s - loss: 0.1784Epoch 00002: val_loss improved from 0.24006 to 0.17663, saving model to embed_NN_3.check\n",
      "1161620/1161620 [==============================] - 14s 12us/step - loss: 0.1784 - val_loss: 0.1766\n",
      "Epoch 3/4\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.1564Epoch 00003: val_loss did not improve\n",
      "1161620/1161620 [==============================] - 14s 12us/step - loss: 0.1564 - val_loss: 0.1804\n",
      "Epoch 4/4\n",
      "1161216/1161620 [============================>.] - ETA: 0s - loss: 0.1440Epoch 00004: val_loss improved from 0.17663 to 0.17199, saving model to embed_NN_3.check\n",
      "1161620/1161620 [==============================] - 14s 12us/step - loss: 0.1440 - val_loss: 0.1720\n",
      "0.3729204 8.352561\n",
      "0.41859579758955956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  8.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  8.7min finished\n"
     ]
    }
   ],
   "source": [
    "nnet3 = EM_NNRegressor(embed_cols=['brand_name','category_name','item_condition_id', 'cat1', 'cat2', 'cat3'], \n",
    "                  embed_dims=[(6000, 30),(1500, 20), (5,4), (15,4), (120, 10), (900, 20)],\n",
    "                  text_embed_cols=['name', 'item_description', 'item_desc2gram', 'item_name'],\n",
    "                  text_embed_dims=[(20000, 50), (100000, 50), (10000, 50), (20000, 20)],\n",
    "                  text_embed_seq_lens =[7, 80, 20, 20],\n",
    "                  #text_embed_tokenizers = [tok_name, tok_desc, tok_desc2],\n",
    "                  dense_cols=['shipping', 'desc_words', 'desc_chars', 'name_chars',\n",
    "                                'iphone_case', 'iphone6', 'iphone6p',\n",
    "                                'iphone5', 'iphone5p', 'iphone7', 'iphone7p', 'unlocked_phone',\n",
    "                              'brand_counts', 'cat_counts',\n",
    "                                   'cat1_counts', 'cat2_counts', 'cat3_counts'],\n",
    "                  epochs=4,\n",
    "                  batchsize=2048 ,\n",
    "                  num_layers = 1,\n",
    "                  layer_dropouts=[0.2],\n",
    "                  layer_dims=[200],\n",
    "                  seed=3,\n",
    "                  val_size=0.02,\n",
    "                 )\n",
    "oof_preds3 = cross_val_predict(nnet3, X, y, verbose=10, cv=cvlist)\n",
    "score = rmse(y, oof_preds3)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import hmean, gmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1481658,)\n",
      "0.8430702 8.256827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4071790911499989"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_preds = np.mean(np.hstack((oof_preds1, oof_preds2, oof_preds3)), axis=1)\n",
    "print(oof_preds.shape)\n",
    "rmse(y, oof_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f25b3789e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.distplot(oof_preds)\n",
    "sns.distplot(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1481658, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   10.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   21.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   32.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:   43.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.988219376342803 4.961022181829234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   54.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   54.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4244249992307715"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfr = RandomForestRegressor(n_estimators=50, max_depth=3, n_jobs=-1, min_samples_leaf=50)\n",
    "oof_X = np.hstack((oof_preds1, oof_preds2, oof_preds3))\n",
    "print(oof_X.shape)\n",
    "oof2 = cross_val_predict(rfr, oof_X, y , cv=5, verbose=10)\n",
    "rmse(y, oof2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding GRU - worsened performance\n",
    "#Flatten - worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_269/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_270/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_271/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_272/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_273/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_274/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_275/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_276/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_277/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_42:0' shape=(?, 21) dtype=float32>]\n",
      "(1444616, 32) (37042, 32) (1444616,) (37042,)\n",
      "Train on 1444616 samples, validate on 37042 samples\n",
      "Epoch 1/5\n",
      "1443840/1444616 [============================>.] - ETA: 0s - loss: 0.4759Epoch 00001: val_loss improved from inf to 0.21140, saving model to embed_NN_1.check\n",
      "1444616/1444616 [==============================] - 27s 19us/step - loss: 0.4757 - val_loss: 0.2114\n",
      "Epoch 2/5\n",
      "1441792/1444616 [============================>.] - ETA: 0s - loss: 0.1769Epoch 00002: val_loss improved from 0.21140 to 0.17565, saving model to embed_NN_1.check\n",
      "1444616/1444616 [==============================] - 18s 12us/step - loss: 0.1769 - val_loss: 0.1757\n",
      "Epoch 3/5\n",
      "1441792/1444616 [============================>.] - ETA: 0s - loss: 0.1592Epoch 00003: val_loss improved from 0.17565 to 0.16952, saving model to embed_NN_1.check\n",
      "1444616/1444616 [==============================] - 18s 12us/step - loss: 0.1592 - val_loss: 0.1695\n",
      "Epoch 4/5\n",
      "1441792/1444616 [============================>.] - ETA: 0s - loss: 0.1497Epoch 00004: val_loss improved from 0.16952 to 0.16942, saving model to embed_NN_1.check\n",
      "1444616/1444616 [==============================] - 18s 12us/step - loss: 0.1496 - val_loss: 0.1694\n",
      "Epoch 5/5\n",
      "1443840/1444616 [============================>.] - ETA: 0s - loss: 0.1433Epoch 00005: val_loss improved from 0.16942 to 0.16762, saving model to embed_NN_1.check\n",
      "1444616/1444616 [==============================] - 18s 12us/step - loss: 0.1433 - val_loss: 0.1676\n",
      "Predicting on test data\n"
     ]
    }
   ],
   "source": [
    "nnet1.fit(train_data, np.log1p(train_data.price) )\n",
    "print(\"Predicting on test data\")\n",
    "#test_preds1 = nnet1.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_55/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_56/Reshape:0' shape=(?, 25) dtype=float32>, <tf.Tensor 'reshape_57/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_58/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_59/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_60/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_61/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_62/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_63/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'dense_cols_11:0' shape=(?, 17) dtype=float32>]\n",
      "(1452024, 32) (29634, 32) (1452024,) (29634,)\n",
      "Train on 1452024 samples, validate on 29634 samples\n",
      "Epoch 1/6\n",
      "1445888/1452024 [============================>.] - ETA: 0s - loss: 0.4870Epoch 00001: val_loss improved from inf to 0.21299, saving model to embed_NN_2.check\n",
      "1452024/1452024 [==============================] - 18s 12us/step - loss: 0.4859 - val_loss: 0.2130\n",
      "Epoch 2/6\n",
      "1445888/1452024 [============================>.] - ETA: 0s - loss: 0.2105Epoch 00002: val_loss improved from 0.21299 to 0.18757, saving model to embed_NN_2.check\n",
      "1452024/1452024 [==============================] - 15s 10us/step - loss: 0.2104 - val_loss: 0.1876\n",
      "Epoch 3/6\n",
      "1445888/1452024 [============================>.] - ETA: 0s - loss: 0.1884Epoch 00003: val_loss improved from 0.18757 to 0.18330, saving model to embed_NN_2.check\n",
      "1452024/1452024 [==============================] - 15s 10us/step - loss: 0.1884 - val_loss: 0.1833\n",
      "Epoch 4/6\n",
      "1449984/1452024 [============================>.] - ETA: 0s - loss: 0.1747Epoch 00004: val_loss improved from 0.18330 to 0.18075, saving model to embed_NN_2.check\n",
      "1452024/1452024 [==============================] - 15s 10us/step - loss: 0.1747 - val_loss: 0.1807\n",
      "Epoch 5/6\n",
      "1445888/1452024 [============================>.] - ETA: 0s - loss: 0.1650Epoch 00005: val_loss improved from 0.18075 to 0.17968, saving model to embed_NN_2.check\n",
      "1452024/1452024 [==============================] - 15s 10us/step - loss: 0.1650 - val_loss: 0.1797\n",
      "Epoch 6/6\n",
      "1445888/1452024 [============================>.] - ETA: 0s - loss: 0.1574Epoch 00006: val_loss improved from 0.17968 to 0.17811, saving model to embed_NN_2.check\n",
      "1452024/1452024 [==============================] - 15s 10us/step - loss: 0.1574 - val_loss: 0.1781\n",
      "Predicting on test data\n"
     ]
    }
   ],
   "source": [
    "nnet2.fit(train_data, np.log1p(train_data.price) )\n",
    "print(\"Predicting on test data\")\n",
    "test_preds2 = nnet2.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_136/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_137/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_138/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_139/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_140/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_141/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_142/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_143/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_144/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_30:0' shape=(?, 21) dtype=float32>]\n",
      "(1444616, 32) (37042, 32) (1444616,) (37042,)\n",
      "Train on 1444616 samples, validate on 37042 samples\n",
      "Epoch 1/5\n",
      "1443840/1444616 [============================>.] - ETA: 0s - loss: 0.5105Epoch 00001: val_loss improved from inf to 0.20131, saving model to embed_NN_1.check\n",
      "1444616/1444616 [==============================] - 24s 17us/step - loss: 0.5103 - val_loss: 0.2013\n",
      "Epoch 2/5\n",
      "1439744/1444616 [============================>.] - ETA: 0s - loss: 0.1957Epoch 00002: val_loss improved from 0.20131 to 0.17799, saving model to embed_NN_1.check\n",
      "1444616/1444616 [==============================] - 18s 12us/step - loss: 0.1956 - val_loss: 0.1780\n",
      "Epoch 3/5\n",
      "1439744/1444616 [============================>.] - ETA: 0s - loss: 0.1730Epoch 00003: val_loss improved from 0.17799 to 0.17397, saving model to embed_NN_1.check\n",
      "1444616/1444616 [==============================] - 18s 12us/step - loss: 0.1730 - val_loss: 0.1740\n",
      "Epoch 4/5\n",
      "1443840/1444616 [============================>.] - ETA: 0s - loss: 0.1602Epoch 00004: val_loss improved from 0.17397 to 0.17274, saving model to embed_NN_1.check\n",
      "1444616/1444616 [==============================] - 18s 12us/step - loss: 0.1602 - val_loss: 0.1727\n",
      "Epoch 5/5\n",
      "1443840/1444616 [============================>.] - ETA: 0s - loss: 0.1503Epoch 00005: val_loss improved from 0.17274 to 0.17062, saving model to embed_NN_1.check\n",
      "1444616/1444616 [==============================] - 18s 12us/step - loss: 0.1503 - val_loss: 0.1706\n",
      "Predicting on test data\n",
      "[<tf.Tensor 'reshape_145/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_146/Reshape:0' shape=(?, 25) dtype=float32>, <tf.Tensor 'reshape_147/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_148/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_149/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_150/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_151/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_152/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_153/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'dense_cols_32:0' shape=(?, 17) dtype=float32>]\n",
      "(1452024, 32) (29634, 32) (1452024,) (29634,)\n",
      "Train on 1452024 samples, validate on 29634 samples\n",
      "Epoch 1/4\n",
      "1445888/1452024 [============================>.] - ETA: 0s - loss: 0.5018Epoch 00001: val_loss improved from inf to 0.20689, saving model to embed_NN_2.check\n",
      "1452024/1452024 [==============================] - 21s 15us/step - loss: 0.5006 - val_loss: 0.2069\n",
      "Epoch 2/4\n",
      "1445888/1452024 [============================>.] - ETA: 0s - loss: 0.1954Epoch 00002: val_loss improved from 0.20689 to 0.18119, saving model to embed_NN_2.check\n",
      "1452024/1452024 [==============================] - 15s 10us/step - loss: 0.1954 - val_loss: 0.1812\n",
      "Epoch 3/4\n",
      "1445888/1452024 [============================>.] - ETA: 0s - loss: 0.1744Epoch 00003: val_loss improved from 0.18119 to 0.17511, saving model to embed_NN_2.check\n",
      "1452024/1452024 [==============================] - 14s 10us/step - loss: 0.1743 - val_loss: 0.1751\n",
      "Epoch 4/4\n",
      "1445888/1452024 [============================>.] - ETA: 0s - loss: 0.1621Epoch 00004: val_loss improved from 0.17511 to 0.17479, saving model to embed_NN_2.check\n",
      "1452024/1452024 [==============================] - 14s 10us/step - loss: 0.1621 - val_loss: 0.1748\n",
      "Predicting on test data\n",
      "[<tf.Tensor 'reshape_154/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_155/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_156/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_157/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_158/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_159/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_160/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_161/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_162/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_34:0' shape=(?, 17) dtype=float32>]\n",
      "(1452024, 32) (29634, 32) (1452024,) (29634,)\n",
      "Train on 1452024 samples, validate on 29634 samples\n",
      "Epoch 1/4\n",
      "1445888/1452024 [============================>.] - ETA: 0s - loss: 0.5142Epoch 00001: val_loss improved from inf to 0.21044, saving model to embed_NN_3.check\n",
      "1452024/1452024 [==============================] - 24s 17us/step - loss: 0.5130 - val_loss: 0.2104\n",
      "Epoch 2/4\n",
      "1447936/1452024 [============================>.] - ETA: 0s - loss: 0.2053Epoch 00002: val_loss improved from 0.21044 to 0.18658, saving model to embed_NN_3.check\n",
      "1452024/1452024 [==============================] - 17s 12us/step - loss: 0.2053 - val_loss: 0.1866\n",
      "Epoch 3/4\n",
      "1449984/1452024 [============================>.] - ETA: 0s - loss: 0.1824Epoch 00003: val_loss improved from 0.18658 to 0.17833, saving model to embed_NN_3.check\n",
      "1452024/1452024 [==============================] - 17s 12us/step - loss: 0.1824 - val_loss: 0.1783\n",
      "Epoch 4/4\n",
      "1445888/1452024 [============================>.] - ETA: 0s - loss: 0.1704Epoch 00004: val_loss improved from 0.17833 to 0.17602, saving model to embed_NN_3.check\n",
      "1452024/1452024 [==============================] - 17s 12us/step - loss: 0.1704 - val_loss: 0.1760\n",
      "Predicting on test data\n",
      "Write out submission\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohsin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/home/mohsin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:3643: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "nnet3.fit(train_data, np.log1p(train_data.price) )\n",
    "print(\"Predicting on test data\")\n",
    "test_preds3 = nnet3.predict(test_data)\n",
    "\n",
    "test_preds = (1/3)*(test_preds1 + test_preds2 + test_preds3)\n",
    "print(\"Write out submission\")\n",
    "submission: pd.DataFrame = test_data[['test_id']]\n",
    "submission['price'] = np.expm1(test_preds)\n",
    "submission.price = submission.price.clip(3, 2000)\n",
    "submission.to_csv(\"embedding_nn_v2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
