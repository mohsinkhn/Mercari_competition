{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohsin/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import threading\n",
    "import multiprocessing\n",
    "import unicodedata\n",
    "import string\n",
    "import math\n",
    "\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder, RobustScaler, MaxAbsScaler, QuantileTransformer\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(tqdm)\n",
    "num_partitions = 8\n",
    "num_cores = 4\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import wordbatch\n",
    "from  wordbatch.extractors import WordSeq\n",
    "# from __future__ import print_function\n",
    "np.random.seed(786)  # for reproducibility\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D, ZeroPadding1D, AveragePooling1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier,  KerasRegressor\n",
    "#Some classes\n",
    "#Functions we need - Feature Selector, Fasttext_Estimator, Preprocessing Transformer, Binary_Encoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "from pandas.api.types import is_numeric_dtype, is_string_dtype\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "rmse_sklearn = make_scorer(rmse, greater_is_better=False)\n",
    "\n",
    "def get_obj_cols(df):\n",
    "    \"\"\"Return columns with object dtypes\"\"\"\n",
    "    obj_cols = []\n",
    "    for idx, dt in enumerate(df.dtypes):\n",
    "        if dt == 'object':\n",
    "            obj_cols.append(df.columns.values[idx])\n",
    "\n",
    "    return obj_cols\n",
    "\n",
    "\n",
    "def convert_input(X):\n",
    "    \"\"\"if input not a dataframe convert it to one\"\"\"\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        if isinstance(X, list):\n",
    "            X = pd.DataFrame(np.array(X))\n",
    "        elif isinstance(X, (np.generic, np.ndarray)):\n",
    "            X = pd.DataFrame(X)\n",
    "        elif isinstance(X, csr_matrix):\n",
    "            X = pd.SparseDataFrame(X)\n",
    "        else:\n",
    "            raise ValueError('Unexpected input type: %s' % (str(type(X))))\n",
    "\n",
    "        #X = X.apply(lambda x: pd.to_numeric(x, errors='ignore'))\n",
    "    return X\n",
    "\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Class to do subset of features in sklearn pipeline\"\"\"\n",
    "    def __init__(self, cols=None, return_df=True, verbose=0):\n",
    "        self.cols = cols\n",
    "        self.return_df = return_df\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        #Do nothing\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        #if the input dataset isn't already a dataframe, convert it to one\n",
    "        X = X.copy(deep=True)\n",
    "        X = convert_input(X)\n",
    "        X = X.loc[:, self.col]\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Selecting columns are {}\".format(self.col))\n",
    "        if self.return_df:\n",
    "            return X\n",
    "        else:\n",
    "            return X.values\n",
    "    \n",
    "class BinaryEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Binary encoding for categorical variables, similar to onehot, \n",
    "    but stores categories as binary bitstrings.\n",
    "    Expects cols to numerical, else throws error\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=0, cols=None, add_to_df=True, return_df=True):\n",
    "        self.return_df = return_df\n",
    "        self.verbose = verbose\n",
    "        self.cols = cols\n",
    "        self.add_to_df = add_to_df\n",
    "        self._dim = None\n",
    "        self.digits_per_col = {}\n",
    "\n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "        # if the input dataset isn't already a dataframe, convert it to one (using default column names)\n",
    "        # first check the type\n",
    "        X = convert_input(X)\n",
    "\n",
    "        self._dim = X.shape[1]\n",
    "\n",
    "        # if columns aren't passed, just use every string column\n",
    "        if self.cols is None:\n",
    "            self.cols = get_obj_cols(X)\n",
    "        #Check if all cols are numeric and no nan's else throws error\n",
    "        if np.any([is_string_dtype(X[col]) for col in self.cols]):\n",
    "            raise ValueError(\"Input contains non-numeric data or is has nan's\")\n",
    "\n",
    "        for col in self.cols:\n",
    "            self.digits_per_col[col] = self.calc_required_digits(X, col)\n",
    "            \n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Perform the transformation to new categorical data. \"\"\"\n",
    "        if self._dim is None:\n",
    "            raise ValueError('Must train encoder before it can be used to transform data.')\n",
    "        # first check the type\n",
    "        X = convert_input(X)\n",
    "        # then make sure that it is the right size\n",
    "        if X.shape[1] != self._dim:\n",
    "            raise ValueError('Unexpected input dimension %d, expected %d' % (X.shape[1], self._dim, ))\n",
    "\n",
    "        #Check if all cols are numeric and no nan's else throws error\n",
    "        if np.any([is_string_dtype(X[col]) for col in self.cols]):\n",
    "            raise ValueError(\"Input contains non-numeric data \")\n",
    "        \n",
    "        X = self.binary(X, cols=self.cols)\n",
    "        print(X.shape)\n",
    "        if self.return_df:\n",
    "            return X\n",
    "        else:\n",
    "            return X.values\n",
    "\n",
    "\n",
    "    def binary(self, X_in, cols=None):\n",
    "        \"\"\"\n",
    "        Binary encoding encodes the integers as binary code with one column per digit.\n",
    "        \"\"\"\n",
    "        X = X_in.copy(deep=True)\n",
    "\n",
    "        if cols is None:\n",
    "            cols = X.columns.values\n",
    "            pass_thru = []\n",
    "        else:\n",
    "            pass_thru = [col for col in X.columns.values if col not in cols]\n",
    "\n",
    "        bin_cols = []\n",
    "        for col in cols:\n",
    "            # get how many digits we need to represent the classes present\n",
    "            digits = self.digits_per_col[col]\n",
    "\n",
    "            # map the ordinal column into a list of these digits, of length digits\n",
    "            X[col] = X[col].map(lambda x: self.col_transform(x, digits))\n",
    "\n",
    "            for dig in range(digits):\n",
    "                X[str(col) + '_%d' % (dig, )] = X[col].map(lambda r: \n",
    "                                                int(r[dig]) if r is not None else None)\n",
    "                bin_cols.append(str(col) + '_%d' % (dig, ))\n",
    "\n",
    "        if self.add_to_df:\n",
    "            X = X.reindex(columns=bin_cols + pass_thru)\n",
    "        else:\n",
    "            X =  X.reindex(columns=bin_cols)\n",
    "        return X\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def calc_required_digits(X, col):\n",
    "        \"\"\"\n",
    "        figure out how many digits we need to represent the classes present\n",
    "        \"\"\"\n",
    "        return int( np.ceil(np.log2(X[col].nunique())) )\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def col_transform(col, digits):\n",
    "        \"\"\"\n",
    "        The lambda body to transform the column values\n",
    "        \"\"\"\n",
    "        if col is None or float(col) < 0.0:\n",
    "            return None\n",
    "        else:\n",
    "            col = format(col, \"0\"+str(digits)+'b')\n",
    "        return col\n",
    "    \n",
    "    \n",
    "    \n",
    "# the following functions allow for a parallelized batch generator\n",
    "class threadsafe_iter(object):\n",
    "    \"\"\"\n",
    "    Takes an iterator/generator and makes it thread-safe by\n",
    "    serializing call to the `next` method of given iterator/generator.\n",
    "    \"\"\"\n",
    "    def __init__(self, it):\n",
    "        self.it = it\n",
    "        self.lock = threading.Lock()\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        with self.lock:\n",
    "            return next(self.it)\n",
    "\n",
    "def threadsafe_generator(f):\n",
    "    \"\"\"\n",
    "    A decorator that takes a generator function and makes it thread-safe.\n",
    "    \"\"\"\n",
    "    def g(*a, **kw):\n",
    "        return threadsafe_iter(f(*a, **kw))\n",
    "    return g\n",
    "\n",
    "@threadsafe_generator\n",
    "def batch_generator(X_data, y_data, batch_size):\n",
    "    \n",
    "    #index = np.random.permutation(X_data.shape[0])    \n",
    "    #X_data = X_data[index]\n",
    "    #y_data = y_data[index]\n",
    "    \n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    index = np.arange(np.shape(y_data)[0])\n",
    "    #idx = 1\n",
    "    while 1:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:].todense()\n",
    "        y_batch = y_data[index_batch]\n",
    "        counter += 1\n",
    "        yield np.array(X_batch),y_batch\n",
    "        #print(\"\")\n",
    "        #print(X_batch.shape)\n",
    "        #print(\"\")\n",
    "        #print('generator yielded a batch %d' % idx)\n",
    "        #idx += 1\n",
    "        if (counter > number_of_batches):\n",
    "            counter=0\n",
    "            \n",
    "            \n",
    "@threadsafe_generator\n",
    "def batch_generator_x(X_data,batch_size):\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    index = np.arange(np.shape(X_data)[0])\n",
    "    while 1:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:].todense()\n",
    "        counter += 1\n",
    "        yield np.array(X_batch)\n",
    "        if (counter > number_of_batches):\n",
    "            counter=0\n",
    "            \n",
    "\n",
    "class TargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols=None, thresh=0, func=np.mean, add_to_orig=False):\n",
    "        self.cols = cols\n",
    "        self.thresh = thresh\n",
    "        self.func = func\n",
    "        self.add_to_orig = add_to_orig\n",
    "    \n",
    "    #@numba.jit        \n",
    "    def fit(self, X, y):\n",
    "        self.prior = self.func(y)\n",
    "        self._dict = {}\n",
    "        for col in self.cols:\n",
    "            if isinstance(col, (list, tuple)):\n",
    "                print('here')\n",
    "                tmp_df = X.loc[: ,col]\n",
    "                col = tuple(col)\n",
    "            else:\n",
    "                tmp_df = X.loc[: ,[col]]\n",
    "            tmp_df['y'] = y\n",
    "            print(tmp_df.columns)\n",
    "            #tmp_df = pd.DataFrame({'eval_col':X[col].values, 'y':y})\n",
    "            if isinstance(col, (list, tuple)):\n",
    "                print('here')\n",
    "                col = tuple(col)\n",
    "            self._dict[col] = tmp_df.groupby(col)['y'].apply(lambda x: \n",
    "                                self.func(x) if len(x) >= self.thresh  else self.prior).to_dict()\n",
    "                                \n",
    "            del tmp_df\n",
    "        return self\n",
    "    #@numba.jit\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for col in self.cols:\n",
    "            \n",
    "            if isinstance(col, (list, tuple)):\n",
    "                tmp_df = X.loc[:, col]\n",
    "                enc = tmp_df[col].apply(lambda x: self._dict[tuple(col)][tuple(x)]\n",
    "                                                                     if tuple(x) in self._dict[tuple(col)]\n",
    "                                                                     else self.prior, axis=1).values\n",
    "            else:\n",
    "                tmp_df = X.loc[:, [col]]\n",
    "                enc = tmp_df[col].apply(lambda x: self._dict[col][x]\n",
    "                                                                     if x in self._dict[col]\n",
    "                                                                     else self.prior).values\n",
    "            del tmp_df\n",
    "            X_transformed.append(enc)\n",
    "        \n",
    "        X_transformed = np.vstack(X_transformed).T\n",
    "        \n",
    "        if self.add_to_orig:\n",
    "            return np.concatenate((X.values, X_transformed), axis=1)\n",
    "            \n",
    "        else:\n",
    "            return X_transformed            \n",
    "stop_words = ['a', 'an', 'this', 'is', 'the', 'of', 'for']\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return  unicodedata.normalize('NFKC', s)\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"'\", r\"\", s)\n",
    "    #s = re.sub(r\"[.!?':;,]\", r\" \", s)\n",
    "    s = re.sub(r\"-\", r\"\", s)\n",
    "    s = re.sub(r\"[^0-9a-zA-Z]+\", r\" \", s)\n",
    "    s = re.sub(r\"lily jade\", r\"lilyjade\", s)\n",
    "    s = re.sub(r\"rae dunn cookie(s){0,1}\", r\"raedunncookie\", s)\n",
    "    s = re.sub(r\"hatchimals\", r\"hatchimal\", s)\n",
    "    s = re.sub(r\"virtual reality\", r\"vr\", s)\n",
    "    s = re.sub(r\" vs \", r\" victorias secret \", s)\n",
    "    s = re.sub(r\"google home\", r\"googlehome\", s)\n",
    "    s = re.sub(r\"16 gb\", r\"16gb \", s)\n",
    "    s = re.sub(r\"256 gb\", r\"256gb \", s)\n",
    "    s = re.sub(r\"32 gb\", r\"32gb \", s)\n",
    "    s = re.sub(r\"(?=\\w{1,2})iphone \", r\"iphone\", s)\n",
    "    s = re.sub(r\"(?=\\w{1,2})galaxy \", r\"galaxy\", s)\n",
    "    s = re.sub(\"14(k){0,1} gold\", '14kgold', s)\n",
    "    s = re.sub(\"lululemon bags\", 'lululemonbags', s)\n",
    "    s = re.sub(\"controller skin\", 'controllerskin', s)\n",
    "    s = re.sub(\"watch box\", 'watchbox', s)\n",
    "    s = re.sub(\"blaze band\", 'blazeband', s)\n",
    "    s = re.sub(\"vault boy\", 'vaultboy', s)\n",
    "    s = re.sub(\"lash boost\", 'lashboost', s)\n",
    "    s = re.sub(\"64 g \", '64gb ', s)\n",
    "    s = re.sub(\"32 g \", '32gb ', s)\n",
    "    s = re.sub(\"go(\\s){0,1}pro hero\", 'goprohero', s)\n",
    "    s = re.sub(\"nmd(s){0,1}(\\s){0,1}(r){0,1}(1){0,1}(\\s|$)\", 'nmdr ', s)\n",
    "    s = re.sub(\"private sale\", 'privatesale', s)\n",
    "    s = re.sub(\"vutton\", 'vuitton', s)\n",
    "    s = re.sub(\"louis vuitton eva\", 'louisvuittoneva', s)\n",
    "    s = re.sub(\"apple watch\", 'applewatch', s)\n",
    "    \n",
    "     \n",
    "    #s = re.sub(r\" 1 \", r\" one \", s)\n",
    "    #s = re.sub(r\" 2 \", r\" two \", s)\n",
    "    #s = re.sub(r\" 3 \", r\" three \", s)\n",
    "    #s = re.sub(r\" 4 \", r\" four \", s)\n",
    "    #s = re.sub(r\" 5 \", r\" five \", s)\n",
    "    #s = re.sub(r\" 6 \", r\" six \", s)\n",
    "    #s = re.sub(r\"7\", r\"seven\", s)\n",
    "    #s = re.sub(r\"8\", r\"eight\", s)\n",
    "    #s = re.sub(r\"/s/s\", r\"/s\", s)\n",
    "    return s\n",
    "\n",
    "def _normalize_and_ngrams(sent, ngram):\n",
    "    input_list = normalizeString(sent).split()\n",
    "    input_list = [word for word in input_list if word not in stop_words]\n",
    "    #s = input_list.copy()\n",
    "    #for i in range(2, ngrams+1):\n",
    "    #    s += [' '.join(input_list[j:j+i]) for j in range(len(input_list)-i + 1)]\n",
    "        #s += list((zip(*[input_list[j:] for j in range(i)])))\n",
    "    s = [''.join(input_list[i:i+ngram]) for i in range(len(input_list))]\n",
    "    return ' '.join(s[:-1])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "def get_cat_1(x): return str(x).split('/')[0]\n",
    "def get_cat_2(x): return str(x).split('/')[1] if len(str(x).split('/')) > 1 else -1\n",
    "def get_cat_3(x): return ' '.join(str(x).split('/')[2:]) if len(str(x).split('/')) > 2 else -1\n",
    "\n",
    "def applycat1(df): \n",
    "    return df['category_name'].progress_apply(get_cat_1)\n",
    "    \n",
    "\n",
    "def applycat2(df): \n",
    "    return df['category_name'].progress_apply(get_cat_2)\n",
    "    \n",
    "\n",
    "def applycat3(df): \n",
    "    return df['category_name'].progress_apply(get_cat_3)\n",
    "\n",
    "def get_words(series): return series.progress_apply(lambda x: len(str(x).split()))\n",
    "\n",
    "def get_chars(series): return series.progress_apply(lambda x: len(str(x)))\n",
    "\n",
    "def get_tokens(series): return np.sum(np.array(series.tolist()) > 0, axis=1)\n",
    "\n",
    "def isphonecase(series): return series.str.contains(' case ', flags=re.IGNORECASE).astype(int)\n",
    "\n",
    "def isiphone6(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('6|six', flags=re.IGNORECASE) &\n",
    "                        ~(series.str.contains('plus|\\+', flags=re.IGNORECASE)) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) ).astype(int)\n",
    "\n",
    "def isiphone6p(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('6|six', flags=re.IGNORECASE) &\n",
    "                        series.str.contains('plus|\\+', flags=re.IGNORECASE) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) ).astype(int)\n",
    "\n",
    "def isiphone5(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('5|five', flags=re.IGNORECASE) &\n",
    "                        ~(series.str.contains('plus|\\+', flags=re.IGNORECASE)) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) ).astype(int)\n",
    "\n",
    "def isiphone5p(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('5|five', flags=re.IGNORECASE) &\n",
    "                        series.str.contains('plus|\\+', flags=re.IGNORECASE) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) ).astype(int)\n",
    "\n",
    "def isiphone7(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('7|seven', flags=re.IGNORECASE) &\n",
    "                        ~(series.str.contains('plus|\\+', flags=re.IGNORECASE)) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) ).astype(int)\n",
    "\n",
    "def isiphone7p(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('7|seven', flags=re.IGNORECASE) &\n",
    "                        series.str.contains('plus|\\+', flags=re.IGNORECASE) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) ).astype(int)\n",
    "\n",
    "def isunlocked(series): return series.str.contains('unlocked', flags=re.IGNORECASE).astype(int)\n",
    "\n",
    "def plussigns(series): return series.apply(lambda x: sum([(s == '+') | (s == 'âž•') for s in str(x)]))\n",
    "\n",
    "def andsigns(series): return series.apply(lambda x: sum([(s == '&') | (s == ' and ') for s in str(x)]))\n",
    "\n",
    "def commas(series): return series.apply(lambda x: sum([s == ',' for s in str(x)]))\n",
    "\n",
    "def add_ngrams(text, ngram=2):\n",
    "    word_list = normalizeString(text).split(' ')\n",
    "    out_list = [''.join(word_list[i:i+ngram]) for i in range(len(word_list))]\n",
    "    return ' '.join(out_list[:-1])\n",
    "\n",
    "def get_2grams(series): return series.apply(lambda x: _normalize_and_ngrams(str(x), 2))\n",
    "\n",
    "def norm3grams(s): return _normalize_and_ngrams(s, 3)\n",
    "\n",
    "def applyname(series): return series.progress_apply(norm3grams)\n",
    "\n",
    "def index2sent1(x, name_vocab): return indexesFromSentence(name_vocab, x, 3, 10)\n",
    "\n",
    "def name2index(series): return series.progress_apply(lambda x: index2sent1(x, name_vocab))\n",
    "\n",
    "def norm2grams(s): return _normalize_and_ngrams(s, 1)\n",
    "\n",
    "def applydesc(series):return series.progress_apply(norm2grams)\n",
    "\n",
    "def index2sent2(x, desc_vocab): return indexesFromSentence(desc_vocab, x, 1, 80)\n",
    "\n",
    "def desc2index(series): return series.progress_apply(lambda x: index2sent2(x, desc_vocab))\n",
    "\n",
    "def indexesFromSentence(vocab, tokens, ngrams, max_len):\n",
    "    num_list = []\n",
    "    for i, item in enumerate(tokens):\n",
    "        if len(num_list) == max_len:\n",
    "            break\n",
    "        elif item in vocab.word2index:\n",
    "            num_list.append(vocab.word2index[item])\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "    if len(num_list) < max_len :\n",
    "        num_list += [0]*(max_len - len(num_list) )\n",
    "        \n",
    "    return num_list\n",
    "\n",
    "def parallelize_dataframe(df, func):\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_data(in_path, out_path):\n",
    "    if os.path.exists(os.path.join(out_path, 'train_2.pkl')) and os.path.exists(os.path.join(out_path, 'test_2.pkl')):\n",
    "        train_data = pd.read_pickle(os.path.join(out_path, 'train_2.pkl'))\n",
    "        test_data  = pd.read_pickle(os.path.join(out_path, 'test_2.pkl'))\n",
    "        \n",
    "        return train_data, test_data\n",
    "    \n",
    "    else:\n",
    "        train_data = pd.read_table(os.path.join(in_path, 'train.tsv'))\n",
    "        test_data  = pd.read_table(os.path.join(in_path, 'test.tsv'))\n",
    "    \n",
    "        train_rows = len(train_data)\n",
    "        data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "        \n",
    "        data[\"item_description\"] = data[\"item_description\"].replace(\"No description yet\", \"missing\")\n",
    "        \n",
    "        data['cat1'] = parallelize_dataframe(data[['category_name']], applycat1)\n",
    "        data['cat2'] = parallelize_dataframe(data[['category_name']], applycat2)\n",
    "        data['cat3'] = parallelize_dataframe(data[['category_name']], applycat3)\n",
    "        data.fillna(-1, inplace=True)\n",
    "        \n",
    "        print(\"Getting word/char len features\")\n",
    "        data['desc_words'] = parallelize_dataframe(data['item_description'], get_words)\n",
    "        data['desc_chars'] = parallelize_dataframe(data['item_description'], get_chars)\n",
    "        data['name_words'] = parallelize_dataframe(data['name'], get_words)\n",
    "        data['name_chars'] = parallelize_dataframe(data['name'], get_chars)\n",
    "        \n",
    "        \n",
    "        print(\"Get iphone features\")\n",
    "        data['iphone_case'] = parallelize_dataframe(data['name'], isphonecase)\n",
    "        data['iphone6'] = parallelize_dataframe(data['name'], isiphone6)\n",
    "        data['iphone6p'] = parallelize_dataframe(data['name'], isiphone6p)\n",
    "        data['iphone5'] = parallelize_dataframe(data['name'], isiphone5)\n",
    "        data['iphone5p'] = parallelize_dataframe(data['name'], isiphone5p)\n",
    "        data['iphone7'] = parallelize_dataframe(data['name'], isiphone7)\n",
    "        data['iphone7p'] = parallelize_dataframe(data['name'], isiphone7p)\n",
    "        data['unlocked_phone'] = parallelize_dataframe(data['name'], isunlocked)\n",
    "        \n",
    "        print(\"Get brand words\")\n",
    "        wb_brands = wordbatch.WordBatch(normalizeString, n_words=4500)\n",
    "        wb_brands.fit(data[\"brand_name\"].fillna(\"missing\").astype(str))\n",
    "        \n",
    "        print(\"Label encoding features\")\n",
    "        cat_cols = ['category_name', 'brand_name', 'cat1', 'cat2', 'cat3', 'item_condition_id']\n",
    "        for col in cat_cols:\n",
    "            data[col] = LabelEncoder().fit_transform(data[col].astype(str)) + 1\n",
    "            \n",
    "        print(\"Get count features\")\n",
    "        data['brand_counts'] = data.brand_name.map(data[\"brand_name\"].value_counts()).fillna(0).astype(int)\n",
    "\n",
    "        data['cat_counts'] = data.brand_name.map(data[\"category_name\"].value_counts()).fillna(0).astype(int)\n",
    "        \n",
    "        data['cat1_counts'] = data.brand_name.map(data[\"cat1\"].value_counts()).fillna(0).astype(int)\n",
    "\n",
    "        data['cat2_counts'] = data.brand_name.map(data[\"cat2\"].value_counts()).fillna(0).astype(int)\n",
    "\n",
    "        data['cat3_counts'] = data.brand_name.map(data[\"cat3\"].value_counts()).fillna(0).astype(int)\n",
    "  \n",
    "        \n",
    "        print(\"Getting punct related features\")\n",
    "        data[\"plus_counts\"] = parallelize_dataframe(data[\"item_description\"], plussigns)\n",
    "        data[\"ands_counts\"] = parallelize_dataframe(data[\"item_description\"], andsigns)\n",
    "        data[\"comma_counts\"] = parallelize_dataframe(data[\"item_description\"], commas)\n",
    "        data[\"all_counts\"] = data[\"plus_counts\"] + data[\"ands_counts\"] + data[\"comma_counts\"]\n",
    "        \n",
    "        #for col in [\"name\", \"item_description\"]:\n",
    "        #    data[col] = data[col].str.replace(\"'\", '').replace('-', '').progress_apply(unicodeToAscii)\n",
    "        #    data[col] = data[col].progress_apply(remove_puncts)\n",
    "        \n",
    "        \n",
    "        num_cols =  [\"desc_words\", \"desc_chars\", \"name_words\", \"name_chars\", \"plus_counts\", \n",
    "                    \"ands_counts\", \"comma_counts\", \"all_counts\", \"brand_counts\", \"cat1_counts\", \n",
    "                   \"cat2_counts\", \"cat3_counts\"]\n",
    "        data[num_cols]  = MaxAbsScaler().fit_transform(data[num_cols])\n",
    "            \n",
    "        data[\"brand_cat\"] = data[\"brand_name\"].astype(str) + ' ' + data[\"category_name\"].astype(str)\n",
    "        data[\"category_shipping\"] = data[\"category_name\"].astype(str) + ' ' + data[\"shipping\"].astype(str)\n",
    "        \n",
    "        print(\"transform brand cat and category_shipping\")\n",
    "        data[\"brand_cat\"] = LabelEncoder().fit_transform(data[\"brand_cat\"])\n",
    "        data[\"category_shipping\"] = LabelEncoder().fit_transform(data[\"category_shipping\"])\n",
    "        data['item_desc2gram'] = parallelize_dataframe(data[\"item_description\"], get_2grams)\n",
    "        \n",
    "        print(\"Name to sequences\")\n",
    "        wb_name = wordbatch.WordBatch(normalizeString, n_words=20000)\n",
    "        wb_name.fit(data[\"name\"])\n",
    "        \n",
    "        seq_name = WordSeq(wb_name, {\"seq_maxlen\": 7,  \"seq_truncstart\":False, \"remove_oovs\":True})\n",
    "        seq_name_desc = WordSeq(wb_name, {\"seq_maxlen\": 30,  \"seq_truncstart\":False, \"remove_oovs\":True})\n",
    "        seq_brands = WordSeq(wb_brands, {\"seq_maxlen\": 3,  \"seq_truncstart\":False, \"remove_oovs\":True})\n",
    "        \n",
    "        data[\"item_name\"] = list(zip(seq_name_desc.transform(wb_name.transform(data[\"item_description\"].astype(str)))))\n",
    "        data[\"name_brand\"] = list(zip(seq_brands.transform(wb_brands.transform(data[\"name\"].astype(str)))))\n",
    "        data[\"name\"] = list(zip(seq_name.transform(wb_name.transform(data[\"name\"].astype(str)))))\n",
    "        \n",
    "        del wb_name, seq_name, seq_name_desc\n",
    "        \n",
    "        print(\"Desc to sequences\")\n",
    "        wb_desc = wordbatch.WordBatch(normalizeString, n_words=50000, extractor=(WordSeq, {\"seq_maxlen\": 70,\n",
    "                                                                                           \"seq_truncstart\":False,\n",
    "                                                                                           \"remove_oovs\":True\n",
    "                                                                            } ))\n",
    "        #wb_desc.fit(data[\"item_description\"].astype(str))\n",
    "        data[\"desc_brand\"] = list(zip(seq_brands.transform(wb_brands.transform(data[\"item_description\"].astype(str)))))\n",
    "        data[\"item_description\"] = list(zip(wb_desc.fit_transform(data[\"item_description\"].astype(str))))\n",
    "        del wb_desc\n",
    "        \n",
    "        print(\"Desc 2gram to sequences\")\n",
    "        #wb_desc2 = wordbatch.WordBatch(normalizeString, n_words=20000, extractor=(WordSeq, {\"seq_maxlen\": 30,\n",
    "        #                                                                                    \"seq_truncstart\":False,\n",
    "        #                                                                                    \"remove_oovs\":True\n",
    "        #                                                                    } ))\n",
    "        #wb_desc2.fit(data[\"item_desc2gram\"].astype(str))\n",
    "        tok_desc2 = Tokenizer(20000)\n",
    "        tok_desc2.fit_on_texts(data['item_desc2gram'].astype(str))\n",
    "        data[\"item_desc2gram\"] = list(zip(sequence.pad_sequences(tok_desc2.texts_to_sequences(data.item_desc2gram.astype(str)),\n",
    "                                         maxlen=20, padding='post', truncating='post')))\n",
    "        #del wb_desc2\n",
    "        \n",
    "        print(\"split train test\")\n",
    "        train_data = data.loc[: train_rows - 1, :].reset_index(drop=True)\n",
    "        train_data = train_data.loc[(train_data.price >= 3), :].reset_index(drop=True)\n",
    "        test_data  = data.loc[train_rows: , :].reset_index(drop=True)\n",
    "        \n",
    "        print(\"Get target encodings for stuff\")\n",
    "        cvlist = list(KFold(6, random_state=100).split(train_data))\n",
    "        \n",
    "        #enc_1 = TargetEncoder(cols=['brand_name'])\n",
    "        #train_data[\"brand_mean\"] = cross_val_predict(enc_1, train_data[['brand_name']], train_data['price'], cv=cvlist, verbose=10, method='transform', n_jobs=1)\n",
    "        #test_data[\"brand_mean\"] = enc_1.fit(train_data[['brand_name']], train_data['price']).transform(test_data)\n",
    "        \n",
    "        \n",
    "        #enc_2 = TargetEncoder(cols=['category_name'])\n",
    "        #train_data[\"category_mean\"] = cross_val_predict(enc_2, train_data[['category_name']], train_data['price'], cv=cvlist, verbose=10, method='transform', n_jobs=1)\n",
    "        #test_data[\"category_mean\"] = enc_2.fit(train_data[['category_name']], train_data['price']).transform(test_data)\n",
    "        \n",
    "        #enc_3 = TargetEncoder(cols=[['brand_name', 'category_name']])\n",
    "        #train_data[\"brandcat_mean\"] = cross_val_predict(enc_3, train_data[['brand_name', 'category_name']], train_data['price'], cv=cvlist, verbose=10, method='transform', n_jobs=1)\n",
    "        #test_data[\"brandcat_mean\"] = enc_3.fit(train_data[['brand_name', 'category_name']], train_data['price']).transform(test_data)\n",
    "        \n",
    "        #train_data[\"brandcat_rat\"] = train_data[\"brandcat_mean\"]/(1 + train_data[\"category_mean\"])\n",
    "        #test_data[\"brandcat_rat\"] = test_data[\"brandcat_mean\"]/(1 + test_data[\"category_mean\"])\n",
    "        \n",
    "        #train_data[\"catbrand_rat\"] = train_data[\"brandcat_mean\"]/(1 + train_data[\"category_mean\"])\n",
    "        #test_data[\"catbrand_rat\"] = test_data[\"brandcat_mean\"]/(1 + test_data[\"brand_mean\"])\n",
    "        \n",
    "        #enc_4 = TargetEncoder(cols=['brand_name'])\n",
    "        #train_data[\"brandvalue\"] = cross_val_predict(enc_4, train_data[['brand_name']], train_data[\"brandcat_rat\"], cv=cvlist, verbose=10, method='transform', n_jobs=1)\n",
    "        #test_data[\"brandvalue\"] = enc_4.fit(train_data[['brand_name']], train_data[\"brandcat_rat\"]).transform(test_data)\n",
    "        \n",
    "        #extra_cols = [\"brand_mean\",\"category_mean\", \"brandcat_mean\",\"catbrand_rat\", \"brandvalue\"]\n",
    "        #scaler = QuantileTransformer()\n",
    "        #train_data[extra_cols]  = scaler.fit_transform(train_data[extra_cols])\n",
    "        #train_data[extra_cols]  = scaler.transform(train_data[extra_cols])\n",
    "        \n",
    "        print(train_data.head())\n",
    "        del train_data['test_id']\n",
    "        del test_data['train_id']\n",
    "        del data \n",
    "        test_data['test_id'] = test_data['test_id'].astype(int)\n",
    "        train_data.to_pickle(os.path.join(out_path, 'train_2.pkl'))\n",
    "        test_data.to_pickle(os.path.join(out_path, 'test_2.pkl'))\n",
    "        \n",
    "        return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EM_NNRegressor(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, embed_cols=None, dense_cols=None, embed_dims=None, \n",
    "                 text_embed_cols=None, text_embed_seq_lens=None, \n",
    "                 text_embed_dims=None, \n",
    "                 #text_embed_tokenizers=None,\n",
    "                 num_layers=2, multiprocess=False,\n",
    "                layer_activations=None, layer_dims=None,layer_dropouts=None, epochs=20, batchsize=32,\n",
    "                optimizer_kwargs=None, val_size=0.1, verbose=1, optim=None, seed=1):\n",
    "        \n",
    "        self.embed_cols = embed_cols\n",
    "        self.dense_cols = dense_cols\n",
    "        self.embed_dims = embed_dims\n",
    "        self.text_embed_cols = text_embed_cols\n",
    "        self.text_embed_dims = text_embed_dims\n",
    "        #self.text_embed_tokenizers = text_embed_tokenizers\n",
    "        self.text_embed_seq_lens = text_embed_seq_lens\n",
    "        self.dense_dims = None\n",
    "        self.num_layers = num_layers\n",
    "        self.layer_dims = layer_dims\n",
    "        self.layer_activations = layer_activations\n",
    "        self.layer_dropouts = layer_dropouts\n",
    "        self.epochs = epochs\n",
    "        self.batchsize = batchsize\n",
    "        self.optimizer_kwargs = optimizer_kwargs\n",
    "        self.val_size = val_size\n",
    "        self.verbose = verbose\n",
    "        self.multiprocess = multiprocess\n",
    "        self.seed = seed\n",
    "        self.optim = optim\n",
    "        self.model = None\n",
    "        if self.dense_cols:\n",
    "            self.dense_dims = len(self.dense_cols)\n",
    "            \n",
    "    def _splitX(self, X):\n",
    "        X_splits = []\n",
    "        \n",
    "        if self.embed_cols:\n",
    "            for col in self.embed_cols :\n",
    "                X_splits.append(X[col].values.reshape(X.shape[0], -1))\n",
    "                \n",
    "        if self.text_embed_cols:\n",
    "            for i, col in enumerate(self.text_embed_cols):\n",
    "                #max_features = self.text_embed_dims[i][0]\n",
    "                #max_len = self.text_embed_seq_lens[i]\n",
    "                #input_text = X[col].astype(str)\n",
    "                #x_train = tok.texts_to_sequences(input_text)\n",
    "                #print(np.mean([len(l) for l in x_train]))\n",
    "                #x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "                #X_splits.append(np.array(x_train).reshape(X.shape[0], -1))\n",
    "                X_splits.append(np.concatenate(X[col].values))\n",
    "                \n",
    "        if self.dense_cols:\n",
    "            X_splits.append(X[self.dense_cols].values.reshape(X.shape[0], -1))\n",
    "            \n",
    "        return X_splits\n",
    "    \n",
    "    \n",
    "    def _build_model(self):\n",
    "        model_inputs = []\n",
    "        model_layers = []\n",
    "        \n",
    "        if self.embed_cols:\n",
    "            for col, dim in zip(self.embed_cols, self.embed_dims):\n",
    "                x1 = Input( shape=(1,), name=col)\n",
    "                model_inputs.append(x1)\n",
    "                x1 = Embedding(input_dim=dim[0], output_dim=dim[1], )(x1)\n",
    "                #x1 = Dropout(0.1)(x1)\n",
    "                x1 = Reshape(target_shape=(dim[1],))(x1)\n",
    "                model_layers.append(x1)\n",
    "                \n",
    "        if self.text_embed_cols:\n",
    "            for col, dim, seq_len in zip(self.text_embed_cols, \n",
    "                                                self.text_embed_dims, \n",
    "                                                self.text_embed_seq_lens):\n",
    "                x3 = Input( shape=(seq_len,))\n",
    "                model_inputs.append(x3)\n",
    "                x3 = Embedding(input_dim=dim[0], output_dim=dim[1], input_length=seq_len)(x3)\n",
    "                x3 = GlobalAveragePooling1D()(x3)\n",
    "                x3 = Reshape(target_shape=(dim[1],))(x3)\n",
    "                model_layers.append(x3)\n",
    "                \n",
    "        if self.dense_cols:\n",
    "            x2 = Input( shape=(self.dense_dims, ), name='dense_cols')\n",
    "            model_inputs.append(x2)\n",
    "            model_layers.append(x2)\n",
    "        print(model_layers)\n",
    "        x = concatenate(model_layers)\n",
    "        \n",
    "        if self.num_layers > 0:\n",
    "            for dim, drops in zip(self.layer_dims, self.layer_dropouts):\n",
    "                x = BatchNormalization()(x)\n",
    "                x = Dropout(rate=drops)(x)\n",
    "                x = Dense(dim, activation='selu', kernel_initializer='he_normal')(x)\n",
    "                x = PReLU()(x)\n",
    "        \n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.03)(x)\n",
    "        output = Dense(1, activation='linear', kernel_initializer='normal')(x)\n",
    "        \n",
    "        model = Model(inputs=model_inputs, outputs=output)\n",
    "        #print(model.summary())\n",
    "        adam = self.optim\n",
    "        #adam = Nadam(lr=0.0012, schedule_decay=0.01)\n",
    "        model.compile(optimizer=adam, loss='mean_squared_error' )\n",
    "        \n",
    "        return model \n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model = self._build_model()\n",
    "        if self.val_size > 0:\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.val_size, random_state=self.seed)\n",
    "            print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "            \n",
    "            callbacks= [ModelCheckpoint(\"embed_NN_\"+str(self.seed)+\".check\", save_best_only=True, verbose=1)]\n",
    "            if self.multiprocess == False:\n",
    "                self.model.fit(self._splitX(X_train), y_train, batch_size=self.batchsize, epochs=self.epochs,\n",
    "                               verbose=self.verbose,\n",
    "                              validation_data=(self._splitX(X_val), y_val), shuffle=True,\n",
    "                              callbacks=callbacks)\n",
    "            else:\n",
    "                X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.val_size, random_state=1)\n",
    "\n",
    "        else:\n",
    "            self.model.fit(self._splitX(X), y, batch_size=self.batchsize, epochs=self.epochs,\n",
    "               verbose=self.verbose, shuffle=True)\n",
    "\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        \n",
    "        if self.model:\n",
    "            if (self.val_size > 0):\n",
    "                model = load_model(\"embed_NN_\"+str(self.seed)+\".check\")\n",
    "                y_hat = model.predict(self._splitX(X))\n",
    "            else:\n",
    "                y_hat = self.model.predict(self._splitX(X))\n",
    "        else:\n",
    "            #y_hat = self.model.predict(self._splitX(X))\n",
    "            raise ValueError(\"Model not fit yet\")\n",
    "            \n",
    "        return y_hat\n",
    "        \n",
    "def add_ngrams(text, ngram=2):\n",
    "    word_list = str(text).lower().split(' ')\n",
    "    out_list = [''.join(word_list[i:i+ngram]) for i in range(len(word_list))]\n",
    "    return ' '.join(out_list[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = read_data(\"../input\", \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>item_description</th>\n",
       "      <th>name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>train_id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>...</th>\n",
       "      <th>plus_counts</th>\n",
       "      <th>ands_counts</th>\n",
       "      <th>comma_counts</th>\n",
       "      <th>all_counts</th>\n",
       "      <th>brand_cat</th>\n",
       "      <th>category_shipping</th>\n",
       "      <th>item_desc2gram</th>\n",
       "      <th>item_name</th>\n",
       "      <th>name_brand</th>\n",
       "      <th>desc_brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>831</td>\n",
       "      <td>3</td>\n",
       "      <td>([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>([2628, 4723, 5010, 122, 20, 5, 54],)</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22240</td>\n",
       "      <td>2138</td>\n",
       "      <td>([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>([306, 1708, 4443],)</td>\n",
       "      <td>([0, 0, 0],)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3891</td>\n",
       "      <td>88</td>\n",
       "      <td>3</td>\n",
       "      <td>([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>([0, 0, 0, 5791, 12917, 10230, 1562],)</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31503</td>\n",
       "      <td>2240</td>\n",
       "      <td>([25, 4, 305, 1337, 7645, 1015, 1822, 6129, 12...</td>\n",
       "      <td>([1084, 1562, 975, 93, 822, 950, 11, 242, 249,...</td>\n",
       "      <td>([0, 0, 821],)</td>\n",
       "      <td>([803, 658, 2846],)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4590</td>\n",
       "      <td>1279</td>\n",
       "      <td>1</td>\n",
       "      <td>([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>([0, 0, 0, 0, 0, 0, 195],)</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10</td>\n",
       "      <td>105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008264</td>\n",
       "      <td>0.008264</td>\n",
       "      <td>37612</td>\n",
       "      <td>603</td>\n",
       "      <td>([624, 4268, 7937, 3321, 235, 2168, 74, 392, 4...</td>\n",
       "      <td>([0, 1354, 19, 76, 169, 24, 85, 11, 169, 646, ...</td>\n",
       "      <td>([0, 0, 0],)</td>\n",
       "      <td>([934, 177, 3323],)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>505</td>\n",
       "      <td>1</td>\n",
       "      <td>([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>([0, 0, 0, 0, 118, 1608, 11691],)</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21883</td>\n",
       "      <td>1426</td>\n",
       "      <td>([6, 9, 92, 193, 37, 3849, 787, 34, 259, 194, ...</td>\n",
       "      <td>([4, 76, 767, 118, 5196, 2585, 7, 137, 2135, 9...</td>\n",
       "      <td>([0, 794, 2679],)</td>\n",
       "      <td>([86, 794, 243],)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1206</td>\n",
       "      <td>1</td>\n",
       "      <td>([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>([0, 0, 0, 3529, 48, 911, 139],)</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21352</td>\n",
       "      <td>445</td>\n",
       "      <td>([2687, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>([0, 1821, 1254],)</td>\n",
       "      <td>([0, 2960, 289],)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   brand_name  category_name  item_condition_id  \\\n",
       "0           3            831                  3   \n",
       "1        3891             88                  3   \n",
       "2        4590           1279                  1   \n",
       "3           3            505                  1   \n",
       "4           3           1206                  1   \n",
       "\n",
       "                                    item_description  \\\n",
       "0  ([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1  ([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "2  ([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "3  ([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "4  ([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "\n",
       "                                     name  price  shipping  train_id  cat1  \\\n",
       "0   ([2628, 4723, 5010, 122, 20, 5, 54],)   10.0         1       0.0     6   \n",
       "1  ([0, 0, 0, 5791, 12917, 10230, 1562],)   52.0         0       1.0     2   \n",
       "2              ([0, 0, 0, 0, 0, 0, 195],)   10.0         1       2.0    10   \n",
       "3       ([0, 0, 0, 0, 118, 1608, 11691],)   35.0         1       3.0     4   \n",
       "4        ([0, 0, 0, 3529, 48, 911, 139],)   44.0         0       4.0    10   \n",
       "\n",
       "   cat2         ...           plus_counts  ands_counts  comma_counts  \\\n",
       "0   104         ...                   0.0          0.0      0.000000   \n",
       "1    32         ...                   0.0          0.0      0.000000   \n",
       "2   105         ...                   0.0          0.0      0.008264   \n",
       "3    57         ...                   0.0          0.0      0.000000   \n",
       "4    60         ...                   0.0          0.0      0.000000   \n",
       "\n",
       "   all_counts  brand_cat  category_shipping  \\\n",
       "0    0.000000      22240               2138   \n",
       "1    0.000000      31503               2240   \n",
       "2    0.008264      37612                603   \n",
       "3    0.000000      21883               1426   \n",
       "4    0.000000      21352                445   \n",
       "\n",
       "                                      item_desc2gram  \\\n",
       "0  ([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1  ([25, 4, 305, 1337, 7645, 1015, 1822, 6129, 12...   \n",
       "2  ([624, 4268, 7937, 3321, 235, 2168, 74, 392, 4...   \n",
       "3  ([6, 9, 92, 193, 37, 3849, 787, 34, 259, 194, ...   \n",
       "4  ([2687, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "\n",
       "                                           item_name            name_brand  \\\n",
       "0  ([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  ([306, 1708, 4443],)   \n",
       "1  ([1084, 1562, 975, 93, 822, 950, 11, 242, 249,...        ([0, 0, 821],)   \n",
       "2  ([0, 1354, 19, 76, 169, 24, 85, 11, 169, 646, ...          ([0, 0, 0],)   \n",
       "3  ([4, 76, 767, 118, 5196, 2585, 7, 137, 2135, 9...     ([0, 794, 2679],)   \n",
       "4  ([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...    ([0, 1821, 1254],)   \n",
       "\n",
       "            desc_brand  \n",
       "0         ([0, 0, 0],)  \n",
       "1  ([803, 658, 2846],)  \n",
       "2  ([934, 177, 3323],)  \n",
       "3    ([86, 794, 243],)  \n",
       "4    ([0, 2960, 289],)  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_183/Reshape:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'reshape_184/Reshape:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'reshape_185/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_186/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_187/Reshape:0' shape=(?, 8) dtype=float32>, <tf.Tensor 'reshape_188/Reshape:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'reshape_189/Reshape:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'reshape_190/Reshape:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'reshape_191/Reshape:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'reshape_192/Reshape:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'reshape_193/Reshape:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'reshape_194/Reshape:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'reshape_195/Reshape:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'dense_cols_14:0' shape=(?, 22) dtype=float32>]\n",
      "(1452027, 38) (29634, 38) (1452027,) (29634,)\n",
      "Train on 1452027 samples, validate on 29634 samples\n",
      "Epoch 1/5\n",
      "1445888/1452027 [============================>.] - ETA: 0s - loss: 0.4243Epoch 00001: val_loss improved from inf to 0.19975, saving model to embed_NN_1.check\n",
      "1452027/1452027 [==============================] - 17s 12us/step - loss: 0.4234 - val_loss: 0.1997\n",
      "Epoch 2/5\n",
      "1445888/1452027 [============================>.] - ETA: 0s - loss: 0.1761Epoch 00002: val_loss improved from 0.19975 to 0.17282, saving model to embed_NN_1.check\n",
      "1452027/1452027 [==============================] - 14s 9us/step - loss: 0.1761 - val_loss: 0.1728\n",
      "Epoch 3/5\n",
      "1445888/1452027 [============================>.] - ETA: 0s - loss: 0.1584Epoch 00003: val_loss improved from 0.17282 to 0.17033, saving model to embed_NN_1.check\n",
      "1452027/1452027 [==============================] - 14s 9us/step - loss: 0.1584 - val_loss: 0.1703\n",
      "Epoch 4/5\n",
      "1445888/1452027 [============================>.] - ETA: 0s - loss: 0.1490Epoch 00004: val_loss did not improve\n",
      "1452027/1452027 [==============================] - 13s 9us/step - loss: 0.1490 - val_loss: 0.1735\n",
      "Epoch 5/5\n",
      "1445888/1452027 [============================>.] - ETA: 0s - loss: 0.1425Epoch 00005: val_loss improved from 0.17033 to 0.16928, saving model to embed_NN_1.check\n",
      "1452027/1452027 [==============================] - 13s 9us/step - loss: 0.1425 - val_loss: 0.1693\n",
      "[<tf.Tensor 'reshape_196/Reshape:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'reshape_197/Reshape:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'reshape_198/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_199/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_200/Reshape:0' shape=(?, 8) dtype=float32>, <tf.Tensor 'reshape_201/Reshape:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'reshape_202/Reshape:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'reshape_203/Reshape:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'reshape_204/Reshape:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'reshape_205/Reshape:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'reshape_206/Reshape:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'reshape_207/Reshape:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'reshape_208/Reshape:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'dense_cols_15:0' shape=(?, 22) dtype=float32>]\n",
      "(1452027, 38) (29634, 38) (1452027,) (29634,)\n",
      "Train on 1452027 samples, validate on 29634 samples\n",
      "Epoch 1/5\n",
      "1445888/1452027 [============================>.] - ETA: 0s - loss: 0.4308Epoch 00001: val_loss improved from inf to 0.18877, saving model to embed_NN_2.check\n",
      "1452027/1452027 [==============================] - 17s 12us/step - loss: 0.4298 - val_loss: 0.1888\n",
      "Epoch 2/5\n",
      "1445888/1452027 [============================>.] - ETA: 0s - loss: 0.1762Epoch 00002: val_loss improved from 0.18877 to 0.17174, saving model to embed_NN_2.check\n",
      "1452027/1452027 [==============================] - 14s 9us/step - loss: 0.1762 - val_loss: 0.1717\n",
      "Epoch 3/5\n",
      "1445888/1452027 [============================>.] - ETA: 0s - loss: 0.1592Epoch 00003: val_loss improved from 0.17174 to 0.16901, saving model to embed_NN_2.check\n",
      "1452027/1452027 [==============================] - 14s 10us/step - loss: 0.1592 - val_loss: 0.1690\n",
      "Epoch 4/5\n",
      "1445888/1452027 [============================>.] - ETA: 0s - loss: 0.1497Epoch 00004: val_loss improved from 0.16901 to 0.16796, saving model to embed_NN_2.check\n",
      "1452027/1452027 [==============================] - 14s 9us/step - loss: 0.1497 - val_loss: 0.1680\n",
      "Epoch 5/5\n",
      "1445888/1452027 [============================>.] - ETA: 0s - loss: 0.1433Epoch 00005: val_loss did not improve\n",
      "1452027/1452027 [==============================] - 13s 9us/step - loss: 0.1433 - val_loss: 0.1701\n",
      "[<tf.Tensor 'reshape_209/Reshape:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'reshape_210/Reshape:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'reshape_211/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_212/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_213/Reshape:0' shape=(?, 8) dtype=float32>, <tf.Tensor 'reshape_214/Reshape:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'reshape_215/Reshape:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'reshape_216/Reshape:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'reshape_217/Reshape:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'reshape_218/Reshape:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'reshape_219/Reshape:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'reshape_220/Reshape:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'reshape_221/Reshape:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'dense_cols_16:0' shape=(?, 22) dtype=float32>]\n",
      "(1452027, 38) (29634, 38) (1452027,) (29634,)\n",
      "Train on 1452027 samples, validate on 29634 samples\n",
      "Epoch 1/5\n",
      "1445888/1452027 [============================>.] - ETA: 0s - loss: 0.4357Epoch 00001: val_loss improved from inf to 0.20303, saving model to embed_NN_3.check\n",
      "1452027/1452027 [==============================] - 18s 12us/step - loss: 0.4347 - val_loss: 0.2030\n",
      "Epoch 2/5\n",
      "1445888/1452027 [============================>.] - ETA: 0s - loss: 0.1762Epoch 00002: val_loss improved from 0.20303 to 0.17488, saving model to embed_NN_3.check\n",
      "1452027/1452027 [==============================] - 14s 9us/step - loss: 0.1762 - val_loss: 0.1749\n",
      "Epoch 3/5\n",
      "1445888/1452027 [============================>.] - ETA: 0s - loss: 0.1590Epoch 00003: val_loss did not improve\n",
      "1452027/1452027 [==============================] - 14s 9us/step - loss: 0.1590 - val_loss: 0.1762\n",
      "Epoch 4/5\n",
      "1445888/1452027 [============================>.] - ETA: 0s - loss: 0.1495Epoch 00004: val_loss improved from 0.17488 to 0.17269, saving model to embed_NN_3.check\n",
      "1452027/1452027 [==============================] - 14s 9us/step - loss: 0.1495 - val_loss: 0.1727\n",
      "Epoch 5/5\n",
      "1445888/1452027 [============================>.] - ETA: 0s - loss: 0.1432Epoch 00005: val_loss improved from 0.17269 to 0.17092, saving model to embed_NN_3.check\n",
      "1452027/1452027 [==============================] - 13s 9us/step - loss: 0.1432 - val_loss: 0.1709\n",
      "[<tf.Tensor 'reshape_222/Reshape:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'reshape_223/Reshape:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'reshape_224/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_225/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_226/Reshape:0' shape=(?, 8) dtype=float32>, <tf.Tensor 'reshape_227/Reshape:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'reshape_228/Reshape:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'reshape_229/Reshape:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'reshape_230/Reshape:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'reshape_231/Reshape:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'reshape_232/Reshape:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'reshape_233/Reshape:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'reshape_234/Reshape:0' shape=(?, 16) dtype=float32>, <tf.Tensor 'dense_cols_17:0' shape=(?, 22) dtype=float32>]\n",
      "(1452027, 38) (29634, 38) (1452027,) (29634,)\n"
     ]
    }
   ],
   "source": [
    "for seed in [1,2,3, 786]:\n",
    "    nnet1 = EM_NNRegressor(embed_cols=['brand_name','category_name','item_condition_id', 'cat1', 'cat2', 'cat3', \n",
    "                                       'category_shipping', \n",
    "                                       \"brand_cat\"], \n",
    "                      embed_dims=[(5600, 32),(1500, 32), (6,4), (16,4), (121, 8), (900, 16), \n",
    "                                  (3500, 32), \n",
    "                                  (51000, 32)],\n",
    "                      text_embed_cols=['name', 'item_description', 'item_desc2gram',  'name_brand', 'desc_brand'],\n",
    "                      text_embed_dims=[(20002, 32), (50002, 32), (20002, 32), (4501, 16), (4501, 16)],\n",
    "                      text_embed_seq_lens =[7, 70, 20, 3, 3],\n",
    "                      #text_embed_tokenizers = [tok_name, tok_desc, tok_desc2],\n",
    "                      dense_cols=['shipping', \n",
    "                                  'desc_words', 'desc_chars', 'name_chars', 'name_words',\n",
    "                                'iphone_case', 'iphone6', 'iphone6p',\n",
    "                                'iphone5', 'iphone5p', 'iphone7', 'iphone7p', 'unlocked_phone',\n",
    "                                  'brand_counts', 'cat_counts',\n",
    "                                   'cat1_counts', 'cat2_counts', 'cat3_counts',\n",
    "                                  'plus_counts', 'ands_counts', 'comma_counts', 'all_counts',\n",
    "                                   #\"brand_mean\",\"category_mean\", \"brandcat_mean\",\"catbrand_rat\", \"brandvalue\"\n",
    "                                   ],\n",
    "                      epochs=5,\n",
    "                      batchsize=2048 ,\n",
    "                      num_layers = 1,\n",
    "                      layer_dropouts=[0.12],\n",
    "                      layer_dims=[256],\n",
    "                      seed=seed,\n",
    "                      val_size=0.02,\n",
    "                      optim=RMSprop(lr=0.009, decay=0.005, clipvalue=2.5, rho=0.9)\n",
    "                      #optim=Adam(lr=0.004, beta_1=0.9, decay=0.004, clipvalue=2.5)\n",
    "                     )\n",
    "    nnet1.fit(train_data, np.log1p(train_data.price) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Base\n",
    "#scores - .1689, .1670, .1706, .1690 - .1689\n",
    "#scores - .1686, .1660, .1739, .1667 - .1688\n",
    "\n",
    "#Removing item_name\n",
    "#scores - .1676, .1685, .1717, .1670 - .1687\n",
    "#scores - .1676, .1679, .1713, .1662 - .1683\n",
    "\n",
    "#Removing len features\n",
    "#scores - .1706, .1681, .1723, .1692 - .1701 -- including again\n",
    "\n",
    "#Adding normalization (item_name, category_shipping removed and len features included)\n",
    "#scores - .1690, .1671, .1713, .1679 - .1689\n",
    "#scores - .1701, .1676, .1727, .1672 - .1694\n",
    "\n",
    "#desc embed dim 64 --> 32\n",
    "#scores - .1683, .1674, .1724, .1672 - .1688 (hard to measure change, lets keep it here)\n",
    "\n",
    "#Introducing grad clip value of 2.5\n",
    "#scores - .1690, .1670, .1715, .1663 - .1685\n",
    "\n",
    "#Introducing grad clip value of 1.0 -- no impact i guess\n",
    "#scores - .1688, .1672, .1714, .1683 - .1689\n",
    "\n",
    "#Add ncategory shipping and grad clip 1.0\n",
    "#scores - .1682, .1671, .1718, .1672 - .1686\n",
    "\n",
    "#Add ncategory shipping and grad clip 5.0\n",
    "#scores - .1686, .1677, .1726, .1674 - \n",
    "\n",
    "#grad clip 2.5 and rho 0.8\n",
    "#scores - .1702, .1674, .1721, .1675\n",
    "\n",
    "#grad clip 2.5 and rho 0.9, epislon 1e-5\n",
    "#scores - .1688, .1674, .1737, .1677\n",
    "\n",
    "#Adam(lr=0.004, beta_1=0.9, decay=0.004, clipvalue=2.5)\n",
    "#scores - .1732, .1735, .1763, .1712\n",
    "\n",
    "#128 --> 256 intermediate layer\n",
    "#scores - .1693, .1680, .\n",
    "\n",
    "#128 --> 64 intermediate layer\n",
    "#scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
