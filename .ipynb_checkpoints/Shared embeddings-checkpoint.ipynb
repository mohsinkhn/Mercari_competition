{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae96f65f47f44f139f915c72e2fe3beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/mohsin/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import hashlib\n",
    "import string\n",
    "import unicodedata\n",
    "import re\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm.pandas(tqdm_notebook())\n",
    "\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder, QuantileTransformer\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "#from nltk.corpus import stopwords\n",
    "#from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from itertools import chain\n",
    "\n",
    "# from __future__ import print_function\n",
    "np.random.seed(786)  # for reproducibility\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D, ZeroPadding1D, AveragePooling1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier,  KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions we need - Feature Selector, Fasttext_Estimator, Preprocessing Transformer, Binary_Encoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "from pandas.api.types import is_numeric_dtype, is_string_dtype\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "rmse_sklearn = make_scorer(rmse, greater_is_better=False)    \n",
    "    \n",
    "# the following functions allow for a parallelized batch generator\n",
    "class threadsafe_iter(object):\n",
    "    \"\"\"\n",
    "    Takes an iterator/generator and makes it thread-safe by\n",
    "    serializing call to the `next` method of given iterator/generator.\n",
    "    \"\"\"\n",
    "    def __init__(self, it):\n",
    "        self.it = it\n",
    "        self.lock = threading.Lock()\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        with self.lock:\n",
    "            return next(self.it)\n",
    "\n",
    "def threadsafe_generator(f):\n",
    "    \"\"\"\n",
    "    A decorator that takes a generator function and makes it thread-safe.\n",
    "    \"\"\"\n",
    "    def g(*a, **kw):\n",
    "        return threadsafe_iter(f(*a, **kw))\n",
    "    return g\n",
    "\n",
    "@threadsafe_generator\n",
    "def batch_generator(X_data, y_data, batch_size):\n",
    "    \n",
    "    #index = np.random.permutation(X_data.shape[0])    \n",
    "    #X_data = X_data[index]\n",
    "    #y_data = y_data[index]\n",
    "    \n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    index = np.arange(np.shape(y_data)[0])\n",
    "    #idx = 1\n",
    "    while 1:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:].todense()\n",
    "        y_batch = y_data[index_batch]\n",
    "        counter += 1\n",
    "        yield np.array(X_batch),y_batch\n",
    "        #print(\"\")\n",
    "        #print(X_batch.shape)\n",
    "        #print(\"\")\n",
    "        #print('generator yielded a batch %d' % idx)\n",
    "        #idx += 1\n",
    "        if (counter > number_of_batches):\n",
    "            counter=0\n",
    "            \n",
    "            \n",
    "@threadsafe_generator\n",
    "def batch_generator_x(X_data,batch_size):\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    index = np.arange(np.shape(X_data)[0])\n",
    "    while 1:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:].todense()\n",
    "        counter += 1\n",
    "        yield np.array(X_batch)\n",
    "        if (counter > number_of_batches):\n",
    "            counter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions = 30\n",
    "num_cores = 16\n",
    "from multiprocessing import Pool, cpu_count\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "stop_words = ['ourselves', 'hers', 'between', 'yourself', 'but', 'again', \n",
    "              'there', 'about', 'once', 'during', 'out', 'very', 'having', \n",
    "              'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', \n",
    "              'its', 'yours', 'such', 'into', 'most', 'itself', 'other', \n",
    "              'off', 'is', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', \n",
    "              'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', \n",
    "              'through', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', \n",
    "              'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', \n",
    "              'ours', 'had', 'she', 'all', 'when', 'at', 'any', 'before', 'them',\n",
    "              'same', 'and', 'been', 'have', 'in', 'will', 'does', 'yourselves', \n",
    "              'then', 'that', 'because', 'what', 'over', 'why’, ‘so', 'can', 'did',\n",
    "              'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only',\n",
    "              'myself', 'which', 'those', 'i','after', 'few', 'whom', 'being', 'if', \n",
    "              'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return  unicodedata.normalize('NFKC', s)\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"'\", r\"\", s)\n",
    "    s = re.sub(r\"[.!?':;,]\", r\" \", s)\n",
    "    s = re.sub(r\"-\", r\"\", s)\n",
    "    s = re.sub(r\"[^0-9a-zA-Z.!?]+\", r\" \", s)\n",
    "    #s = re.sub(r\"0\", r\"zero\", s)\n",
    "    #s = re.sub(r\"1\", r\"one\", s)\n",
    "    #s = re.sub(r\"2\", r\"two\", s)\n",
    "    #s = re.sub(r\"3\", r\"three\", s)\n",
    "    #s = re.sub(r\"4\", r\"four\", s)\n",
    "    #s = re.sub(r\"5\", r\"five\", s)\n",
    "    #s = re.sub(r\"6\", r\"six\", s)\n",
    "    #s = re.sub(r\"7\", r\"seven\", s)\n",
    "    #s = re.sub(r\"8\", r\"eight\", s)\n",
    "    #s = re.sub(r\"/s/s\", r\"/s\", s)\n",
    "    return s\n",
    "\n",
    "def _normalize_and_ngrams(sent, ngrams):\n",
    "    input_list = normalizeString(sent).split()\n",
    "    input_list = [word for word in input_list if word not in stop_words]\n",
    "    s = input_list.copy()\n",
    "    for i in range(2, ngrams+1):\n",
    "        s += [' '.join(input_list[j:j+i]) for j in range(len(input_list)-i + 1)]\n",
    "        #s += list((zip(*[input_list[j:] for j in range(i)])))\n",
    "    return s\n",
    "\n",
    "#tmp = \"I am not a dance'r and i am a 6ixy   c-o:d;er programmer\"\n",
    "#print(normalizeString(tmp))\n",
    "#print(_normalize_and_ngrams(tmp, 3))\n",
    "\n",
    "class Vocab_topwords():\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.index2word = {}\n",
    "        \n",
    "    def fit_data(self, data, cols, ngrams=3, max_features=50000):\n",
    "        c = Counter()\n",
    "        for col in cols:\n",
    "            c += Counter(list(chain.from_iterable(data[col].tolist())))\n",
    "        for i, (w, count) in enumerate(c.most_common(max_features)):\n",
    "            self.word2index[w] = i\n",
    "        return\n",
    "    \n",
    "\n",
    "            \n",
    "            \n",
    "def prepareVocab(name, data, cols, max_features):\n",
    "    vocab = Vocab_topwords(name)\n",
    "    vocab.fit_data(data, cols, max_features=max_features)\n",
    "    \n",
    "    print(\"Counted words:\")\n",
    "    print(vocab.name, len(vocab.word2index))\n",
    "    return vocab\n",
    "\n",
    "def indexesFromSentence(vocab, tokens, ngrams, max_len):\n",
    "    num_list = []\n",
    "    for i, item in enumerate(tokens):\n",
    "        if len(num_list) == max_len:\n",
    "            break\n",
    "        elif item in vocab.word2index:\n",
    "            num_list.append(vocab.word2index[item])\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "    if len(num_list) < max_len :\n",
    "        num_list += [0]*(max_len - len(num_list) )\n",
    "        \n",
    "    return num_list\n",
    "\n",
    "def parallelize_dataframe(df, func):\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def get_cat_1(x): return str(x).split('/')[0]\n",
    "def get_cat_2(x): return str(x).split('/')[1] if len(str(x).split('/')) > 1 else -1\n",
    "def get_cat_3(x): return ' '.join(str(x).split('/')[2:]) if len(str(x).split('/')) > 2 else -1\n",
    "\n",
    "def applycat1(df): \n",
    "    df['cat1'] = df['category_name'].progress_apply(get_cat_1)\n",
    "    return df\n",
    "\n",
    "def applycat2(df): \n",
    "    df['cat2'] = df['category_name'].progress_apply(get_cat_2)\n",
    "    return df\n",
    "\n",
    "def applycat3(df): \n",
    "    df['cat3'] = df['category_name'].progress_apply(get_cat_3)\n",
    "    return df\n",
    "\n",
    "def norm3grams(s): return _normalize_and_ngrams(s, 3)\n",
    "\n",
    "def norm1grams(s): return _normalize_and_ngrams(s, 1)\n",
    "\n",
    "def applyname(series): return series.progress_apply(norm3grams)\n",
    "\n",
    "def applycategory(series): return series.progress_apply(norm1grams)\n",
    "\n",
    "def index2sent1(x, name_vocab): return indexesFromSentence(name_vocab, x, 3, 10)\n",
    "\n",
    "def name2index(series): return series.progress_apply(lambda x: index2sent1(x, all_vocab))\n",
    "\n",
    "def applydesc(series):return series.progress_apply(norm1grams)\n",
    "\n",
    "def index2sent2(x, desc_vocab): return indexesFromSentence(desc_vocab, x, 1, 20)\n",
    "\n",
    "def desc2index(series): return series.progress_apply(lambda x: index2sent2(x, all_vocab))\n",
    "\n",
    "def read_data(in_path, out_path):\n",
    "    if False and os.path.exists(os.path.join(out_path, 'train_2.pkl')) and os.path.exists(os.path.join(out_path, 'test_2.pkl')):\n",
    "        train_data = pd.read_pickle(os.path.join(out_path, 'train_2.pkl'))\n",
    "        test_data  = pd.read_pickle(os.path.join(out_path, 'test_2.pkl'))\n",
    "        \n",
    "        return train_data, test_data\n",
    "    \n",
    "    else:\n",
    "        train_data = pd.read_table(os.path.join(in_path, 'train.tsv'))\n",
    "        test_data  = pd.read_table(os.path.join(in_path, 'test.tsv'))\n",
    "    \n",
    "        train_rows = len(train_data)\n",
    "        data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "        \n",
    "        data['name'] = data['name'].astype(str)\n",
    "        data['item_description'] = data['item_description'].astype(str)\n",
    "        \n",
    "        #ddata = dd.from_pandas(data, 4)\n",
    "        data['category_words'] = parallelize_dataframe(data['category_name'].astype(str), applycategory)\n",
    "        \n",
    "        data = applycat1(data)\n",
    "        data = applycat2(data)\n",
    "        data = applycat3(data)\n",
    "        data.fillna(-1, inplace=True)\n",
    "        cat_cols = ['category_name', 'brand_name', 'item_condition_id', 'cat1', 'cat2', 'cat3']\n",
    "        print(\"Label enoding categoricals\")\n",
    "        for col in cat_cols:\n",
    "            data[col] = LabelEncoder().fit_transform(data[col].astype(str)).astype(np.int32)\n",
    "            \n",
    "        print(\"Tokenizing text columns\")\n",
    "        data['name'] = parallelize_dataframe(data['name'], applyname)\n",
    "\n",
    "        print(\"Preparing vocabs\")\n",
    "        #global name_vocab\n",
    "        #name_vocab = prepareVocab('name', data, ['name'], 50000)\n",
    "        global all_vocab\n",
    "        all_vocab = prepareVocab('name', data, ['name', 'category_words'], 300000)\n",
    "        \n",
    "        data['name'] = name2index(data['name'])\n",
    "        #del name_vocab\n",
    "        \n",
    "        print(\"Transforming text to sequences\")\n",
    "        data['item_description'] = parallelize_dataframe(data['item_description'], applydesc)\n",
    "        #global desc_vocab\n",
    "        #desc_vocab = prepareVocab('item_description', data, ['item_description'], 250000)\n",
    "        data['item_description'] = desc2index(data['item_description'])\n",
    "        #del desc_vocab\n",
    "        del data['category_words']\n",
    "        \n",
    "        train_data = data.loc[: train_rows - 1, :]\n",
    "        train_data = train_data.loc[(train_data.price >= 1) & (train_data.price <= 2100), :].reset_index(drop=True)\n",
    "        test_data  = data.loc[train_rows: , :].reset_index(drop=True)\n",
    "        \n",
    "        del train_data['test_id']\n",
    "        del test_data['train_id']\n",
    "        del data\n",
    "        gc.collect()\n",
    "        print(\"Writing out new pickles dataframes\")\n",
    "        train_data.to_pickle(os.path.join(out_path, 'train_2.pkl'))\n",
    "        test_data.to_pickle(os.path.join(out_path, 'test_2.pkl'))\n",
    "        \n",
    "        return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "class EM_NNRegressor(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, embed_cols=None, dense_cols=None, embed_dims=None, \n",
    "                 text_embed_cols=None, text_embed_seq_lens=None, \n",
    "                 text_embed_dims=None,\n",
    "                 num_layers=2, multiprocess=False,\n",
    "                layer_activations=None, layer_dims=None,layer_dropouts=None, epochs=20, batchsize=32,\n",
    "                optimizer_kwargs=None, val_size=0.1, verbose=1, seed=1):\n",
    "        \n",
    "        self.embed_cols = embed_cols\n",
    "        self.dense_cols = dense_cols\n",
    "        self.embed_dims = embed_dims\n",
    "        self.text_embed_cols = text_embed_cols\n",
    "        self.text_embed_dims = text_embed_dims\n",
    "        #self.text_embed_tokenizers = text_embed_tokenizers\n",
    "        self.text_embed_seq_lens = text_embed_seq_lens\n",
    "        self.dense_dims = None\n",
    "        self.num_layers = num_layers\n",
    "        self.layer_dims = layer_dims\n",
    "        self.layer_activations = layer_activations\n",
    "        self.layer_dropouts = layer_dropouts\n",
    "        self.epochs = epochs\n",
    "        self.batchsize = batchsize\n",
    "        self.optimizer_kwargs = optimizer_kwargs\n",
    "        self.val_size = val_size\n",
    "        self.verbose = verbose\n",
    "        self.multiprocess = multiprocess\n",
    "        self.seed = seed\n",
    "        self.model = None\n",
    "        if self.dense_cols:\n",
    "            self.dense_dims = len(self.dense_cols)\n",
    "            \n",
    "    def _splitX(self, X):\n",
    "        X_splits = []\n",
    "        \n",
    "        if self.embed_cols:\n",
    "            for col in self.embed_cols :\n",
    "                X_splits.append(np.asarray(X[col]))\n",
    "                \n",
    "        if self.text_embed_cols:\n",
    "            for i, col in enumerate(self.text_embed_cols):\n",
    "                X_splits.append(np.asarray([*X[col].values]))\n",
    "                \n",
    "        if self.dense_cols:\n",
    "            X_splits.append(X[self.dense_cols].values.reshape(X.shape[0], -1))\n",
    "            \n",
    "        return X_splits\n",
    "    \n",
    "    \n",
    "    def _build_model(self):\n",
    "        np.random.seed(786)\n",
    "        model_inputs = []\n",
    "        model_layers = []\n",
    "        \n",
    "        if self.embed_cols:\n",
    "            for col, dim in zip(self.embed_cols, self.embed_dims):\n",
    "                x1 = Input( shape=(1,), name=col)\n",
    "                model_inputs.append(x1)\n",
    "                x1 = Embedding(input_dim=dim[0], output_dim=dim[1],)(x1)\n",
    "                #x1 = Dropout(0.1)(x1)\n",
    "                x1 = Reshape(target_shape=(dim[1],))(x1)\n",
    "                model_layers.append(x1)\n",
    "                \n",
    "        if self.text_embed_cols:\n",
    "            for col, dim, seq_len in zip(self.text_embed_cols, \n",
    "                                                self.text_embed_dims, \n",
    "                                                self.text_embed_seq_lens):\n",
    "                x3 = Input( shape=(seq_len,))\n",
    "                model_inputs.append(x3)\n",
    "                x3 = Embedding(input_dim=dim[0], output_dim=dim[1], input_length=seq_len,)(x3)\n",
    "                #x3 = Conv1D(16, return_sequences=True)(x3)\n",
    "                x3 = GlobalAveragePooling1D()(x3)\n",
    "                x3 = Reshape(target_shape=(dim[1],))(x3)\n",
    "                model_layers.append(x3)\n",
    "                \n",
    "        if self.dense_cols:\n",
    "            x2 = Input( shape=(self.dense_dims, ), name='dense_cols')\n",
    "            model_inputs.append(x2)\n",
    "            model_layers.append(x2)\n",
    "        print(model_layers)\n",
    "        x = concatenate(model_layers)\n",
    "        \n",
    "        if self.num_layers > 0:\n",
    "            for dim, drops in zip(self.layer_dims, self.layer_dropouts):\n",
    "                x = BatchNormalization()(x)\n",
    "                x = Dropout(rate=drops, seed=self.seed)(x)\n",
    "                x = Dense(dim, kernel_initializer='he_normal')(x)\n",
    "                #x = Dense(dim, activation='selu', kernel_initializer='he_normal')(x)\n",
    "                x = LeakyReLU()(x)\n",
    "        \n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.05, seed=self.seed)(x)\n",
    "        output = Dense(1, activation='linear', kernel_initializer='he_normal')(x)\n",
    "        \n",
    "        model = Model(inputs=model_inputs, outputs=output)\n",
    "        #print(model.summary())\n",
    "        adam = RMSprop(lr=0.001, decay=0.002)\n",
    "        #adam = Adam(lr=0.001, decay=1e-4)\n",
    "        model.compile(optimizer=adam, loss='mean_squared_error')\n",
    "        \n",
    "        return model \n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model = self._build_model()\n",
    "        if self.val_size > 0:\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.val_size, random_state=self.seed)\n",
    "            print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "            \n",
    "            callbacks= [ModelCheckpoint(\"embed_NN_\"+str(self.seed)+\".check\", save_best_only=True, verbose=1)]\n",
    "            if self.multiprocess == False:\n",
    "                self.model.fit(self._splitX(X_train), y_train, batch_size=self.batchsize, epochs=self.epochs,\n",
    "                               verbose=self.verbose,\n",
    "                              validation_data=(self._splitX(X_val), y_val), shuffle=True,\n",
    "                              callbacks=callbacks)\n",
    "            else:\n",
    "                X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.val_size, random_state=1)\n",
    "\n",
    "        else:\n",
    "            self.model.fit(self._splitX(X), y, batch_size=self.batchsize, epochs=self.epochs,\n",
    "               verbose=self.verbose, shuffle=True,)\n",
    "\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        \n",
    "        if self.model:\n",
    "            self.model = load_model(\"embed_NN_\"+str(self.seed)+\".check\")\n",
    "            y_hat = self.model.predict(self._splitX(X))\n",
    "        else:\n",
    "            raise ValueError(\"Model not fit yet\")\n",
    "            \n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "class EM_NNRegressor2(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, embed_cols=None, dense_cols=None, embed_dims=None, embed_weights=None,\n",
    "                 text_embed_cols=None, text_embed_seq_lens=None, \n",
    "                 text_embed_dims=None, text_embed_weights=None,\n",
    "                 num_layers=2, multiprocess=False,\n",
    "                layer_activations=None, layer_dims=None,layer_dropouts=None, epochs=20, batchsize=32,\n",
    "                optimizer_kwargs=None, val_size=0.1, verbose=1, seed=1):\n",
    "        \n",
    "        self.embed_cols = embed_cols\n",
    "        self.dense_cols = dense_cols\n",
    "        self.embed_dims = embed_dims\n",
    "        self.embed_weights = embed_weights\n",
    "        self.text_embed_cols = text_embed_cols\n",
    "        self.text_embed_dims = text_embed_dims\n",
    "        self.text_embed_weights = text_embed_weights\n",
    "        self.text_embed_seq_lens = text_embed_seq_lens\n",
    "        self.dense_dims = None\n",
    "        self.num_layers = num_layers\n",
    "        self.layer_dims = layer_dims\n",
    "        self.layer_activations = layer_activations\n",
    "        self.layer_dropouts = layer_dropouts\n",
    "        self.epochs = epochs\n",
    "        self.batchsize = batchsize\n",
    "        self.optimizer_kwargs = optimizer_kwargs\n",
    "        self.val_size = val_size\n",
    "        self.verbose = verbose\n",
    "        self.multiprocess = multiprocess\n",
    "        self.seed = seed\n",
    "        self.model = None\n",
    "        if self.dense_cols:\n",
    "            self.dense_dims = len(self.dense_cols)\n",
    "            \n",
    "    def _splitX(self, X):\n",
    "        X_splits = []\n",
    "        \n",
    "        if self.embed_cols:\n",
    "            for col in self.embed_cols :\n",
    "                X_splits.append(np.asarray(X[col]))\n",
    "                \n",
    "        if self.text_embed_cols:\n",
    "            for i, col in enumerate(self.text_embed_cols):\n",
    "                X_splits.append(np.asarray([*X[col].values]))\n",
    "                \n",
    "        if self.dense_cols:\n",
    "            X_splits.append(X[self.dense_cols].values.reshape(X.shape[0], -1))\n",
    "            \n",
    "        return X_splits\n",
    "    \n",
    "    \n",
    "    def _build_model(self):\n",
    "        np.random.seed(786)\n",
    "        model_inputs = []\n",
    "        model_layers = []\n",
    "        \n",
    "        if self.embed_cols:\n",
    "            for col, dim, weights in zip(self.embed_cols, self.embed_dims, self.embed_weights):\n",
    "                x1 = Input( shape=(1,), name=col)\n",
    "                model_inputs.append(x1)\n",
    "                x1 = Embedding(input_dim=dim[0], output_dim=dim[1], weights=weights)(x1)\n",
    "                #x1 = Dropout(0.1)(x1)\n",
    "                x1 = Reshape(target_shape=(dim[1],))(x1)\n",
    "                model_layers.append(x1)\n",
    "                \n",
    "        if self.text_embed_cols:\n",
    "            for col, dim, seq_len, weights in zip(self.text_embed_cols, \n",
    "                                                self.text_embed_dims, \n",
    "                                                self.text_embed_seq_lens,\n",
    "                                                self.text_embed_weights):\n",
    "                x3 = Input( shape=(seq_len,))\n",
    "                model_inputs.append(x3)\n",
    "                x3 = Embedding(input_dim=dim[0], output_dim=dim[1], input_length=seq_len,\n",
    "                              weights=weights, trainable=False)(x3)\n",
    "                #x3 = Conv1D(16, return_sequences=True)(x3)\n",
    "                x3 = GlobalAveragePooling1D()(x3)\n",
    "                x3 = Reshape(target_shape=(dim[1],))(x3)\n",
    "                model_layers.append(x3)\n",
    "                \n",
    "        if self.dense_cols:\n",
    "            x2 = Input( shape=(self.dense_dims, ), name='dense_cols')\n",
    "            model_inputs.append(x2)\n",
    "            model_layers.append(x2)\n",
    "        print(model_layers)\n",
    "        x = concatenate(model_layers)\n",
    "        \n",
    "        if self.num_layers > 0:\n",
    "            for dim, drops in zip(self.layer_dims, self.layer_dropouts):\n",
    "                x = BatchNormalization()(x)\n",
    "                x = Dropout(rate=drops, seed=self.seed)(x)\n",
    "                x = Dense(dim, kernel_initializer='he_normal')(x)\n",
    "                #x = Dense(dim, activation='selu', kernel_initializer='he_normal')(x)\n",
    "                x = LeakyReLU()(x)\n",
    "        \n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.02, seed=self.seed)(x)\n",
    "        output = Dense(1, activation='linear', kernel_initializer='he_normal')(x)\n",
    "        \n",
    "        model = Model(inputs=model_inputs, outputs=output)\n",
    "        #print(model.summary())\n",
    "        #adam = RMSprop(lr=0.001, decay=0.001)\n",
    "        #adam = Adam(lr=0.001, decay=0)\n",
    "        adam= Nadam()\n",
    "        model.compile(optimizer=adam, loss='mean_squared_error')\n",
    "        \n",
    "        return model \n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model = self._build_model()\n",
    "        if self.val_size > 0:\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.val_size, random_state=self.seed)\n",
    "            print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "            \n",
    "            callbacks= [ModelCheckpoint(\"embed_NN_\"+str(self.seed)+\".check\", save_best_only=True, verbose=1)]\n",
    "            if self.multiprocess == False:\n",
    "                self.model.fit(self._splitX(X_train), y_train, batch_size=self.batchsize, epochs=self.epochs,\n",
    "                               verbose=self.verbose,\n",
    "                              validation_data=(self._splitX(X_val), y_val), shuffle=True,\n",
    "                              callbacks=callbacks)\n",
    "            else:\n",
    "                X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.val_size, random_state=1)\n",
    "\n",
    "        else:\n",
    "            self.model.fit(self._splitX(X), y, batch_size=self.batchsize, epochs=self.epochs,\n",
    "               verbose=self.verbose, shuffle=True,)\n",
    "\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        \n",
    "        if self.model:\n",
    "            self.model = load_model(\"embed_NN_\"+str(self.seed)+\".check\")\n",
    "            y_hat = self.model.predict(self._splitX(X))\n",
    "        else:\n",
    "            raise ValueError(\"Model not fit yet\")\n",
    "            \n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 75/72530 [00:00<00:33, 2137.58it/s]\n",
      "  0%|          | 123/72530 [00:00<00:42, 1710.69it/s]\n",
      "  0%|          | 177/72530 [00:00<00:34, 2092.61it/s]\n",
      "  0%|          | 264/72530 [00:00<00:20, 3549.70it/s]\n",
      "\n",
      "  0%|          | 101/72530 [00:00<00:55, 1316.78it/s]\n",
      "  0%|          | 154/72530 [00:00<00:33, 2135.27it/s]\n",
      "  0%|          | 0/72530 [00:00<?, ?it/s]1858.95it/s]\n",
      "  0%|          | 358/72530 [00:00<00:05, 13785.40it/s]\n",
      "  0%|          | 160/72530 [00:00<00:32, 2256.27it/s]\n",
      "  0%|          | 151/72530 [00:00<00:32, 2214.46it/s]\n",
      "  0%|          | 10/72530 [00:00<07:20, 164.81it/s]\n",
      "  0%|          | 180/72530 [00:00<00:28, 2526.26it/s]\n",
      "\n",
      "  0%|          | 185/72530 [00:00<00:20, 3551.21it/s]\n",
      "  0%|          | 127/72530 [00:00<00:17, 4217.62it/s]\n",
      "  0%|          | 223/72530 [00:00<00:02, 35554.41it/s]\n",
      "  0%|          | 176/72530 [00:00<00:20, 3450.36it/s]\n",
      "  0%|          | 207/72530 [00:00<00:15, 4743.31it/s]\n",
      "  1%|          | 485/72530 [00:00<00:06, 10943.41it/s]\n",
      "  1%|▏         | 1076/72530 [00:00<00:04, 16696.22it/s]\n",
      "  0%|          | 143/72530 [00:00<00:29, 2489.97it/s]\n",
      "  0%|          | 83/72530 [00:00<00:18, 4007.73it/s]\n",
      "  0%|          | 251/72530 [00:00<00:07, 9168.64it/s]\n",
      "  0%|          | 56/72529 [00:00<00:12, 5949.82it/s]\n",
      "  0%|          | 68/72529 [00:00<00:11, 6529.00it/s]\n",
      "  0%|          | 74/72529 [00:00<00:10, 6632.02it/s]\n",
      "  0%|          | 134/72529 [00:00<00:05, 13957.75it/s]\n",
      "  0%|          | 234/72529 [00:00<00:03, 23372.72it/s]\n",
      "  1%|          | 431/72529 [00:00<00:02, 27841.87it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/mohsin/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/mohsin/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"<ipython-input-41-239994cbaf14>\", line 126, in applycategory\n    def applycategory(series): return series.progress_apply(norm1grams)\n  File \"/home/mohsin/anaconda3/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 630, in inner\n    result = getattr(df, df_function)(wrapper, *args, **kwargs)\n  File \"/home/mohsin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\", line 2551, in apply\n    mapped = lib.map_infer(values, f, convert=convert_dtype)\n  File \"pandas/_libs/src/inference.pyx\", line 1521, in pandas._libs.lib.map_infer\n  File \"/home/mohsin/anaconda3/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 626, in wrapper\n    return func(*args, **kwargs)\n  File \"<ipython-input-41-239994cbaf14>\", line 122, in norm1grams\n    def norm1grams(s): return _normalize_and_ngrams(s, 1)\n  File \"<ipython-input-41-239994cbaf14>\", line 44, in _normalize_and_ngrams\n    input_list = normalizeString(sent).split()\n  File \"<ipython-input-41-239994cbaf14>\", line 26, in normalizeString\n    s = unicodeToAscii(s.lower().strip())\nAttributeError: 'float' object has no attribute 'lower'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-239994cbaf14>\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(in_path, out_path)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m#ddata = dd.from_pandas(data, 4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'category_words'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallelize_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'category_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapplycategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapplycat1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-239994cbaf14>\u001b[0m in \u001b[0;36mparallelize_dataframe\u001b[0;34m(df, func)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mdf_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_partitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_cores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data, test_data = read_data(\"../input\", \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_150/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_151/Reshape:0' shape=(?, 15) dtype=float32>, <tf.Tensor 'reshape_152/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_153/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_154/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_155/Reshape:0' shape=(?, 15) dtype=float32>, <tf.Tensor 'reshape_156/Reshape:0' shape=(?, 80) dtype=float32>, <tf.Tensor 'reshape_157/Reshape:0' shape=(?, 80) dtype=float32>, <tf.Tensor 'dense_cols_20:0' shape=(?, 1) dtype=float32>]\n",
      "(1407577, 11) (74084, 11) (1407577,) (74084,)\n",
      "Train on 1407577 samples, validate on 74084 samples\n",
      "Epoch 1/15\n",
      "1402880/1407577 [============================>.] - ETA: 0s - loss: 0.9852Epoch 00001: val_loss improved from inf to 0.23463, saving model to embed_NN_5.check\n",
      "1407577/1407577 [==============================] - 18s 13us/step - loss: 0.9828 - val_loss: 0.2346\n",
      "Epoch 2/15\n",
      "1402880/1407577 [============================>.] - ETA: 0s - loss: 0.2369Epoch 00002: val_loss improved from 0.23463 to 0.20054, saving model to embed_NN_5.check\n",
      "1407577/1407577 [==============================] - 13s 9us/step - loss: 0.2369 - val_loss: 0.2005\n",
      "Epoch 3/15\n",
      "1402880/1407577 [============================>.] - ETA: 0s - loss: 0.2058Epoch 00003: val_loss improved from 0.20054 to 0.19444, saving model to embed_NN_5.check\n",
      "1407577/1407577 [==============================] - 13s 9us/step - loss: 0.2058 - val_loss: 0.1944\n",
      "Epoch 4/15\n",
      "1404928/1407577 [============================>.] - ETA: 0s - loss: 0.1921Epoch 00004: val_loss improved from 0.19444 to 0.19010, saving model to embed_NN_5.check\n",
      "1407577/1407577 [==============================] - 13s 9us/step - loss: 0.1921 - val_loss: 0.1901\n",
      "Epoch 5/15\n",
      "1402880/1407577 [============================>.] - ETA: 0s - loss: 0.1836Epoch 00005: val_loss improved from 0.19010 to 0.18791, saving model to embed_NN_5.check\n",
      "1407577/1407577 [==============================] - 13s 9us/step - loss: 0.1836 - val_loss: 0.1879\n",
      "Epoch 6/15\n",
      "1402880/1407577 [============================>.] - ETA: 0s - loss: 0.1781Epoch 00006: val_loss improved from 0.18791 to 0.18685, saving model to embed_NN_5.check\n",
      "1407577/1407577 [==============================] - 13s 9us/step - loss: 0.1781 - val_loss: 0.1868\n",
      "Epoch 7/15\n",
      "1402880/1407577 [============================>.] - ETA: 0s - loss: 0.1735Epoch 00007: val_loss improved from 0.18685 to 0.18597, saving model to embed_NN_5.check\n",
      "1407577/1407577 [==============================] - 13s 9us/step - loss: 0.1735 - val_loss: 0.1860\n",
      "Epoch 8/15\n",
      "1402880/1407577 [============================>.] - ETA: 0s - loss: 0.1699Epoch 00008: val_loss did not improve\n",
      "1407577/1407577 [==============================] - 13s 9us/step - loss: 0.1700 - val_loss: 0.1862\n",
      "Epoch 9/15\n",
      "1402880/1407577 [============================>.] - ETA: 0s - loss: 0.1671Epoch 00009: val_loss improved from 0.18597 to 0.18498, saving model to embed_NN_5.check\n",
      "1407577/1407577 [==============================] - 13s 9us/step - loss: 0.1671 - val_loss: 0.1850\n",
      "Epoch 10/15\n",
      "1402880/1407577 [============================>.] - ETA: 0s - loss: 0.1644Epoch 00010: val_loss improved from 0.18498 to 0.18475, saving model to embed_NN_5.check\n",
      "1407577/1407577 [==============================] - 13s 9us/step - loss: 0.1644 - val_loss: 0.1847\n",
      "Epoch 11/15\n",
      "1402880/1407577 [============================>.] - ETA: 0s - loss: 0.1622Epoch 00011: val_loss improved from 0.18475 to 0.18448, saving model to embed_NN_5.check\n",
      "1407577/1407577 [==============================] - 13s 9us/step - loss: 0.1622 - val_loss: 0.1845\n",
      "Epoch 12/15\n",
      "1402880/1407577 [============================>.] - ETA: 0s - loss: 0.1603Epoch 00012: val_loss did not improve\n",
      "1407577/1407577 [==============================] - 13s 9us/step - loss: 0.1603 - val_loss: 0.1849\n",
      "Epoch 13/15\n",
      "1402880/1407577 [============================>.] - ETA: 0s - loss: 0.1587Epoch 00013: val_loss improved from 0.18448 to 0.18432, saving model to embed_NN_5.check\n",
      "1407577/1407577 [==============================] - 13s 9us/step - loss: 0.1587 - val_loss: 0.1843\n",
      "Epoch 14/15\n",
      "1402880/1407577 [============================>.] - ETA: 0s - loss: 0.1569Epoch 00014: val_loss did not improve\n",
      "1407577/1407577 [==============================] - 13s 9us/step - loss: 0.1569 - val_loss: 0.1845\n",
      "Epoch 15/15\n",
      "1402880/1407577 [============================>.] - ETA: 0s - loss: 0.1556Epoch 00015: val_loss improved from 0.18432 to 0.18410, saving model to embed_NN_5.check\n",
      "1407577/1407577 [==============================] - 13s 9us/step - loss: 0.1556 - val_loss: 0.1841\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EM_NNRegressor(batchsize=2048, dense_cols=['shipping'],\n",
       "        embed_cols=['brand_name', 'category_name', 'item_condition_id', 'cat1', 'cat2', 'cat3'],\n",
       "        embed_dims=[(6000, 30), (1500, 15), (5, 4), (15, 4), (120, 10), (900, 15)],\n",
       "        epochs=15, layer_activations=None, layer_dims=[200],\n",
       "        layer_dropouts=[0.1], multiprocess=False, num_layers=1,\n",
       "        optimizer_kwargs=None, seed=5,\n",
       "        text_embed_cols=['name', 'item_description'],\n",
       "        text_embed_dims=[(50000, 80), (250000, 80)],\n",
       "        text_embed_seq_lens=[10, 10], val_size=0.05, verbose=1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet = EM_NNRegressor(embed_cols=['brand_name','category_name','item_condition_id', 'cat1', 'cat2', 'cat3'], \n",
    "                      embed_dims=[(6000, 30),(1500, 15), (5,4), (15,4), (120, 10), (900, 15)],\n",
    "                      text_embed_cols=['name', 'item_description'],\n",
    "                      text_embed_dims=[(50000, 80), (250000, 80)],\n",
    "                      text_embed_seq_lens =[10, 10], \n",
    "                      #text_embed_cols=['name'],\n",
    "                      #text_embed_dims=[(50000, 80)],\n",
    "                      #text_embed_seq_lens =[10], \n",
    "                      dense_cols=['shipping'],\n",
    "                      epochs=15,\n",
    "                      batchsize=2048,\n",
    "                      num_layers = 1,\n",
    "                      layer_dropouts=[0.1],\n",
    "                      layer_dims=[200],\n",
    "                      val_size=0.05,\n",
    "                      seed=5,\n",
    "                     )\n",
    "\n",
    "nnet.fit(train_data, np.log1p(train_data.price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_preds_1 = nnet.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbf016c36d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import  matplotlib.pyplot as plt\n",
    "sns.distplot(nn_preds_1)\n",
    "sns.distplot(np.log1p(train_data.price))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 80)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "brand_name (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "category_name (InputLayer)      (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_condition_id (InputLayer)  (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat1 (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat2 (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat3 (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_15 (Embedding)        (None, 10, 80)       4000000     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_16 (Embedding)        (None, 80, 80)       8000000     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 1, 30)        180000      brand_name[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 1, 15)        22500       category_name[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 1, 4)         20          item_condition_id[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 1, 4)         60          cat1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 1, 10)        1200        cat2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 1, 15)        13500       cat3[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 80)           0           embedding_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 80)           0           embedding_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, 30)           0           embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)            (None, 15)           0           embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_11 (Reshape)            (None, 4)            0           embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_12 (Reshape)            (None, 4)            0           embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_13 (Reshape)            (None, 10)           0           embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_14 (Reshape)            (None, 15)           0           embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_15 (Reshape)            (None, 80)           0           global_average_pooling1d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "reshape_16 (Reshape)            (None, 80)           0           global_average_pooling1d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_cols (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 239)          0           reshape_9[0][0]                  \n",
      "                                                                 reshape_10[0][0]                 \n",
      "                                                                 reshape_11[0][0]                 \n",
      "                                                                 reshape_12[0][0]                 \n",
      "                                                                 reshape_13[0][0]                 \n",
      "                                                                 reshape_14[0][0]                 \n",
      "                                                                 reshape_15[0][0]                 \n",
      "                                                                 reshape_16[0][0]                 \n",
      "                                                                 dense_cols[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 239)          956         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 239)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 200)          48000       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 200)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 200)          800         leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 200)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            201         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 12,267,237\n",
      "Trainable params: 12,266,359\n",
      "Non-trainable params: 878\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nnet.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_weights = [nnet.model.layers[10].get_weights(), \n",
    "                nnet.model.layers[11].get_weights(),\n",
    "                nnet.model.layers[12].get_weights(),\n",
    "                nnet.model.layers[13].get_weights(),\n",
    "                nnet.model.layers[14].get_weights(),\n",
    "                nnet.model.layers[15].get_weights()]\n",
    "text_embed_weights = [nnet.model.layers[8].get_weights(),\n",
    "                     nnet.model.layers[9].get_weights()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_113/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_114/Reshape:0' shape=(?, 15) dtype=float32>, <tf.Tensor 'reshape_115/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_116/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_117/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_118/Reshape:0' shape=(?, 15) dtype=float32>, <tf.Tensor 'reshape_119/Reshape:0' shape=(?, 80) dtype=float32>, <tf.Tensor 'reshape_120/Reshape:0' shape=(?, 80) dtype=float32>, <tf.Tensor 'dense_cols_15:0' shape=(?, 1) dtype=float32>]\n",
      "(1407575, 11) (74083, 11) (1407575,) (74083,)\n",
      "Train on 1407575 samples, validate on 74083 samples\n",
      "Epoch 1/10\n",
      "1400832/1407575 [============================>.] - ETA: 0s - loss: 1.0512Epoch 00001: val_loss improved from inf to 0.21195, saving model to embed_NN_5.check\n",
      "1407575/1407575 [==============================] - 12s 8us/step - loss: 1.0473 - val_loss: 0.2120\n",
      "Epoch 2/10\n",
      "1406976/1407575 [============================>.] - ETA: 0s - loss: 0.2124Epoch 00002: val_loss improved from 0.21195 to 0.18134, saving model to embed_NN_5.check\n",
      "1407575/1407575 [==============================] - 9s 7us/step - loss: 0.2124 - val_loss: 0.1813\n",
      "Epoch 3/10\n",
      "1402880/1407575 [============================>.] - ETA: 0s - loss: 0.1890Epoch 00003: val_loss improved from 0.18134 to 0.17955, saving model to embed_NN_5.check\n",
      "1407575/1407575 [==============================] - 9s 7us/step - loss: 0.1889 - val_loss: 0.1796\n",
      "Epoch 4/10\n",
      "1404928/1407575 [============================>.] - ETA: 0s - loss: 0.1817Epoch 00004: val_loss improved from 0.17955 to 0.17793, saving model to embed_NN_5.check\n",
      "1407575/1407575 [==============================] - 9s 7us/step - loss: 0.1817 - val_loss: 0.1779\n",
      "Epoch 5/10\n",
      "1398784/1407575 [============================>.] - ETA: 0s - loss: 0.1790Epoch 00005: val_loss did not improve\n",
      "1407575/1407575 [==============================] - 9s 6us/step - loss: 0.1790 - val_loss: 0.1796\n",
      "Epoch 6/10\n",
      "1398784/1407575 [============================>.] - ETA: 0s - loss: 0.1775Epoch 00006: val_loss improved from 0.17793 to 0.17743, saving model to embed_NN_5.check\n",
      "1407575/1407575 [==============================] - 9s 7us/step - loss: 0.1776 - val_loss: 0.1774\n",
      "Epoch 7/10\n",
      "1406976/1407575 [============================>.] - ETA: 0s - loss: 0.1767Epoch 00007: val_loss improved from 0.17743 to 0.17659, saving model to embed_NN_5.check\n",
      "1407575/1407575 [==============================] - 9s 7us/step - loss: 0.1767 - val_loss: 0.1766\n",
      "Epoch 8/10\n",
      "1402880/1407575 [============================>.] - ETA: 0s - loss: 0.1757Epoch 00008: val_loss improved from 0.17659 to 0.17567, saving model to embed_NN_5.check\n",
      "1407575/1407575 [==============================] - 9s 7us/step - loss: 0.1757 - val_loss: 0.1757\n",
      "Epoch 9/10\n",
      "1404928/1407575 [============================>.] - ETA: 0s - loss: 0.1746Epoch 00009: val_loss did not improve\n",
      "1407575/1407575 [==============================] - 9s 7us/step - loss: 0.1746 - val_loss: 0.1780\n",
      "Epoch 10/10\n",
      "1400832/1407575 [============================>.] - ETA: 0s - loss: 0.1736Epoch 00010: val_loss did not improve\n",
      "1407575/1407575 [==============================] - 9s 7us/step - loss: 0.1736 - val_loss: 0.1763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EM_NNRegressor2(batchsize=2048, dense_cols=['shipping'],\n",
       "        embed_cols=['brand_name', 'category_name', 'item_condition_id', 'cat1', 'cat2', 'cat3'],\n",
       "        embed_dims=[(6000, 30), (1500, 15), (5, 4), (15, 4), (120, 10), (900, 15)],\n",
       "        embed_weights=[[array([[-0.03694,  0.00951, ...,  0.04599, -0.02136],\n",
       "       [-0.04944, -0.04953, ..., -0.03893, -0.05821],\n",
       "       ...,\n",
       "       [-0.03562, -0.03545, ...,  0.00207,  0.03777],\n",
       "       [-0.04307, -0.03057, ...,  0.04079, -0.04243]], dtype=float32)], [array([[ 0.0184 , -0.07228, ..., -0....8, ...,  0.04034,  0.03413],\n",
       "       [ 0.04098, -0.03547, ..., -0.00034, -0.04395]], dtype=float32)]],\n",
       "        epochs=10, layer_activations=None, layer_dims=[200, 20],\n",
       "        layer_dropouts=[0.3, 0.1], multiprocess=False, num_layers=1,\n",
       "        optimizer_kwargs=None, seed=5,\n",
       "        text_embed_cols=['name', 'item_description'],\n",
       "        text_embed_dims=[(50000, 80), (100000, 80)],\n",
       "        text_embed_seq_lens=[10, 80],\n",
       "        text_embed_weights=[[array([[ 0.00042, -0.00845, ...,  0.01716, -0.00461],\n",
       "       [-0.07918, -0.01571, ..., -0.0651 ,  0.03538],\n",
       "       ...,\n",
       "       [-0.00782,  0.03121, ...,  0.0015 ,  0.01182],\n",
       "       [-0.00465, -0.02297, ..., -0.00643,  0.01884]], dtype=float32)], [array([[-0.02278, -0.0274 , ...... , ..., -0.04321,  0.04   ],\n",
       "       [-0.04414, -0.00169, ...,  0.0357 ,  0.02693]], dtype=float32)]],\n",
       "        val_size=0.05, verbose=1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet2 = EM_NNRegressor2(embed_cols=['brand_name','category_name','item_condition_id', 'cat1', 'cat2', 'cat3'], \n",
    "                      embed_dims=[(6000, 30),(1500, 15), (5,4), (15,4), (120, 10), (900, 15)],\n",
    "                      embed_weights = embed_weights,\n",
    "                      text_embed_cols=['name', 'item_description'],\n",
    "                      text_embed_dims=[(50000, 80), (100000, 80)],\n",
    "                      text_embed_seq_lens =[10, 80], \n",
    "                      text_embed_weights = text_embed_weights,\n",
    "                      dense_cols=['shipping'],\n",
    "                      epochs=10,\n",
    "                      batchsize=512*4,\n",
    "                      num_layers = 1,\n",
    "                      layer_dropouts=[0.3, 0.1],\n",
    "                      layer_dims=[200, 20],\n",
    "                      val_size=0.05,\n",
    "                      seed=5,\n",
    "                     )\n",
    "\n",
    "nnet2.fit(train_data, np.log1p(train_data.price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
