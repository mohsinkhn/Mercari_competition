{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1\n",
    "* Tokenizer - Keras\n",
    "* Embeddings training from uniform; separate for each one\n",
    "* ** Optimizers and learning rate - test to check local cv vs lb **\n",
    "* Extra features - None for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohsin/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/mohsin/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import threading\n",
    "import multiprocessing\n",
    "\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "\n",
    "# from __future__ import print_function\n",
    "np.random.seed(786)  # for reproducibility\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D, ZeroPadding1D, AveragePooling1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier,  KerasRegressor\n",
    "#Some classes\n",
    "#Functions we need - Feature Selector, Fasttext_Estimator, Preprocessing Transformer, Binary_Encoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "from pandas.api.types import is_numeric_dtype, is_string_dtype\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    print(np.min(y_pred), np.max(y_pred))\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "rmse_sklearn = make_scorer(rmse, greater_is_better=False)\n",
    "\n",
    "def get_obj_cols(df):\n",
    "    \"\"\"Return columns with object dtypes\"\"\"\n",
    "    obj_cols = []\n",
    "    for idx, dt in enumerate(df.dtypes):\n",
    "        if dt == 'object':\n",
    "            obj_cols.append(df.columns.values[idx])\n",
    "\n",
    "    return obj_cols\n",
    "\n",
    "\n",
    "def convert_input(X):\n",
    "    \"\"\"if input not a dataframe convert it to one\"\"\"\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        if isinstance(X, list):\n",
    "            X = pd.DataFrame(np.array(X))\n",
    "        elif isinstance(X, (np.generic, np.ndarray)):\n",
    "            X = pd.DataFrame(X)\n",
    "        elif isinstance(X, csr_matrix):\n",
    "            X = pd.SparseDataFrame(X)\n",
    "        else:\n",
    "            raise ValueError('Unexpected input type: %s' % (str(type(X))))\n",
    "\n",
    "        #X = X.apply(lambda x: pd.to_numeric(x, errors='ignore'))\n",
    "    return X\n",
    "\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Class to do subset of features in sklearn pipeline\"\"\"\n",
    "    def __init__(self, cols=None, return_df=True, verbose=0):\n",
    "        self.cols = cols\n",
    "        self.return_df = return_df\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        #Do nothing\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        #if the input dataset isn't already a dataframe, convert it to one\n",
    "        X = X.copy(deep=True)\n",
    "        X = convert_input(X)\n",
    "        X = X.loc[:, self.col]\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Selecting columns are {}\".format(self.col))\n",
    "        if self.return_df:\n",
    "            return X\n",
    "        else:\n",
    "            return X.values\n",
    "    \n",
    "      \n",
    "#Data reading function\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "#(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.)\n",
    "\n",
    "def read_data(in_path, out_path):\n",
    "    if False and os.path.exists(os.path.join(out_path, 'train_2.pkl')) and os.path.exists(os.path.join(out_path, 'test_2.pkl')):\n",
    "        train_data = pd.read_pickle(os.path.join(out_path, 'train_2.pkl'))\n",
    "        test_data  = pd.read_pickle(os.path.join(out_path, 'test_2.pkl'))\n",
    "        \n",
    "        return train_data, test_data\n",
    "    \n",
    "    else:\n",
    "        train_data = pd.read_table(os.path.join(in_path, 'train.tsv'))\n",
    "        test_data  = pd.read_table(os.path.join(in_path, 'test.tsv'))\n",
    "    \n",
    "        train_rows = len(train_data)\n",
    "        data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "    \n",
    "        data['cat1'] = data['category_name'].apply(lambda x: str(x).split('/')[0])\n",
    "        data['cat2'] = data['category_name'].apply(lambda x: str(x).split('/')[1] \n",
    "                                                   if len(str(x).split('/')) > 1 else -1)\n",
    "        data['cat3'] = data['category_name'].apply(lambda x: ' '.join(str(x).split('/')[2:]) \n",
    "                                                   if len(str(x).split('/')) > 2 else -1)\n",
    "        data.fillna(-1, inplace=True)\n",
    "        cat_cols = ['category_name', 'brand_name', 'cat1', 'cat2', 'cat3', 'item_condition_id']\n",
    "        for col in cat_cols:\n",
    "            data[col] = LabelEncoder().fit_transform(data[col].astype(str))\n",
    "        \n",
    "        #tkn_desc = Tokenizer(50000)\n",
    "        #tkn_desc.fit_on_texts(data.item_description.astype(str))\n",
    "        #data['desc_seq'] = pad_sequences(tkn_desc.texts_to_sequences(data.item_description.astype(str)),\n",
    "        #                                 maxlen=100, padding='post', truncating='post')\n",
    "        \n",
    "        #tkn_name = Tokenizer(4000)\n",
    "        #tkn_name.fit_on_texts(data.name.astype(str))\n",
    "        #data['name_seq'] = pad_sequences(tkn_name.texts_to_sequences(data.name.astype(str)),\n",
    "        #                                 maxlen=6, padding='post', truncating='post')\n",
    "        \n",
    "        \n",
    "        train_data = data.loc[: train_rows - 1, :].reset_index(drop=True)\n",
    "        train_data = train_data.loc[(train_data.price >= 1) & (train_data.price <= 2000), :].reset_index(drop=True)\n",
    "        test_data  = data.loc[train_rows: , :].reset_index(drop=True)\n",
    "        \n",
    "        del train_data['test_id']\n",
    "        del test_data['train_id']\n",
    "        del data \n",
    "        test_data['test_id'] = test_data['test_id'].astype(int)\n",
    "        #train_data.to_pickle(os.path.join(out_path, 'train_2.pkl'))\n",
    "        #test_data.to_pickle(os.path.join(out_path, 'test_2.pkl'))\n",
    "        \n",
    "        return train_data, test_data\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EM_NNRegressor(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, embed_cols=None, dense_cols=None, embed_dims=None, \n",
    "                 text_embed_cols=None, text_embed_seq_lens=None, \n",
    "                 text_embed_dims=None, text_embed_tokenizers=None,\n",
    "                 num_layers=2, multiprocess=False,\n",
    "                layer_activations=None, layer_dims=None,layer_dropouts=None, epochs=20, batchsize=32,\n",
    "                optimizer_kwargs=None, val_size=0.1, verbose=1, seed=1):\n",
    "        \n",
    "        self.embed_cols = embed_cols\n",
    "        self.dense_cols = dense_cols\n",
    "        self.embed_dims = embed_dims\n",
    "        self.text_embed_cols = text_embed_cols\n",
    "        self.text_embed_dims = text_embed_dims\n",
    "        self.text_embed_tokenizers = text_embed_tokenizers\n",
    "        self.text_embed_seq_lens = text_embed_seq_lens\n",
    "        self.dense_dims = None\n",
    "        self.num_layers = num_layers\n",
    "        self.layer_dims = layer_dims\n",
    "        self.layer_activations = layer_activations\n",
    "        self.layer_dropouts = layer_dropouts\n",
    "        self.epochs = epochs\n",
    "        self.batchsize = batchsize\n",
    "        self.optimizer_kwargs = optimizer_kwargs\n",
    "        self.val_size = val_size\n",
    "        self.verbose = verbose\n",
    "        self.multiprocess = multiprocess\n",
    "        self.seed = seed\n",
    "        self.model = None\n",
    "        if self.dense_cols:\n",
    "            self.dense_dims = len(self.dense_cols)\n",
    "            \n",
    "    def _splitX(self, X):\n",
    "        X_splits = []\n",
    "        \n",
    "        if self.embed_cols:\n",
    "            for col in self.embed_cols :\n",
    "                X_splits.append(X[col].values.reshape(X.shape[0], -1))\n",
    "                \n",
    "        if self.text_embed_cols:\n",
    "            for i, (col, tok) in enumerate(zip(self.text_embed_cols, self.text_embed_tokenizers)):\n",
    "                max_features = self.text_embed_dims[i][0]\n",
    "                max_len = self.text_embed_seq_lens[i]\n",
    "                input_text = X[col].astype(str)\n",
    "                x_train = tok.texts_to_sequences(input_text)\n",
    "                print(np.mean([len(l) for l in x_train]))\n",
    "                x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "                X_splits.append(np.array(x_train).reshape(X.shape[0], -1))\n",
    "                \n",
    "        if self.dense_cols:\n",
    "            X_splits.append(X[self.dense_cols].values.reshape(X.shape[0], -1))\n",
    "            \n",
    "        return X_splits\n",
    "    \n",
    "    \n",
    "    def _build_model(self):\n",
    "        model_inputs = []\n",
    "        model_layers = []\n",
    "        \n",
    "        if self.embed_cols:\n",
    "            for col, dim in zip(self.embed_cols, self.embed_dims):\n",
    "                x1 = Input( shape=(1,), name=col)\n",
    "                model_inputs.append(x1)\n",
    "                x1 = Embedding(input_dim=dim[0], output_dim=dim[1], )(x1)\n",
    "                #x1 = Dropout(0.1)(x1)\n",
    "                x1 = Reshape(target_shape=(dim[1],))(x1)\n",
    "                model_layers.append(x1)\n",
    "                \n",
    "        if self.text_embed_cols:\n",
    "            for col, dim, seq_len in zip(self.text_embed_cols, \n",
    "                                                self.text_embed_dims, \n",
    "                                                self.text_embed_seq_lens):\n",
    "                x3 = Input( shape=(seq_len,))\n",
    "                model_inputs.append(x3)\n",
    "                x3 = Embedding(input_dim=dim[0], output_dim=dim[1], input_length=seq_len)(x3)\n",
    "                x3 = GlobalAveragePooling1D()(x3)\n",
    "                x3 = Reshape(target_shape=(dim[1],))(x3)\n",
    "                model_layers.append(x3)\n",
    "                \n",
    "        if self.dense_cols:\n",
    "            x2 = Input( shape=(self.dense_dims, ), name='dense_cols')\n",
    "            model_inputs.append(x2)\n",
    "            model_layers.append(x2)\n",
    "        print(model_layers)\n",
    "        x = concatenate(model_layers)\n",
    "        \n",
    "        if self.num_layers > 0:\n",
    "            for dim, drops in zip(self.layer_dims, self.layer_dropouts):\n",
    "                x = BatchNormalization()(x)\n",
    "                x = Dropout(rate=drops)(x)\n",
    "                x = Dense(dim, kernel_initializer='he_normal')(x)\n",
    "                x = LeakyReLU()(x)\n",
    "        \n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.05)(x)\n",
    "        output = Dense(1, activation='linear', kernel_initializer='normal')(x)\n",
    "        \n",
    "        model = Model(inputs=model_inputs, outputs=output)\n",
    "        #print(model.summary())\n",
    "        adam = Nadam(lr=0.0012, schedule_decay=0.01)\n",
    "        #adam = Adam(lr=0.003, decay=0.003)\n",
    "        #adam = SGD(lr=0.01, nesterov=True, momentum=0.9, decay=0.003)\n",
    "        #adam = RMSprop(lr=0.004, decay=0.004)\n",
    "        model.compile(optimizer=adam, loss='mean_squared_error' )\n",
    "        \n",
    "        return model \n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model = self._build_model()\n",
    "        if self.val_size > 0:\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.val_size, random_state=self.seed)\n",
    "            print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "            \n",
    "            callbacks= [ModelCheckpoint(\"embed_NN_\"+str(self.seed)+\".check\", save_best_only=True, verbose=1)]\n",
    "            if self.multiprocess == False:\n",
    "                self.model.fit(self._splitX(X_train), y_train, batch_size=self.batchsize, epochs=self.epochs,\n",
    "                               verbose=self.verbose,\n",
    "                              validation_data=(self._splitX(X_val), y_val), shuffle=True,\n",
    "                              callbacks=callbacks)\n",
    "            else:\n",
    "                X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.val_size, random_state=1)\n",
    "\n",
    "        else:\n",
    "            self.model.fit(self._splitX(X), y, batch_size=self.batchsize, epochs=self.epochs,\n",
    "               verbose=self.verbose, shuffle=True)\n",
    "\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        \n",
    "        if self.model:\n",
    "            model = load_model(\"embed_NN_\"+str(self.seed)+\".check\")\n",
    "            y_hat = model.predict(self._splitX(X))\n",
    "        else:\n",
    "            raise ValueError(\"Model not fit yet\")\n",
    "            \n",
    "        return y_hat\n",
    "        \n",
    "def add_ngrams(text, ngram=2):\n",
    "    word_list = str(text).lower().split(' ')\n",
    "    out_list = [''.join(word_list[i:i+ngram]) for i in range(len(word_list))]\n",
    "    return ' '.join(out_list[:-1])\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1481658, 11) (693359, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>item_description</th>\n",
       "      <th>name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>train_id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>830</td>\n",
       "      <td>2</td>\n",
       "      <td>No description yet</td>\n",
       "      <td>MLB Cincinnati Reds T Shirt Size XL</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>103</td>\n",
       "      <td>774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3890</td>\n",
       "      <td>87</td>\n",
       "      <td>2</td>\n",
       "      <td>This keyboard is in great condition and works ...</td>\n",
       "      <td>Razer BlackWidow Chroma Keyboard</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4589</td>\n",
       "      <td>1278</td>\n",
       "      <td>0</td>\n",
       "      <td>Adorable top with a hint of lace and a key hol...</td>\n",
       "      <td>AVA-VIV Blouse</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9</td>\n",
       "      <td>104</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>504</td>\n",
       "      <td>0</td>\n",
       "      <td>New with tags. Leather horses. Retail for [rm]...</td>\n",
       "      <td>Leather Horse Statues</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1205</td>\n",
       "      <td>0</td>\n",
       "      <td>Complete with certificate of authenticity</td>\n",
       "      <td>24K GOLD plated rose</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9</td>\n",
       "      <td>59</td>\n",
       "      <td>543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   brand_name  category_name  item_condition_id  \\\n",
       "0           2            830                  2   \n",
       "1        3890             87                  2   \n",
       "2        4589           1278                  0   \n",
       "3           2            504                  0   \n",
       "4           2           1205                  0   \n",
       "\n",
       "                                    item_description  \\\n",
       "0                                 No description yet   \n",
       "1  This keyboard is in great condition and works ...   \n",
       "2  Adorable top with a hint of lace and a key hol...   \n",
       "3  New with tags. Leather horses. Retail for [rm]...   \n",
       "4          Complete with certificate of authenticity   \n",
       "\n",
       "                                  name  price  shipping  train_id  cat1  cat2  \\\n",
       "0  MLB Cincinnati Reds T Shirt Size XL   10.0         1       0.0     5   103   \n",
       "1     Razer BlackWidow Chroma Keyboard   52.0         0       1.0     1    31   \n",
       "2                       AVA-VIV Blouse   10.0         1       2.0     9   104   \n",
       "3                Leather Horse Statues   35.0         1       3.0     3    56   \n",
       "4                 24K GOLD plated rose   44.0         0       4.0     9    59   \n",
       "\n",
       "   cat3  \n",
       "0   774  \n",
       "1   216  \n",
       "2    98  \n",
       "3   411  \n",
       "4   543  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read data\n",
    "train_data, test_data = read_data(\"../input\", \"./\")\n",
    "print(train_data.shape, test_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['item_desc2gram'] = train_data.item_description.apply(lambda x: add_ngrams(x, 2))\n",
    "test_data['item_desc2gram'] = test_data.item_description.apply(lambda x: add_ngrams(x, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1481658, 12) (693359, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>item_description</th>\n",
       "      <th>name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>train_id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>item_desc2gram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>830</td>\n",
       "      <td>2</td>\n",
       "      <td>No description yet</td>\n",
       "      <td>MLB Cincinnati Reds T Shirt Size XL</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>103</td>\n",
       "      <td>774</td>\n",
       "      <td>nodescription descriptionyet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3890</td>\n",
       "      <td>87</td>\n",
       "      <td>2</td>\n",
       "      <td>This keyboard is in great condition and works ...</td>\n",
       "      <td>Razer BlackWidow Chroma Keyboard</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>216</td>\n",
       "      <td>thiskeyboard keyboardis isin ingreat greatcond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4589</td>\n",
       "      <td>1278</td>\n",
       "      <td>0</td>\n",
       "      <td>Adorable top with a hint of lace and a key hol...</td>\n",
       "      <td>AVA-VIV Blouse</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9</td>\n",
       "      <td>104</td>\n",
       "      <td>98</td>\n",
       "      <td>adorabletop topwith witha ahint hintof oflace ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>504</td>\n",
       "      <td>0</td>\n",
       "      <td>New with tags. Leather horses. Retail for [rm]...</td>\n",
       "      <td>Leather Horse Statues</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>411</td>\n",
       "      <td>newwith withtags. tags.leather leatherhorses. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1205</td>\n",
       "      <td>0</td>\n",
       "      <td>Complete with certificate of authenticity</td>\n",
       "      <td>24K GOLD plated rose</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9</td>\n",
       "      <td>59</td>\n",
       "      <td>543</td>\n",
       "      <td>completewith withcertificate certificateof ofa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   brand_name  category_name  item_condition_id  \\\n",
       "0           2            830                  2   \n",
       "1        3890             87                  2   \n",
       "2        4589           1278                  0   \n",
       "3           2            504                  0   \n",
       "4           2           1205                  0   \n",
       "\n",
       "                                    item_description  \\\n",
       "0                                 No description yet   \n",
       "1  This keyboard is in great condition and works ...   \n",
       "2  Adorable top with a hint of lace and a key hol...   \n",
       "3  New with tags. Leather horses. Retail for [rm]...   \n",
       "4          Complete with certificate of authenticity   \n",
       "\n",
       "                                  name  price  shipping  train_id  cat1  cat2  \\\n",
       "0  MLB Cincinnati Reds T Shirt Size XL   10.0         1       0.0     5   103   \n",
       "1     Razer BlackWidow Chroma Keyboard   52.0         0       1.0     1    31   \n",
       "2                       AVA-VIV Blouse   10.0         1       2.0     9   104   \n",
       "3                Leather Horse Statues   35.0         1       3.0     3    56   \n",
       "4                 24K GOLD plated rose   44.0         0       4.0     9    59   \n",
       "\n",
       "   cat3                                     item_desc2gram  \n",
       "0   774                       nodescription descriptionyet  \n",
       "1   216  thiskeyboard keyboardis isin ingreat greatcond...  \n",
       "2    98  adorabletop topwith witha ahint hintof oflace ...  \n",
       "3   411  newwith withtags. tags.leather leatherhorses. ...  \n",
       "4   543  completewith withcertificate certificateof ofa...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_data.shape, test_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing data\")\n",
    "tok_name  = Tokenizer(20000)\n",
    "tok_name.fit_on_texts(pd.concat([train_data, test_data])['name'].astype(str))\n",
    "\n",
    "tok_desc= Tokenizer(50000)\n",
    "tok_desc.fit_on_texts(pd.concat([train_data, test_data])['item_description'].astype(str))\n",
    "\n",
    "tok_desc2 = Tokenizer(20000)\n",
    "tok_desc2.fit_on_texts(pd.concat([train_data, test_data])['item_desc2gram'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data\n",
    "y = np.log1p(train_data.price)\n",
    "\n",
    "cvlist= list(KFold(5, random_state=786).split(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_136/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_137/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_138/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_139/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_140/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_141/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_142/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_143/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_144/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_27:0' shape=(?, 1) dtype=float32>]\n",
      "(1161619, 12) (23707, 12) (1161619,) (23707,)\n",
      "4.334399661162567\n",
      "25.62508447261968\n",
      "19.236752325848666\n",
      "4.335765807567385\n",
      "25.815370987472054\n",
      "19.345889399755347\n",
      "Train on 1161619 samples, validate on 23707 samples\n",
      "Epoch 1/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 1.1020Epoch 00001: val_loss improved from inf to 0.26875, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 17s 15us/step - loss: 1.0989 - val_loss: 0.2687\n",
      "Epoch 2/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.2583Epoch 00002: val_loss improved from 0.26875 to 0.20260, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 9us/step - loss: 0.2582 - val_loss: 0.2026\n",
      "Epoch 3/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.2119Epoch 00003: val_loss improved from 0.20260 to 0.19813, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 9us/step - loss: 0.2119 - val_loss: 0.1981\n",
      "Epoch 4/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1866Epoch 00004: val_loss improved from 0.19813 to 0.18843, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 9us/step - loss: 0.1866 - val_loss: 0.1884\n",
      "Epoch 5/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1707Epoch 00005: val_loss improved from 0.18843 to 0.18729, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 9us/step - loss: 0.1707 - val_loss: 0.1873\n",
      "4.3303051982236145\n",
      "25.599698986272156\n",
      "19.212106691143717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_145/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_146/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_147/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_148/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_149/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_150/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_151/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_152/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_153/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_29:0' shape=(?, 1) dtype=float32>]\n",
      "(1161619, 12) (23707, 12) (1161619,) (23707,)\n",
      "4.332856986671189\n",
      "25.62327837268502\n",
      "19.241469879538815\n",
      "4.3357236259332685\n",
      "25.797232884801957\n",
      "19.36866748217826\n",
      "Train on 1161619 samples, validate on 23707 samples\n",
      "Epoch 1/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 1.0983Epoch 00001: val_loss improved from inf to 0.33470, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 18s 15us/step - loss: 1.0953 - val_loss: 0.3347\n",
      "Epoch 2/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.2590Epoch 00002: val_loss improved from 0.33470 to 0.20482, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.2590 - val_loss: 0.2048\n",
      "Epoch 3/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.2110Epoch 00003: val_loss improved from 0.20482 to 0.19238, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.2110 - val_loss: 0.1924\n",
      "Epoch 4/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1862Epoch 00004: val_loss improved from 0.19238 to 0.18488, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.1862 - val_loss: 0.1849\n",
      "Epoch 5/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1704Epoch 00005: val_loss did not improve\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.1704 - val_loss: 0.1866\n",
      "4.336355844120783\n",
      "25.608229958290025\n",
      "19.19179163910749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  7.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_154/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_155/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_156/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_157/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_158/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_159/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_160/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_161/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_162/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_31:0' shape=(?, 1) dtype=float32>]\n",
      "(1161619, 12) (23707, 12) (1161619,) (23707,)\n",
      "4.332971482043596\n",
      "25.60273635331378\n",
      "19.215828942191887\n",
      "4.327118572573501\n",
      "25.920993799299787\n",
      "19.504070527692242\n",
      "Train on 1161619 samples, validate on 23707 samples\n",
      "Epoch 1/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 1.0977Epoch 00001: val_loss improved from inf to 0.27301, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 18s 16us/step - loss: 1.0946 - val_loss: 0.2730\n",
      "Epoch 2/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.2567Epoch 00002: val_loss improved from 0.27301 to 0.19853, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 9us/step - loss: 0.2566 - val_loss: 0.1985\n",
      "Epoch 3/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.2099Epoch 00003: val_loss improved from 0.19853 to 0.19079, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 9us/step - loss: 0.2099 - val_loss: 0.1908\n",
      "Epoch 4/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1858Epoch 00004: val_loss improved from 0.19079 to 0.18743, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 9us/step - loss: 0.1858 - val_loss: 0.1874\n",
      "Epoch 5/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1697Epoch 00005: val_loss improved from 0.18743 to 0.18380, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 9us/step - loss: 0.1697 - val_loss: 0.1838\n",
      "4.33659544024945\n",
      "25.678853448159497\n",
      "19.28147145768935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 10.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_163/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_164/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_165/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_166/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_167/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_168/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_169/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_170/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_171/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_33:0' shape=(?, 1) dtype=float32>]\n",
      "(1161620, 12) (23707, 12) (1161620,) (23707,)\n",
      "4.334197069609683\n",
      "25.619374666414146\n",
      "19.225807923417296\n",
      "4.338802885223774\n",
      "25.7503269076644\n",
      "19.368751845446493\n",
      "Train on 1161620 samples, validate on 23707 samples\n",
      "Epoch 1/5\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 1.1091Epoch 00001: val_loss improved from inf to 0.27112, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 19s 16us/step - loss: 1.1060 - val_loss: 0.2711\n",
      "Epoch 2/5\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.2596Epoch 00002: val_loss improved from 0.27112 to 0.20044, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 11s 10us/step - loss: 0.2595 - val_loss: 0.2004\n",
      "Epoch 3/5\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.2124Epoch 00003: val_loss improved from 0.20044 to 0.18895, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 11s 10us/step - loss: 0.2124 - val_loss: 0.1889\n",
      "Epoch 4/5\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.1872Epoch 00004: val_loss improved from 0.18895 to 0.18704, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 11s 10us/step - loss: 0.1872 - val_loss: 0.1870\n",
      "Epoch 5/5\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.1708Epoch 00005: val_loss improved from 0.18704 to 0.18468, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 11s 10us/step - loss: 0.1709 - val_loss: 0.1847\n",
      "4.33085637344726\n",
      "25.62728502924095\n",
      "19.25317972132514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 14.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_172/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_173/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_174/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_175/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_176/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_177/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_178/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_179/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_180/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_35:0' shape=(?, 1) dtype=float32>]\n",
      "(1161620, 12) (23707, 12) (1161620,) (23707,)\n",
      "4.333615123706548\n",
      "25.628269141371533\n",
      "19.234100652536974\n",
      "4.329269835913443\n",
      "25.640654658961488\n",
      "19.260935588644703\n",
      "Train on 1161620 samples, validate on 23707 samples\n",
      "Epoch 1/5\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 1.1077Epoch 00001: val_loss improved from inf to 0.26383, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 19s 17us/step - loss: 1.1046 - val_loss: 0.2638\n",
      "Epoch 2/5\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.2558Epoch 00002: val_loss improved from 0.26383 to 0.20433, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 11s 10us/step - loss: 0.2557 - val_loss: 0.2043\n",
      "Epoch 3/5\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.2109Epoch 00003: val_loss improved from 0.20433 to 0.18770, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 11s 10us/step - loss: 0.2109 - val_loss: 0.1877\n",
      "Epoch 4/5\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.1861Epoch 00004: val_loss did not improve\n",
      "1161620/1161620 [==============================] - 11s 9us/step - loss: 0.1861 - val_loss: 0.1883\n",
      "Epoch 5/5\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.1703Epoch 00005: val_loss improved from 0.18770 to 0.18126, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 11s 10us/step - loss: 0.1702 - val_loss: 0.1813\n",
      "4.333900266931236\n",
      "25.601192585318444\n",
      "19.229297643513505\n",
      "0.62817353 8.929645\n",
      "0.42734638381172635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 17.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 17.6min finished\n"
     ]
    }
   ],
   "source": [
    "nnet1 = EM_NNRegressor(embed_cols=['brand_name','category_name','item_condition_id', 'cat1', 'cat2', 'cat3'], \n",
    "                  embed_dims=[(6000, 40),(1500, 30), (5,4), (15,4), (120, 10), (900, 20)],\n",
    "                  text_embed_cols=['name', 'item_description', 'item_desc2gram'],\n",
    "                  text_embed_dims=[(20000, 50), (50000, 50), (20000, 50)],\n",
    "                  text_embed_seq_lens =[7, 60, 30],\n",
    "                  text_embed_tokenizers = [tok_name, tok_desc, tok_desc2],\n",
    "                  dense_cols=['shipping'],\n",
    "                  epochs=5,\n",
    "                  batchsize=2048 ,\n",
    "                  num_layers = 1,\n",
    "                  layer_dropouts=[0.2],\n",
    "                  layer_dims=[200],\n",
    "                  seed=1,\n",
    "                  val_size=0.02,\n",
    "                 )\n",
    "\n",
    "oof_preds1 = cross_val_predict(nnet1, X, y, verbose=10, cv=cvlist)\n",
    "score = rmse(y, oof_preds1)\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_181/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_182/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_183/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_184/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_185/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_186/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_187/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_188/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_189/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_37:0' shape=(?, 1) dtype=float32>]\n",
      "(1161619, 12) (23707, 12) (1161619,) (23707,)\n",
      "4.334399661162567\n",
      "25.62508447261968\n",
      "19.236752325848666\n",
      "4.335765807567385\n",
      "25.815370987472054\n",
      "19.345889399755347\n",
      "Train on 1161619 samples, validate on 23707 samples\n",
      "Epoch 1/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 1.0930Epoch 00001: val_loss improved from inf to 0.29811, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 20s 17us/step - loss: 1.0900 - val_loss: 0.2981\n",
      "Epoch 2/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.2590Epoch 00002: val_loss improved from 0.29811 to 0.20670, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.2589 - val_loss: 0.2067\n",
      "Epoch 3/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.2122Epoch 00003: val_loss improved from 0.20670 to 0.18914, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.2122 - val_loss: 0.1891\n",
      "Epoch 4/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1871Epoch 00004: val_loss improved from 0.18914 to 0.18645, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.1871 - val_loss: 0.1864\n",
      "Epoch 5/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1711Epoch 00005: val_loss did not improve\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.1711 - val_loss: 0.1885\n",
      "4.3303051982236145\n",
      "25.599698986272156\n",
      "19.212106691143717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_190/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_191/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_192/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_193/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_194/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_195/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_196/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_197/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_198/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_39:0' shape=(?, 1) dtype=float32>]\n",
      "(1161619, 12) (23707, 12) (1161619,) (23707,)\n",
      "4.332856986671189\n",
      "25.62327837268502\n",
      "19.241469879538815\n",
      "4.3357236259332685\n",
      "25.797232884801957\n",
      "19.36866748217826\n",
      "Train on 1161619 samples, validate on 23707 samples\n",
      "Epoch 1/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 1.0943Epoch 00001: val_loss improved from inf to 0.27835, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 20s 18us/step - loss: 1.0912 - val_loss: 0.2784\n",
      "Epoch 2/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.2584Epoch 00002: val_loss improved from 0.27835 to 0.20105, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.2584 - val_loss: 0.2010\n",
      "Epoch 3/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.2113Epoch 00003: val_loss improved from 0.20105 to 0.18883, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 9us/step - loss: 0.2112 - val_loss: 0.1888\n",
      "Epoch 4/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1869Epoch 00004: val_loss improved from 0.18883 to 0.18810, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 9us/step - loss: 0.1869 - val_loss: 0.1881\n",
      "Epoch 5/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1706Epoch 00005: val_loss improved from 0.18810 to 0.18398, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.1706 - val_loss: 0.1840\n",
      "4.336355844120783\n",
      "25.608229958290025\n",
      "19.19179163910749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  7.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_199/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_200/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_201/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_202/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_203/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_204/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_205/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_206/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_207/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_41:0' shape=(?, 1) dtype=float32>]\n",
      "(1161619, 12) (23707, 12) (1161619,) (23707,)\n",
      "4.332971482043596\n",
      "25.60273635331378\n",
      "19.215828942191887\n",
      "4.327118572573501\n",
      "25.920993799299787\n",
      "19.504070527692242\n",
      "Train on 1161619 samples, validate on 23707 samples\n",
      "Epoch 1/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 1.1098Epoch 00001: val_loss improved from inf to 0.29727, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 21s 18us/step - loss: 1.1067 - val_loss: 0.2973\n",
      "Epoch 2/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.2611Epoch 00002: val_loss improved from 0.29727 to 0.20724, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.2611 - val_loss: 0.2072\n",
      "Epoch 3/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.2127Epoch 00003: val_loss improved from 0.20724 to 0.18975, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.2127 - val_loss: 0.1898\n",
      "Epoch 4/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1872Epoch 00004: val_loss improved from 0.18975 to 0.18465, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.1873 - val_loss: 0.1847\n",
      "Epoch 5/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1714Epoch 00005: val_loss improved from 0.18465 to 0.18317, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.1714 - val_loss: 0.1832\n",
      "4.33659544024945\n",
      "25.678853448159497\n",
      "19.28147145768935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 10.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_208/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_209/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_210/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_211/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_212/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_213/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_214/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_215/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_216/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_43:0' shape=(?, 1) dtype=float32>]\n",
      "(1161620, 12) (23707, 12) (1161620,) (23707,)\n",
      "4.334197069609683\n",
      "25.619374666414146\n",
      "19.225807923417296\n",
      "4.338802885223774\n",
      "25.7503269076644\n",
      "19.368751845446493\n",
      "Train on 1161620 samples, validate on 23707 samples\n",
      "Epoch 1/5\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 1.1085Epoch 00001: val_loss improved from inf to 0.28398, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 21s 18us/step - loss: 1.1054 - val_loss: 0.2840\n",
      "Epoch 2/5\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.2565Epoch 00002: val_loss improved from 0.28398 to 0.20193, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 11s 10us/step - loss: 0.2564 - val_loss: 0.2019\n",
      "Epoch 3/5\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.2102Epoch 00003: val_loss improved from 0.20193 to 0.19049, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 11s 10us/step - loss: 0.2102 - val_loss: 0.1905\n",
      "Epoch 4/5\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.1858Epoch 00004: val_loss improved from 0.19049 to 0.19020, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 11s 10us/step - loss: 0.1858 - val_loss: 0.1902\n",
      "Epoch 5/5\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.1699Epoch 00005: val_loss improved from 0.19020 to 0.18737, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 11s 10us/step - loss: 0.1699 - val_loss: 0.1874\n",
      "4.33085637344726\n",
      "25.62728502924095\n",
      "19.25317972132514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 14.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_217/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_218/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_219/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_220/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_221/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_222/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_223/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_224/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_225/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_45:0' shape=(?, 1) dtype=float32>]\n",
      "(1161620, 12) (23707, 12) (1161620,) (23707,)\n",
      "4.333615123706548\n",
      "25.628269141371533\n",
      "19.234100652536974\n",
      "4.329269835913443\n",
      "25.640654658961488\n",
      "19.260935588644703\n",
      "Train on 1161620 samples, validate on 23707 samples\n",
      "Epoch 1/5\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 1.0911Epoch 00001: val_loss improved from inf to 0.25750, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 22s 19us/step - loss: 1.0881 - val_loss: 0.2575\n",
      "Epoch 2/5\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.2559Epoch 00002: val_loss improved from 0.25750 to 0.19802, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 11s 10us/step - loss: 0.2559 - val_loss: 0.1980\n",
      "Epoch 3/5\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.2103Epoch 00003: val_loss improved from 0.19802 to 0.19791, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 11s 10us/step - loss: 0.2103 - val_loss: 0.1979\n",
      "Epoch 4/5\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.1859Epoch 00004: val_loss improved from 0.19791 to 0.18297, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 11s 10us/step - loss: 0.1859 - val_loss: 0.1830\n",
      "Epoch 5/5\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.1699Epoch 00005: val_loss did not improve\n",
      "1161620/1161620 [==============================] - 11s 10us/step - loss: 0.1699 - val_loss: 0.1840\n",
      "4.333900266931236\n",
      "25.601192585318444\n",
      "19.229297643513505\n",
      "0.6468892 8.82268\n",
      "0.4285122119043365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 18.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 18.0min finished\n"
     ]
    }
   ],
   "source": [
    "nnet2 = EM_NNRegressor(embed_cols=['brand_name','category_name','item_condition_id', 'cat1', 'cat2', 'cat3'], \n",
    "                  embed_dims=[(6000, 30),(1500, 25), (5,4), (15,4), (120, 10), (900, 20)],\n",
    "                  text_embed_cols=['name', 'item_description', 'item_desc2gram'],\n",
    "                  text_embed_dims=[(20000, 30), (50000, 30), (20000, 30)],\n",
    "                  text_embed_seq_lens =[7, 70, 30],\n",
    "                  text_embed_tokenizers = [tok_name, tok_desc, tok_desc2],\n",
    "                  dense_cols=['shipping'],\n",
    "                  epochs=4,\n",
    "                  batchsize=2048 ,\n",
    "                  num_layers = 1,\n",
    "                  layer_dropouts=[0.2],\n",
    "                  layer_dims=[100],\n",
    "                  seed=2,\n",
    "                  val_size=0.02\n",
    "                 )\n",
    "oof_preds2 = cross_val_predict(nnet1, X, y, verbose=10, cv=cvlist)\n",
    "score = rmse(y, oof_preds2)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_226/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_227/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_228/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_229/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_230/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_231/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_232/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_233/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_234/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_47:0' shape=(?, 1) dtype=float32>]\n",
      "(1161619, 12) (23707, 12) (1161619,) (23707,)\n",
      "4.334399661162567\n",
      "25.62508447261968\n",
      "19.236752325848666\n",
      "4.335765807567385\n",
      "25.815370987472054\n",
      "19.345889399755347\n",
      "Train on 1161619 samples, validate on 23707 samples\n",
      "Epoch 1/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 1.1022Epoch 00001: val_loss improved from inf to 0.27612, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 22s 19us/step - loss: 1.0991 - val_loss: 0.2761\n",
      "Epoch 2/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.2576Epoch 00002: val_loss improved from 0.27612 to 0.19724, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.2575 - val_loss: 0.1972\n",
      "Epoch 3/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.2111Epoch 00003: val_loss improved from 0.19724 to 0.18764, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.2111 - val_loss: 0.1876\n",
      "Epoch 4/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1868Epoch 00004: val_loss did not improve\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.1868 - val_loss: 0.1877\n",
      "Epoch 5/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1705Epoch 00005: val_loss did not improve\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.1705 - val_loss: 0.1882\n",
      "4.3303051982236145\n",
      "25.599698986272156\n",
      "19.212106691143717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_235/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_236/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_237/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_238/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_239/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_240/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_241/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_242/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_243/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_49:0' shape=(?, 1) dtype=float32>]\n",
      "(1161619, 12) (23707, 12) (1161619,) (23707,)\n",
      "4.332856986671189\n",
      "25.62327837268502\n",
      "19.241469879538815\n",
      "4.3357236259332685\n",
      "25.797232884801957\n",
      "19.36866748217826\n",
      "Train on 1161619 samples, validate on 23707 samples\n",
      "Epoch 1/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 1.1063Epoch 00001: val_loss improved from inf to 0.29548, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 23s 20us/step - loss: 1.1032 - val_loss: 0.2955\n",
      "Epoch 2/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.2614Epoch 00002: val_loss improved from 0.29548 to 0.19578, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.2613 - val_loss: 0.1958\n",
      "Epoch 3/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.2142Epoch 00003: val_loss improved from 0.19578 to 0.18954, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.2142 - val_loss: 0.1895\n",
      "Epoch 4/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1885Epoch 00004: val_loss improved from 0.18954 to 0.18628, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.1884 - val_loss: 0.1863\n",
      "Epoch 5/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1721Epoch 00005: val_loss did not improve\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.1721 - val_loss: 0.1877\n",
      "4.336355844120783\n",
      "25.608229958290025\n",
      "19.19179163910749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  7.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_244/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_245/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_246/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_247/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_248/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_249/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_250/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_251/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_252/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_51:0' shape=(?, 1) dtype=float32>]\n",
      "(1161619, 12) (23707, 12) (1161619,) (23707,)\n",
      "4.332971482043596\n",
      "25.60273635331378\n",
      "19.215828942191887\n",
      "4.327118572573501\n",
      "25.920993799299787\n",
      "19.504070527692242\n",
      "Train on 1161619 samples, validate on 23707 samples\n",
      "Epoch 1/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 1.1046Epoch 00001: val_loss improved from inf to 0.28939, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 23s 20us/step - loss: 1.1015 - val_loss: 0.2894\n",
      "Epoch 2/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.2582Epoch 00002: val_loss improved from 0.28939 to 0.20343, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.2581 - val_loss: 0.2034\n",
      "Epoch 3/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.2112Epoch 00003: val_loss improved from 0.20343 to 0.18747, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.2112 - val_loss: 0.1875\n",
      "Epoch 4/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1864Epoch 00004: val_loss improved from 0.18747 to 0.18626, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.1865 - val_loss: 0.1863\n",
      "Epoch 5/5\n",
      "1157120/1161619 [============================>.] - ETA: 0s - loss: 0.1705Epoch 00005: val_loss improved from 0.18626 to 0.18368, saving model to embed_NN_1.check\n",
      "1161619/1161619 [==============================] - 11s 10us/step - loss: 0.1705 - val_loss: 0.1837\n",
      "4.33659544024945\n",
      "25.678853448159497\n",
      "19.28147145768935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 11.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_253/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_254/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_255/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_256/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_257/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_258/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_259/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_260/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_261/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_53:0' shape=(?, 1) dtype=float32>]\n",
      "(1161620, 12) (23707, 12) (1161620,) (23707,)\n",
      "4.334197069609683\n",
      "25.619374666414146\n",
      "19.225807923417296\n",
      "4.338802885223774\n",
      "25.7503269076644\n",
      "19.368751845446493\n",
      "Train on 1161620 samples, validate on 23707 samples\n",
      "Epoch 1/5\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 1.1072Epoch 00001: val_loss improved from inf to 0.35123, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 24s 21us/step - loss: 1.1041 - val_loss: 0.3512\n",
      "Epoch 2/5\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.2580Epoch 00002: val_loss improved from 0.35123 to 0.19724, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 11s 10us/step - loss: 0.2579 - val_loss: 0.1972\n",
      "Epoch 3/5\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.2108Epoch 00003: val_loss improved from 0.19724 to 0.18783, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 11s 10us/step - loss: 0.2108 - val_loss: 0.1878\n",
      "Epoch 4/5\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.1861Epoch 00004: val_loss did not improve\n",
      "1161620/1161620 [==============================] - 11s 10us/step - loss: 0.1860 - val_loss: 0.1893\n",
      "Epoch 5/5\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.1701Epoch 00005: val_loss improved from 0.18783 to 0.18334, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 11s 10us/step - loss: 0.1701 - val_loss: 0.1833\n",
      "4.33085637344726\n",
      "25.62728502924095\n",
      "19.25317972132514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 14.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_262/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_263/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_264/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_265/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_266/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_267/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_268/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_269/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_270/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_55:0' shape=(?, 1) dtype=float32>]\n",
      "(1161620, 12) (23707, 12) (1161620,) (23707,)\n",
      "4.333615123706548\n",
      "25.628269141371533\n",
      "19.234100652536974\n",
      "4.329269835913443\n",
      "25.640654658961488\n",
      "19.260935588644703\n",
      "Train on 1161620 samples, validate on 23707 samples\n",
      "Epoch 1/5\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 1.1026Epoch 00001: val_loss improved from inf to 0.34187, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 24s 21us/step - loss: 1.0994 - val_loss: 0.3419\n",
      "Epoch 2/5\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.2586Epoch 00002: val_loss improved from 0.34187 to 0.20626, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 11s 10us/step - loss: 0.2585 - val_loss: 0.2063\n",
      "Epoch 3/5\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.2116Epoch 00003: val_loss improved from 0.20626 to 0.19346, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 11s 10us/step - loss: 0.2115 - val_loss: 0.1935\n",
      "Epoch 4/5\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.1863Epoch 00004: val_loss improved from 0.19346 to 0.18729, saving model to embed_NN_1.check\n",
      "1161620/1161620 [==============================] - 11s 10us/step - loss: 0.1863 - val_loss: 0.1873\n",
      "Epoch 5/5\n",
      "1157120/1161620 [============================>.] - ETA: 0s - loss: 0.1699Epoch 00005: val_loss did not improve\n",
      "1161620/1161620 [==============================] - 11s 10us/step - loss: 0.1700 - val_loss: 0.1927\n",
      "4.333900266931236\n",
      "25.601192585318444\n",
      "19.229297643513505\n",
      "0.63433325 8.578509\n",
      "0.42917971016691675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 18.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 18.7min finished\n"
     ]
    }
   ],
   "source": [
    "nnet3 = EM_NNRegressor(embed_cols=['brand_name','category_name','item_condition_id', 'cat1', 'cat2', 'cat3'], \n",
    "                  embed_dims=[(6000, 30),(1500, 20), (5,4), (15,4), (120, 10), (900, 20)],\n",
    "                  text_embed_cols=['name', 'item_description', 'item_desc2gram'],\n",
    "                  text_embed_dims=[(20000, 50), (50000, 50), (20000, 50)],\n",
    "                  text_embed_seq_lens =[7, 60, 30],\n",
    "                  text_embed_tokenizers = [tok_name, tok_desc, tok_desc2],\n",
    "                  dense_cols=['shipping'],\n",
    "                  epochs=4,\n",
    "                  batchsize=2048 ,\n",
    "                  num_layers = 1,\n",
    "                  layer_dropouts=[0.2],\n",
    "                  layer_dims=[200],\n",
    "                  seed=3,\n",
    "                  val_size=0.02,\n",
    "                 )\n",
    "oof_preds3 = cross_val_predict(nnet1, X, y, verbose=10, cv=cvlist)\n",
    "score = rmse(y, oof_preds3)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import hmean, gmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1481658,)\n",
      "0.72281456 8.776944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.41751682030956144"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_preds = np.mean(np.hstack((oof_preds1, oof_preds2, oof_preds3)), axis=1)\n",
    "print(oof_preds.shape)\n",
    "rmse(y, oof_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With Nadam lr=0.015, decay=0.005 - mean of 3 models is 0.4185 (0.429, 0.429, 0.428)\n",
    "#RMSProp lr=0.003, decay=0.005 - mean 0.420 (0.425, 0.426, 0.424)\n",
    "#RMSProp lr=0.01, decay=0.005 - mean 0.425 (0.434, 0.431, 0.430)\n",
    "#RMSProp lr=0.004, decay=0.004 - mean 0.421 (0.428, 0.429, 0.427) \n",
    "#Adam lr=0.003, decay=0.003 - mean -- (0.4)\n",
    "#With Nadam lr=0.012, decay=0.01 - mean of 3 models is 0.4175 (0.427, 0.428, 0.429)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet1.fit(train_data, np.log1p(train_data.price) )\n",
    "print(\"Predicting on test data\")\n",
    "test_preds1 = nnet1.predict(test_data)\n",
    "\n",
    "nnet2.fit(train_data, np.log1p(train_data.price) )\n",
    "print(\"Predicting on test data\")\n",
    "test_preds2 = nnet2.predict(test_data)\n",
    "\n",
    "nnet3.fit(train_data, np.log1p(train_data.price) )\n",
    "print(\"Predicting on test data\")\n",
    "test_preds3 = nnet3.predict(test_data)\n",
    "\n",
    "test_preds = (1/3)*(test_preds1 + test_preds2 + test_preds3)\n",
    "print(\"Write out submission\")\n",
    "submission: pd.DataFrame = test_data[['test_id']]\n",
    "submission['price'] = np.expm1(test_preds)\n",
    "submission.price = submission.price.clip(3, 2000)\n",
    "submission.to_csv(\"embedding_nn_v2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
