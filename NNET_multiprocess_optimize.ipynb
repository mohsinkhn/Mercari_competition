{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO\n",
    "\n",
    "- multiple runs on different processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohsin/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "#os.environ[\"THEANO_FLAGS\"] = \"floatX=float32,device=cpu\"\n",
    "import copy\n",
    "import numpy as np\n",
    "np.random.seed(786)\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype, is_string_dtype\n",
    "import sys\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "import math\n",
    "import gc\n",
    "import time\n",
    "from time import time\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(tqdm)\n",
    "\n",
    "from fastcache import clru_cache as lru_cache\n",
    "import threading\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool, cpu_count\n",
    "num_partitions = 8\n",
    "num_cores = 4\n",
    "\n",
    "import wordbatch\n",
    "from  wordbatch.extractors import WordSeq, WordBag, WordHash\n",
    "from wordbatch.models import FTRL, FM_FTRL, NN_ReLU_H1, NN_ReLU_H2\n",
    "\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder, RobustScaler, MaxAbsScaler, QuantileTransformer, OneHotEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from scipy.sparse import hstack\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Reshape, concatenate, Dense, GlobalAveragePooling1D, BatchNormalization, Flatten, Dropout, PReLU, LeakyReLU, Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.optimizers import Adam, RMSprop, Nadam\n",
    "import keras.backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tensorflow'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "rmse_sklearn = make_scorer(rmse, greater_is_better=False)\n",
    "from keras import backend as K\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n",
    "\n",
    "stop_words = ['a', 'an', 'this', 'is', 'the', 'of', 'for']\n",
    "\n",
    "def unicodeToAscii(series):\n",
    "    return  series.apply(lambda s: unicodedata.normalize('NFKC', str(s)))\n",
    "\n",
    "def multiple_replace(text, adict):\n",
    "    rx = re.compile('|'.join(map(re.escape, adict)))\n",
    "    def one_xlat(match):\n",
    "        return adict[match.group(0)]\n",
    "    return rx.sub(one_xlat, text)\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(series):\n",
    "    series = unicodeToAscii(series)\n",
    "\n",
    "    replace_dict = {r\" one \": \" 1 \",\n",
    "                    r\" two \": r\" 2 \",\n",
    "                    r\" three \": \" 3 \",\n",
    "                    r\" four \" : \" 4 \", \n",
    "                    r\" five \" : \" 5 \",\n",
    "                    r\" six \"  : \" 6 \", \n",
    "                    r\" seven \": \" 7 \",   \n",
    "                    r\"lily jade\": \"lilyjade\", \n",
    "                    r\"rae dunn cookies\": \"raedunncookie\",\n",
    "                    r\"rae dunn cookie\": \"raedunncookie\",\n",
    "                    r\"hatchimals\" : \"hatchimal\", \n",
    "                    r\"virtual reality\" : \"vr\", \n",
    "                    r\" vs \" : \" victorias secret \",\n",
    "                    r\" mk \" : \" michael kors \", \n",
    "                    r\" victoria secret \" : \" victorias secret \",\n",
    "                    r\"google home\" : \"googlehome\", \n",
    "                    r\"16 gb\" : \"16gb \", \n",
    "                    r\"256 gb\" : \"256gb \",\n",
    "                    r\"32 gb\" : \"32gb \", \n",
    "                    r\"14k gold\" : '14kgold', \n",
    "                    r\"14 gold\" : '14kgold',\n",
    "                    r\"14 k gold\" : '14kgold',\n",
    "                    r\"lululemon bags\" : 'lululemonbags',\n",
    "                    r\"controller skin\" : 'controllerskin',\n",
    "                    r\"watch box\" : 'watchbox',\n",
    "                    r\"blaze band\" : 'blazeband', \n",
    "                    r\"vault boy\" : 'vaultboy',\n",
    "                    r\"lash boost\" : 'lashboost', \n",
    "                    r\"64 g \" : '64gb ',\n",
    "                    r\"32 g \" : '32gb',\n",
    "                    r\"go pro hero\" : 'goprohero', \n",
    "                    r\"gopro hero\" : 'goprohero', \n",
    "                    r\"nmd r \" : 'nmdr ', \n",
    "                    r\"nmd r1 \" : 'nmdr ', \n",
    "                    r\"nmds r \" : 'nmdr ',  \n",
    "                    r\"private sale\" : 'privatesale', \n",
    "                    r\"vutton\" : 'vuitton',\n",
    "                    r\"louis vuitton eva\" : 'louisvuittoneva',\n",
    "                    r\"apple watch\" : 'applewatch'}\n",
    "    rx = re.compile('|'.join(map(re.escape, replace_dict)))\n",
    "    def one_xlat(match):\n",
    "        return replace_dict[match.group(0)]\n",
    "    \n",
    "    series = series.str.lower()\n",
    "    series = series.str.replace(r\"'\", \"\")\n",
    "    series = series.str.replace(r\"-\", \"\")\n",
    "    series = series.str.replace(r\"[^0-9a-zA-Z]+\", \" \")\n",
    "    series = series.str.replace(r\"(?=\\w{1,2})iphone \", \"iphone\")\n",
    "    series = series.str.replace(r\"(?=\\w{1,2})galaxy \", \"galaxy\")\n",
    "    series = series.str.replace(rx, one_xlat)\n",
    "\n",
    "    return series\n",
    "\n",
    "\n",
    "def _normalize_and_ngrams(sent, ngram):\n",
    "    input_list = sent.split()\n",
    "    input_list = [word for word in input_list if word not in stop_words]\n",
    "    #s = input_list.copy()\n",
    "    #for i in range(2, ngrams+1):\n",
    "    #    s += [' '.join(input_list[j:j+i]) for j in range(len(input_list)-i + 1)]\n",
    "        #s += list((zip(*[input_list[j:] for j in range(i)])))\n",
    "    s = [''.join(input_list[i:i+ngram]) for i in range(len(input_list))]\n",
    "    return ' '.join(s[:-1])\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "@lru_cache(1024)\n",
    "def stem(s):\n",
    "    return stemmer.stem(s)\n",
    "\n",
    "whitespace = re.compile(r'\\s+')\n",
    "non_letter = re.compile(r'\\W+')\n",
    "\n",
    "def tokenize(text):\n",
    "    #text = normalizeString(str(text))\n",
    "    text = non_letter.sub(' ', text)\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for t in text.split():\n",
    "        #t = stem(t)\n",
    "        tokens.append(t)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, min_df=10, tokenizer=str.split):\n",
    "        self.min_df = min_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.doc_freq = None\n",
    "        self.vocab = None\n",
    "        self.vocab_idx = None\n",
    "        self.max_len = None\n",
    "\n",
    "    def fit_transform(self, texts):\n",
    "        tokenized = []\n",
    "        doc_freq = Counter()\n",
    "        n = len(texts)\n",
    "\n",
    "        for text in texts:\n",
    "            sentence = self.tokenizer(text)\n",
    "            tokenized.append(sentence)\n",
    "            doc_freq.update(set(sentence))\n",
    "\n",
    "        vocab = sorted([t for (t, c) in doc_freq.items() if c >= self.min_df])\n",
    "        vocab_idx = {t: (i + 1) for (i, t) in enumerate(vocab)}\n",
    "        doc_freq = [doc_freq[t] for t in vocab]\n",
    "\n",
    "        self.doc_freq = doc_freq\n",
    "        self.vocab = vocab\n",
    "        self.vocab_idx = vocab_idx\n",
    "\n",
    "        max_len = 0\n",
    "        result_list = []\n",
    "        for text in tokenized:\n",
    "            text = self.text_to_idx(text)\n",
    "            max_len = max(max_len, len(text))\n",
    "            result_list.append(text)\n",
    "\n",
    "        self.max_len = max_len\n",
    "        result = np.zeros(shape=(n, max_len), dtype=np.int32)\n",
    "        for i in range(n):\n",
    "            text = result_list[i]\n",
    "            result[i, :len(text)] = text\n",
    "\n",
    "        return result    \n",
    "\n",
    "    def text_to_idx(self, tokenized):\n",
    "        return [self.vocab_idx[t] for t in tokenized if t in self.vocab_idx]\n",
    "\n",
    "    def transform(self, texts):\n",
    "        n = len(texts)\n",
    "        result = np.zeros(shape=(n, self.max_len), dtype=np.int32)\n",
    "\n",
    "        for i in range(n):\n",
    "            text = self.tokenizer(texts[i])\n",
    "            text = self.text_to_idx(text)[:self.max_len]\n",
    "            result[i, :len(text)] = text\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def vocabulary_size(self):\n",
    "        return len(self.vocab) + 1\n",
    "\n",
    "\n",
    "def get_cat_1(x): return str(x).split('/')[0]\n",
    "def get_cat_2(x): return str(x).split('/')[1] if len(str(x).split('/')) > 1 else -1\n",
    "def get_cat_3(x): return ' '.join(str(x).split('/')[2:]) if len(str(x).split('/')) > 2 else -1\n",
    "\n",
    "def applycat1(df): \n",
    "    return df['category_name'].progress_apply(get_cat_1)\n",
    "    \n",
    "\n",
    "def applycat2(df): \n",
    "    return df['category_name'].progress_apply(get_cat_2)\n",
    "    \n",
    "\n",
    "def applycat3(df): \n",
    "    return df['category_name'].progress_apply(get_cat_3)\n",
    "\n",
    "def get_words(series): return series.progress_apply(lambda x: len(str(x).split()))\n",
    "\n",
    "def get_chars(series): return series.progress_apply(lambda x: len(str(x)))\n",
    "\n",
    "def get_tokens(series): return np.sum(np.array(series.tolist()) > 0, axis=1)\n",
    "\n",
    "def isphonecase(series): return series.str.contains(' case ', flags=re.IGNORECASE).astype(int)\n",
    "\n",
    "def isiphone6(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('6|six', flags=re.IGNORECASE) &\n",
    "                        ~(series.str.contains('plus|\\+', flags=re.IGNORECASE)) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) ).astype(int)\n",
    "\n",
    "def isiphone6p(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('6|six', flags=re.IGNORECASE) &\n",
    "                        series.str.contains('plus|\\+', flags=re.IGNORECASE) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) ).astype(int)\n",
    "\n",
    "def isiphone5(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('5|five', flags=re.IGNORECASE) &\n",
    "                        ~(series.str.contains('plus|\\+', flags=re.IGNORECASE)) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) ).astype(int)\n",
    "\n",
    "def isiphone5p(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('5|five', flags=re.IGNORECASE) &\n",
    "                        series.str.contains('plus|\\+', flags=re.IGNORECASE) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) ).astype(int)\n",
    "\n",
    "def isiphone7(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('7|seven', flags=re.IGNORECASE) &\n",
    "                        ~(series.str.contains('plus|\\+', flags=re.IGNORECASE)) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) ).astype(int)\n",
    "\n",
    "def isiphone7p(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('7|seven', flags=re.IGNORECASE) &\n",
    "                        series.str.contains('plus|\\+', flags=re.IGNORECASE) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) ).astype(int)\n",
    "\n",
    "def isunlocked(series): return series.str.contains('unlocked', flags=re.IGNORECASE).astype(int)\n",
    "\n",
    "def plussigns(series): return series.apply(lambda x: sum([(s == '+') | (s == '➕') for s in str(x)]))\n",
    "\n",
    "def andsigns(series): return series.apply(lambda x: sum([(s == '&') | (s == ' and ') for s in str(x)]))\n",
    "\n",
    "def commas(series): return series.apply(lambda x: sum([s == ',' for s in str(x)]))\n",
    "\n",
    "def add_ngrams(text, ngram=2):\n",
    "    word_list = normalizeString(text).split(' ')\n",
    "    out_list = [''.join(word_list[i:i+ngram]) for i in range(len(word_list))]\n",
    "    return ' '.join(out_list[:-1])\n",
    "\n",
    "def get_2grams(series): return series.apply(lambda x: _normalize_and_ngrams(str(x), 2))\n",
    "\n",
    "def norm3grams(s): return _normalize_and_ngrams(s, 3)\n",
    "\n",
    "def applyname(series): return series.progress_apply(norm3grams)\n",
    "\n",
    "def index2sent1(x, name_vocab): return indexesFromSentence(name_vocab, x, 3, 10)\n",
    "\n",
    "def name2index(series): return series.progress_apply(lambda x: index2sent1(x, name_vocab))\n",
    "\n",
    "def norm2grams(s): return _normalize_and_ngrams(s, 1)\n",
    "\n",
    "def applydesc(series):return series.progress_apply(norm2grams)\n",
    "\n",
    "def index2sent2(x, desc_vocab): return indexesFromSentence(desc_vocab, x, 1, 80)\n",
    "\n",
    "def desc2index(series): return series.progress_apply(lambda x: index2sent2(x, desc_vocab))\n",
    "\n",
    "def parallelize_dataframe(df, func):\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def cutting(dataset):\n",
    "    pop_brand = dataset['brand_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n",
    "    dataset.loc[~dataset['brand_name'].isin(pop_brand), 'brand_name'] = 'missing'\n",
    "    pop_category1 = dataset['category_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n",
    "    dataset.loc[~dataset['category_name'].isin(pop_category1), 'category_name'] = 'missing'\n",
    "\n",
    "    \n",
    "def generate_features(data):\n",
    "        data[\"item_description\"] = data[\"item_description\"].replace(\"No description yet\", \"missing\")\n",
    "        \n",
    "        data['cat1'] = parallelize_dataframe(data[['category_name']], applycat1)\n",
    "        data['cat2'] = parallelize_dataframe(data[['category_name']], applycat2)\n",
    "        data['cat3'] = parallelize_dataframe(data[['category_name']], applycat3)\n",
    "        data.fillna(-1, inplace=True)\n",
    "        \n",
    "        print(\"Getting word/char len features\")\n",
    "        data['desc_words'] = parallelize_dataframe(data['item_description'], get_words)\n",
    "        data['desc_chars'] = parallelize_dataframe(data['item_description'], get_chars)\n",
    "        data['name_words'] = parallelize_dataframe(data['name'], get_words)\n",
    "        data['name_chars'] = parallelize_dataframe(data['name'], get_chars)\n",
    "        \n",
    "        \n",
    "        print(\"Get iphone features\")\n",
    "        data['iphone_case'] = parallelize_dataframe(data['name'], isphonecase)\n",
    "        data['iphone6'] = parallelize_dataframe(data['name'], isiphone6)\n",
    "        data['iphone6p'] = parallelize_dataframe(data['name'], isiphone6p)\n",
    "        data['iphone5'] = parallelize_dataframe(data['name'], isiphone5)\n",
    "        data['iphone5p'] = parallelize_dataframe(data['name'], isiphone5p)\n",
    "        data['iphone7'] = parallelize_dataframe(data['name'], isiphone7)\n",
    "        data['iphone7p'] = parallelize_dataframe(data['name'], isiphone7p)\n",
    "        data['unlocked_phone'] = parallelize_dataframe(data['name'], isunlocked)\n",
    "        \n",
    "        all_brands = set(data['brand_name'].values)\n",
    "        def brandfinder(line):\n",
    "            brand = line[0]\n",
    "            name = line[1]\n",
    "            namesplit = name.split(' ')\n",
    "            if brand == 'missing':\n",
    "                for x in namesplit:\n",
    "                    if x in all_brands:\n",
    "                        return x\n",
    "            #else:\n",
    "            #    return brand\n",
    "            #if name in all_brands:\n",
    "            #    return name\n",
    "            return brand\n",
    "        #data['brand_name2'] = data[['brand_name','name']].apply(brandfinder, axis = 1)   \n",
    "        \n",
    "        #print(\"Label encoding features\")\n",
    "        #cat_cols = ['category_name', 'brand_name', 'brand_name2', 'cat1', 'cat2', 'cat3', 'item_condition_id']\n",
    "        #for col in cat_cols:\n",
    "        #    data[col] = LabelEncoder().fit_transform(data[col].astype(str)) + 1\n",
    "            \n",
    "        print(\"Get count features\")\n",
    "        data['brand_counts'] = data.brand_name.map(data[\"brand_name\"].value_counts()).fillna(0).astype(int)\n",
    "\n",
    "        data['cat_counts'] = data.category_name.map(data[\"category_name\"].value_counts()).fillna(0).astype(int)\n",
    "        \n",
    "        data['cat1_counts'] = data.cat1.map(data[\"cat1\"].value_counts()).fillna(0).astype(int)\n",
    "\n",
    "        data['cat2_counts'] = data.cat2.map(data[\"cat2\"].value_counts()).fillna(0).astype(int)\n",
    "\n",
    "        data['cat3_counts'] = data.cat3.map(data[\"cat3\"].value_counts()).fillna(0).astype(int)\n",
    "  \n",
    "        \n",
    "        print(\"Getting punct related features\")\n",
    "        data[\"plus_counts\"] = parallelize_dataframe(data[\"item_description\"], plussigns)\n",
    "        data[\"ands_counts\"] = parallelize_dataframe(data[\"item_description\"], andsigns)\n",
    "        data[\"comma_counts\"] = parallelize_dataframe(data[\"item_description\"], commas)\n",
    "        data[\"all_counts\"] = data[\"plus_counts\"] + data[\"ands_counts\"] + data[\"comma_counts\"]\n",
    "        \n",
    "        #for col in [\"name\", \"item_description\"]:\n",
    "        #    data[col] = data[col].str.replace(\"'\", '').replace('-', '').progress_apply(unicodeToAscii)\n",
    "        #    data[col] = data[col].progress_apply(remove_puncts)\n",
    "        \n",
    "        data[\"brand_cat\"] = data[\"brand_name\"].astype(str) + ' ' + data[\"category_name\"].astype(str)\n",
    "        data[\"category_shipping\"] = data[\"category_name\"].astype(str) + ' ' + data[\"shipping\"].astype(str)\n",
    "        \n",
    "        data['brand_cat_counts'] = data.brand_cat.map(data[\"brand_cat\"].value_counts()).fillna(0).astype(int)\n",
    "        \n",
    "        num_cols =  [\"desc_words\", \"desc_chars\", \"name_words\", \"name_chars\", \"plus_counts\", \n",
    "                    \"ands_counts\", \"comma_counts\", \"all_counts\", \"brand_counts\", \"cat_counts\", \"cat1_counts\", \n",
    "                   \"cat2_counts\", \"cat3_counts\", \"brand_cat_counts\"]\n",
    "        data[num_cols]  = QuantileTransformer(output_distribution='normal', random_state=1).fit_transform(data[num_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1481661, 8) (693359, 7)\n",
      "94.826384 38.828184\n",
      "156.60152\n",
      "CPU times: user 6.6 s, sys: 517 ms, total: 7.12 s\n",
      "Wall time: 7.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "in_path = \"../input/\"\n",
    "train_data = pd.read_table(os.path.join(in_path, 'train.tsv'))\n",
    "test_data  = pd.read_table(os.path.join(in_path, 'test.tsv'))\n",
    "train_data = train_data.loc[(train_data.price > 2)].reset_index(drop=True)\n",
    "\n",
    "train_rows = len(train_data)\n",
    "data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "\n",
    "print(train_data.shape, test_data.shape)\n",
    "print(train_data.memory_usage().sum().sum()/10**6, test_data.memory_usage().sum().sum()/10**6)\n",
    "print(data.memory_usage().sum().sum()/10**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 271878/271878 [00:00<00:00, 671986.14it/s]\n",
      "100%|██████████| 271878/271878 [00:00<00:00, 644136.67it/s]\n",
      "100%|██████████| 271878/271878 [00:00<00:00, 673123.38it/s]\n",
      "100%|██████████| 271878/271878 [00:00<00:00, 663182.11it/s]\n",
      "100%|██████████| 271877/271877 [00:00<00:00, 679479.71it/s]\n",
      "100%|██████████| 271877/271877 [00:00<00:00, 713206.26it/s]\n",
      "100%|██████████| 271877/271877 [00:00<00:00, 682853.60it/s]\n",
      "100%|██████████| 271877/271877 [00:00<00:00, 654636.44it/s]\n",
      "100%|██████████| 271878/271878 [00:00<00:00, 494509.53it/s]\n",
      "100%|██████████| 271878/271878 [00:00<00:00, 503817.26it/s]\n",
      " 92%|█████████▏| 250114/271878 [00:00<00:00, 429484.25it/s]\n",
      "100%|██████████| 271878/271878 [00:00<00:00, 418892.99it/s]\n",
      "100%|██████████| 271877/271877 [00:00<00:00, 516197.64it/s]\n",
      "100%|██████████| 271877/271877 [00:00<00:00, 537277.17it/s]\n",
      "100%|██████████| 271877/271877 [00:00<00:00, 493795.71it/s]\n",
      "100%|██████████| 271877/271877 [00:00<00:00, 478776.00it/s]\n",
      "100%|██████████| 271878/271878 [00:00<00:00, 496571.19it/s]\n",
      "100%|██████████| 271878/271878 [00:00<00:00, 491622.05it/s]\n",
      "100%|██████████| 271878/271878 [00:00<00:00, 486447.91it/s]\n",
      "100%|██████████| 271878/271878 [00:00<00:00, 483341.53it/s]\n",
      "100%|██████████| 271877/271877 [00:00<00:00, 485745.34it/s]\n",
      "100%|██████████| 271877/271877 [00:00<00:00, 490494.42it/s]\n",
      " 82%|████████▏ | 221996/271877 [00:00<00:00, 402850.34it/s]\n",
      "100%|██████████| 271877/271877 [00:00<00:00, 456293.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting word/char len features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 271878/271878 [00:00<00:00, 372001.74it/s]\n",
      "100%|██████████| 271878/271878 [00:00<00:00, 404241.07it/s]\n",
      "100%|██████████| 271878/271878 [00:00<00:00, 372812.46it/s]\n",
      "100%|██████████| 271877/271877 [00:00<00:00, 375756.95it/s]\n",
      "100%|██████████| 271877/271877 [00:00<00:00, 375208.00it/s]\n",
      "100%|██████████| 271878/271878 [00:01<00:00, 212247.07it/s]\n",
      "100%|██████████| 271877/271877 [00:00<00:00, 392304.01it/s]\n",
      "100%|██████████| 271877/271877 [00:00<00:00, 400895.91it/s]\n",
      " 77%|███████▋  | 210185/271878 [00:00<00:00, 674722.55it/s]\n",
      "  0%|          | 0/271877 [00:00<?, ?it/s]0, 666157.45it/s]\n",
      "100%|██████████| 271878/271878 [00:00<00:00, 563928.49it/s]\n",
      "100%|██████████| 271878/271878 [00:00<00:00, 580033.84it/s]\n",
      "100%|██████████| 271877/271877 [00:00<00:00, 669396.37it/s]\n",
      "100%|██████████| 271877/271877 [00:00<00:00, 741176.44it/s]\n",
      "100%|██████████| 271877/271877 [00:00<00:00, 752148.63it/s]\n",
      "100%|██████████| 271877/271877 [00:00<00:00, 776072.14it/s]\n",
      "100%|██████████| 271878/271878 [00:00<00:00, 619742.71it/s]\n",
      "  0%|          | 0/271877 [00:00<?, ?it/s]0, 644834.76it/s]\n",
      "100%|██████████| 271878/271878 [00:00<00:00, 612113.99it/s]\n",
      " 40%|████      | 109457/271877 [00:00<00:00, 472968.63it/s]\n",
      "100%|██████████| 271877/271877 [00:00<00:00, 585793.74it/s]\n",
      "100%|██████████| 271877/271877 [00:00<00:00, 606699.18it/s]\n",
      "100%|██████████| 271877/271877 [00:00<00:00, 599223.12it/s]\n",
      "100%|██████████| 271877/271877 [00:00<00:00, 562939.25it/s]\n",
      "100%|██████████| 271878/271878 [00:00<00:00, 641953.00it/s]\n",
      "100%|██████████| 271878/271878 [00:00<00:00, 747579.44it/s]\n",
      "100%|██████████| 271878/271878 [00:00<00:00, 653094.16it/s]\n",
      "100%|██████████| 271878/271878 [00:00<00:00, 659496.67it/s]\n",
      "100%|██████████| 271877/271877 [00:00<00:00, 773076.36it/s]\n",
      "100%|██████████| 271877/271877 [00:00<00:00, 709712.04it/s]\n",
      "100%|██████████| 271877/271877 [00:00<00:00, 777611.09it/s]\n",
      "100%|██████████| 271877/271877 [00:00<00:00, 764326.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get iphone features\n",
      "Get count features\n",
      "Getting punct related features\n",
      "(2175020, 36)\n",
      "CPU times: user 29.8 s, sys: 9.89 s, total: 39.7 s\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NUM_BRANDS = 4500\n",
    "NUM_CATEGORIES = 1200\n",
    "\n",
    "cutting(data)\n",
    "generate_features(data)\n",
    "\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.826384 38.828184\n",
      "626.40584\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "print(train_data.memory_usage().sum().sum()/10**6, test_data.memory_usage().sum().sum()/10**6)\n",
    "print(data.memory_usage().sum().sum()/10**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 33s, sys: 234 ms, total: 1min 34s\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for col in ['name', 'item_description']:\n",
    "    train_data[col] = normalizeString(train_data[col])\n",
    "    test_data[col] = normalizeString(test_data[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['brand new without tag price is firm material polyester cotton color green mint as pictured size 190 x 85 cm 74 x 34 inches this is adult size fits most average adult features super cozy comfy snuggle woolen yarn blanket for adult and girls with beautiful knitted pattern to complement your living room or bedroom decor blanket features with fish scales pattern open mermaid fins with tassels handcrafted and exquisite ideal gifts for girls and women great to wrap on the sofa couch bed or car for reading watching tv or nap sleep',\n",
       "       'size18 in plus size'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['item_description'].sample(2).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing categories...\n",
      "shipping\n",
      "desc_words\n",
      "desc_chars\n",
      "name_words\n",
      "name_chars\n",
      "iphone_case\n",
      "iphone6\n",
      "iphone6p\n",
      "iphone5\n",
      "iphone5p\n",
      "iphone7\n",
      "iphone7p\n",
      "unlocked_phone\n",
      "brand_counts\n",
      "cat_counts\n",
      "cat1_counts\n",
      "cat2_counts\n",
      "cat3_counts\n",
      "plus_counts\n",
      "ands_counts\n",
      "comma_counts\n",
      "all_counts\n",
      "(1481661, 22) (693359, 22)\n",
      "CPU times: user 9.05 s, sys: 741 ms, total: 9.79 s\n",
      "Wall time: 9.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('processing categories...')\n",
    "X_cats = {}\n",
    "X_test_cats = {}\n",
    "for col in [\"brand_name\", \"category_name\", \"cat1\", \"cat2\", \"cat3\", \"brand_cat\", \"category_shipping\", \"item_condition_id\"]:\n",
    "    data[col] = data[col].astype('category').cat.codes + 1\n",
    "    X_cats[col] = data.loc[:train_rows-1, col].astype(np.int32).values.reshape(-1,1)\n",
    "    X_test_cats[col] = data.loc[train_rows:, col].astype(np.int32).values.reshape(-1,1)\n",
    "    \n",
    "X_conts = []\n",
    "X_test_conts = []\n",
    "for col in ['shipping', 'desc_words', 'desc_chars',\n",
    "            'name_words', 'name_chars',  'iphone_case', 'iphone6', 'iphone6p', \n",
    "            'iphone5', 'iphone5p', 'iphone7', 'iphone7p', 'unlocked_phone', \n",
    "            'brand_counts', 'cat_counts', 'cat1_counts', \n",
    "            'cat2_counts', 'cat3_counts', 'plus_counts', 'ands_counts', \n",
    "            'comma_counts', 'all_counts']:\n",
    "    print(col)\n",
    "    X_conts.append(data.loc[:train_rows-1, col].replace('missing', 0).astype(np.float32).values)\n",
    "    X_test_conts.append(data.loc[train_rows:, col].replace('missing', 0).astype(np.float32).values)\n",
    "\n",
    "X_conts = np.array(X_conts).T\n",
    "X_test_conts = np.array(X_test_conts).T\n",
    "\n",
    "print(X_conts.shape, X_test_conts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "497"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#test_data['name'] = test_data['name'].str.replace(\"nmd(s){0,1}(\\s){0,1}(r){0,1}(1){0,1}(\\s|$)\", 'nmdr ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing title...\n",
      "26399\n",
      "CPU times: user 22.2 s, sys: 176 ms, total: 22.4 s\n",
      "Wall time: 22.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('processing title...')\n",
    "name_num_col = 7\n",
    "#name_voc_size = 26000\n",
    "name_tok = Tokenizer(min_df=5)\n",
    "#name_tok.fit_on_texts(train_data.name)\n",
    "#X_name = pad_sequences(name_tok.texts_to_sequences(train_data.name), maxlen=name_num_col, truncating='post')\n",
    "#X_test_name = pad_sequences(name_tok.texts_to_sequences(test_data.name), maxlen=name_num_col, truncating='post')\n",
    "X_name = name_tok.fit_transform(train_data.name)\n",
    "X_test_name = name_tok.transform(test_data.name)\n",
    "\n",
    "X_name = X_name[:, :name_num_col]\n",
    "X_test_name = X_test_name[:, :name_num_col]\n",
    "\n",
    "name_voc_size = name_tok.vocabulary_size()\n",
    "print(name_voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing description...\n",
      "34245\n",
      "CPU times: user 47.8 s, sys: 1.27 s, total: 49.1 s\n",
      "Wall time: 49.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('processing description...')\n",
    "\n",
    "desc_num_col = 70\n",
    "#desc_voc_size = 50000\n",
    "\n",
    "#desc_tok = Tokenizer(num_words=desc_voc_size)\n",
    "#desc_tok.fit_on_texts(train_data.item_description)\n",
    "\n",
    "#X_desc = pad_sequences(desc_tok.texts_to_sequences(train_data.item_description.astype(str)), maxlen=desc_num_col, truncating='post')\n",
    "#X_test_desc = pad_sequences(desc_tok.texts_to_sequences(test_data.item_description.astype(str)), maxlen=desc_num_col, truncating='post')\n",
    "\n",
    "desc_tok = Tokenizer(min_df=10)\n",
    "X_desc = desc_tok.fit_transform(train_data[\"item_description\"])\n",
    "X_test_desc = desc_tok.transform(test_data[\"item_description\"])\n",
    "X_desc = X_desc[:, :desc_num_col]\n",
    "X_test_desc = X_test_desc[:, :desc_num_col]\n",
    "desc_voc_size = desc_tok.vocabulary_size()\n",
    "print(desc_voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing description2...\n",
      "18200\n",
      "CPU times: user 58.2 s, sys: 2.33 s, total: 1min\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('processing description2...')\n",
    "\n",
    "train_data['item_desc2gram'] = parallelize_dataframe(train_data[\"item_description\"], get_2grams)\n",
    "test_data['item_desc2gram'] = get_2grams(test_data[\"item_description\"])\n",
    "\n",
    "desc2_num_col = 20\n",
    "#desc2_voc_size = 20000\n",
    "#desc_tok2 = Tokenizer(num_words= desc2_voc_size)\n",
    "#desc_tok2.fit_on_texts(train_data.item_desc2gram)\n",
    "\n",
    "#X_desc2 = pad_sequences(desc_tok2.texts_to_sequences(train_data.item_desc2gram), maxlen=desc2_num_col, truncating='post')\n",
    "#X_test_desc2 = pad_sequences(desc_tok2.texts_to_sequences(test_data.item_desc2gram), maxlen=desc2_num_col, truncating='post')\n",
    "\n",
    "desc_tok2 = Tokenizer(min_df=200)\n",
    "X_desc2 = desc_tok2.fit_transform(train_data['item_desc2gram'])\n",
    "X_test_desc2 = desc_tok2.transform(test_data['item_desc2gram'])\n",
    "X_desc2 = X_desc2[:, :desc2_num_col]\n",
    "X_test_desc2 = X_test_desc2[:, :desc2_num_col]\n",
    "\n",
    "desc2_voc_size = desc_tok2.vocabulary_size()\n",
    "print(desc2_voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing item name...\n",
      "26399\n",
      "CPU times: user 53.8 s, sys: 51.7 ms, total: 53.8 s\n",
      "Wall time: 53.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('processing item name...')\n",
    "itemname_num_col = 15\n",
    "#X_itemname = pad_sequences(name_tok.texts_to_sequences(train_data.item_description.astype(str)), maxlen=itemname_num_col, truncating='post')\n",
    "#X_test_itemname = pad_sequences(name_tok.texts_to_sequences(test_data.item_description.astype(str)), maxlen=itemname_num_col, truncating='post')\n",
    "\n",
    "X_itemname = name_tok.transform(train_data['item_description'])\n",
    "X_test_itemname = name_tok.transform(test_data['item_description'])\n",
    "X_itemname = X_itemname[:, :itemname_num_col]\n",
    "X_test_itemname = X_test_itemname[:, :itemname_num_col]\n",
    "name_voc_size = name_tok.vocabulary_size()\n",
    "print(name_voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.486508 19.414052\n",
      "414.86508 194.14052\n",
      "118.53288 55.46872\n",
      "88.89966 41.60154\n",
      "130.386168 61.015592\n",
      "106.679672 44.375056\n"
     ]
    }
   ],
   "source": [
    "#Lets get sizes\n",
    "gc.collect()\n",
    "print(X_name.nbytes/10**6, X_test_name.nbytes/10**6)\n",
    "print(X_desc.nbytes/10**6, X_test_desc.nbytes/10**6)\n",
    "print(X_desc2.nbytes/10**6, X_test_desc2.nbytes/10**6)\n",
    "print(X_itemname.nbytes/10**6, X_test_itemname.nbytes/10**6)\n",
    "print(X_conts.nbytes/10**6, X_test_conts.nbytes/10**6)\n",
    "print(train_data.memory_usage().sum().sum()/10**6, test_data.memory_usage().sum().sum()/10**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#del name_tok, desc_tok, desc_tok2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def keras model\n",
    "y = np.log1p(train_data.price).values.reshape(-1,1)\n",
    "cvlist = list(KFold(40, random_state=1).split(X_desc))\n",
    "std = np.std(y)\n",
    "mean = np.mean(y)\n",
    "ynorm = (y - mean)/std\n",
    "\n",
    "name_seq_len = X_name.shape[1]\n",
    "name_embeddings_dim = 50\n",
    "\n",
    "itemname_embeddings_dim = 30\n",
    "\n",
    "desc_seq_len = X_desc.shape[1]\n",
    "desc_embeddings_dim = 50\n",
    "\n",
    "desc2_seq_len = X_desc2.shape[1]\n",
    "desc2_embeddings_dim = 30\n",
    "\n",
    "brand_voc_size = 5600\n",
    "brand_embeddings_dim = 50\n",
    "\n",
    "cat_voc_size = 1500\n",
    "cat_embeddings_dim = 40\n",
    "\n",
    "cat1_voc_size = 16\n",
    "cat1_embeddings_dim = 6\n",
    "\n",
    "cat2_voc_size = 121\n",
    "cat2_embeddings_dim = 15\n",
    "\n",
    "cat3_voc_size = 900\n",
    "cat3_embeddings_dim = 20\n",
    "\n",
    "brandcat_voc_size = 51000\n",
    "brandcat_embeddings_dim = 30\n",
    "\n",
    "catship_voc_size = 3500\n",
    "catship_embeddings_dim = 20\n",
    "\n",
    "cond_voc_size = 6\n",
    "cond_embedding_size = 4\n",
    "\n",
    "def keras_mercari_model(seed, params):\n",
    "    #Get all input params\n",
    "    name_embeddings_dim, desc_embeddings_dim, desc2_embeddings_dim, \\\n",
    "    brand_embeddings_dim, cat_embeddings_dim, cat1_embeddings_dim, \\\n",
    "    cat2_embeddings_dim, cat3_embeddings_dim, cond_embedding_size, \\\n",
    "    brandcat_embeddings_dim, catship_embeddings_dim, itemname_embeddings_dim, \\\n",
    "    name_drop, desc_drop, desc2_drop, brand_drop, cat_drop, cat1_drop, cat2_drop, \\\n",
    "    cat3_drop, cond_drop, brandcat_drop, catship_drop, itemname_drop, \\\n",
    "    dense_dim, lr, lr_decay = params\n",
    "    \n",
    "    \n",
    "    name = Input(shape=(name_seq_len,))\n",
    "    desc = Input(shape=(desc_seq_len,))\n",
    "    desc2 = Input(shape=(desc2_seq_len,))\n",
    "    itemname = Input(shape=(itemname_num_col,))\n",
    "    brand = Input(shape=(1,))\n",
    "    cat = Input(shape=(1,))\n",
    "    cat1 = Input(shape=(1,))\n",
    "    cat2 = Input(shape=(1,))\n",
    "    cat3 = Input(shape=(1,))\n",
    "    brandcat = Input(shape=(1,))\n",
    "    catship = Input(shape=(1,))\n",
    "    cond = Input(shape=(1,))\n",
    "    conts = Input(shape=(X_conts.shape[1],))\n",
    "    inputs = [name, desc, desc2, itemname, brand, cat, cat1, cat2, cat3, brandcat, catship, cond, conts]\n",
    "    \n",
    "    \n",
    "    embed_name = Embedding(name_voc_size, name_embeddings_dim, embeddings_initializer='he_uniform')\n",
    "    name = embed_name(name)\n",
    "    desc = Embedding(desc_voc_size, desc_embeddings_dim, embeddings_initializer='he_uniform')(desc)\n",
    "    desc2 = Embedding(desc2_voc_size, desc2_embeddings_dim, embeddings_initializer='he_uniform')(desc2)\n",
    "    itemname = Embedding(name_voc_size, itemname_embeddings_dim, embeddings_initializer='he_uniform')(itemname)\n",
    "    \n",
    "    brand = Embedding(brand_voc_size, brand_embeddings_dim, embeddings_initializer='he_uniform')(brand)\n",
    "    cat = Embedding(cat_voc_size, cat_embeddings_dim, embeddings_initializer='he_uniform')(cat)\n",
    "    cat1 = Embedding(cat1_voc_size, cat1_embeddings_dim, embeddings_initializer='he_uniform')(cat1)\n",
    "    cat2 = Embedding(cat2_voc_size, cat2_embeddings_dim, embeddings_initializer='he_uniform')(cat2)\n",
    "    cat3 = Embedding(cat3_voc_size, cat3_embeddings_dim, embeddings_initializer='he_uniform')(cat3)\n",
    "    brandcat = Embedding(brandcat_voc_size, brandcat_embeddings_dim, embeddings_initializer='he_uniform')(brandcat)\n",
    "    catship = Embedding(catship_voc_size, catship_embeddings_dim, embeddings_initializer='he_uniform')(catship)\n",
    "    cond = Embedding(cond_voc_size, cond_embedding_size, embeddings_initializer='he_uniform')(cond)\n",
    "    \n",
    "    #name2 = Conv1D(32, 2, activation='relu', kernel_initializer='he_normal', use_bias=False)(name)\n",
    "    #name2 = Dropout(0.2)(name2)\n",
    "    #name2 = MaxPooling1D()(name2)\n",
    "    #name2 = GlobalAveragePooling1D()(name2)\n",
    "    \n",
    "    #name3 = Conv1D(32, 3, activation='relu', kernel_initializer='he_normal', use_bias=False)(name)\n",
    "    #name3 = Dropout(0.2)(name3)\n",
    "    #name3 = MaxPooling1D()(name3)\n",
    "    #name3 = GlobalAveragePooling1D()(name3)\n",
    "    \n",
    "    #name2 = concatenate([name2, name3])\n",
    "    #name2 = GlobalAveragePooling1D()(name2)\n",
    "    #name3 = Conv1D(name_embeddings_dim, 3, activation='relu', kernel_initializer='he_normal', use_bias=False)(name)\n",
    "    print(name.shape)\n",
    "    #name = concatenate([name, name2], 1)\n",
    "    name = Dropout(name_drop, seed=seed)(name)\n",
    "    name = GlobalAveragePooling1D()(name)\n",
    "    \n",
    "    #name = Flatten()(name)\n",
    "    \n",
    "    #desc = Conv1D(20, 3, activation='relu', kernel_initializer='he_normal', use_bias=False)(desc)\n",
    "\n",
    "    #desc = Flatten()(desc)\n",
    "    #desc21 = Conv1D(32, 2, activation='relu', kernel_initializer='he_normal', use_bias=False)(desc)\n",
    "    #desc21 = Dropout(0.4)(desc21)\n",
    "    #desc21 = MaxPooling1D()(desc21)\n",
    "    #desc21 = GlobalAveragePooling1D()(desc21)\n",
    "    \n",
    "    #desc3 = Conv1D(16, 3, activation='relu', kernel_initializer='he_normal', use_bias=False)(desc)\n",
    "    #desc3 = Dropout(0.6)(desc3)\n",
    "    #desc3 = MaxPooling1D()(desc3)\n",
    "    #desc3 = GlobalAveragePooling1D()(desc3)\n",
    "    \n",
    "    #desc23 = concatenate([desc21,desc3])\n",
    "    \n",
    "    desc = Dropout(desc_drop, seed=seed)(desc)\n",
    "    desc = GlobalAveragePooling1D()(desc)\n",
    "    #\n",
    "    desc2 = Dropout(desc2_drop, seed=seed)(desc2)\n",
    "    desc2 = GlobalAveragePooling1D()(desc2)\n",
    "    #desc2 = Flatten()(desc2)\n",
    "    \n",
    "    itemname = Dropout(itemname_drop, seed=seed)(itemname)\n",
    "    itemname = GlobalAveragePooling1D()(itemname)\n",
    "    #itemname = Flatten()(itemname)\n",
    "    \n",
    "    brandcat = Dropout(brandcat_drop, seed=seed)(brandcat)\n",
    "    catship = Dropout(catship_drop, seed=seed)(catship)\n",
    "    \n",
    "    #conts = Dropout(0.02, seed=786)(conts)\n",
    "    cat = Dropout(cat_drop, seed=seed)(cat)\n",
    "    cat1 = Dropout(cat1_drop, seed=seed)(cat)\n",
    "    cat2 = Dropout(cat2_drop, seed=seed)(cat)\n",
    "    cat3 = Dropout(cat3_drop, seed=seed)(cat)\n",
    "    brand = Dropout(brand_drop, seed=seed)(cat)\n",
    "    \n",
    "    brand = Flatten()(brand)\n",
    "    cat = Flatten() (cat)\n",
    "    cat1 = Flatten()(cat1)\n",
    "    cat2 = Flatten()(cat2)\n",
    "    cat3 = Flatten()(cat3)\n",
    "    brandcat = Flatten()(brandcat)\n",
    "    catship = Flatten()(catship)\n",
    "    cond = Flatten()(cond)\n",
    "    \n",
    "    out = concatenate([name, \n",
    "                       #name2,\n",
    "                       desc, \n",
    "                       desc2,\n",
    "                       #desc3,\n",
    "                       itemname, \n",
    "                       brand, \n",
    "                       cat, \n",
    "                       cat1, \n",
    "                       cat2,\n",
    "                       cat3, \n",
    "                       brandcat, \n",
    "                       catship, \n",
    "                       cond, \n",
    "                       #conts\n",
    "                      ])\n",
    "    print(out.shape)\n",
    "    out = BatchNormalization(momentum=0.9)(out)\n",
    "    out = Dropout(0.00, seed=786)(out)\n",
    "    \n",
    "    out = Dense(dense_dim, activation='selu',kernel_initializer='he_normal')(out)\n",
    "    out = PReLU()(out)\n",
    "    out = BatchNormalization(momentum=0.99)(out)\n",
    "    out = Dropout(0.00, seed=786)(out)\n",
    "    out = Dense(1, kernel_initializer='normal')(out)\n",
    "    \n",
    "    model = Model(inputs, out)\n",
    "    opt = RMSprop(lr=lr, decay=lr_decay, clipvalue=0.1)\n",
    "    model.compile(optimizer=opt, loss=\"mse\" )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_cores = 1\n",
    "GPU= True\n",
    "CPU= False\n",
    "if GPU:\n",
    "    num_GPU = 1\n",
    "    num_CPU = 1\n",
    "if CPU:\n",
    "    num_CPU = 1\n",
    "    num_GPU = 0\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=num_cores,\\\n",
    "        inter_op_parallelism_threads=num_cores, allow_soft_placement=True,\\\n",
    "        device_count = {'CPU' : num_CPU, 'GPU' : num_GPU})\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TestCallback(Callback):\n",
    "    def __init__(self, x_val, x_test):\n",
    "        self.x_val = x_val\n",
    "        self.x_test = x_test\n",
    "        self.val_preds = []\n",
    "        self.test_preds = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_val_preds = self.model.predict(self.x_val)\n",
    "        y_test_preds = self.model.predict(self.x_test)\n",
    "        self.val_preds.append(y_val_preds)\n",
    "        self.test_preds.append(y_test_preds)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [64, 64, 32, 32, \n",
    "          32, 8, 16, 24, 5,\n",
    "          32, 32, 24, \n",
    "         0.05, 0.3, 0.05, 0.05,\n",
    "          0.1, 0.1, 0.1, 0.1,\n",
    "          0.0, 0.3, 0.1, 0.2,\n",
    "         200, 0.009, 0.006]\n",
    "X = [X_name, X_desc, X_desc2, X_itemname, X_cats[\"brand_name\"], X_cats[\"category_name\"],\n",
    "    X_cats[\"cat1\"], X_cats[\"cat2\"], X_cats[\"cat3\"], X_cats[\"brand_cat\"],\n",
    "    X_cats[\"category_shipping\"], X_cats[\"item_condition_id\"], X_conts]\n",
    "\n",
    "X_test = [X_test_name, X_test_desc, X_test_desc2, X_test_itemname, X_test_cats[\"brand_name\"], X_test_cats[\"category_name\"],\n",
    "    X_test_cats[\"cat1\"], X_test_cats[\"cat2\"], X_test_cats[\"cat3\"], X_test_cats[\"brand_cat\"],\n",
    "    X_test_cats[\"category_shipping\"], X_test_cats[\"item_condition_id\"], X_test_conts]\n",
    "\n",
    "def train_model(params):\n",
    "    y_trues = []\n",
    "    y_preds = []\n",
    "    for seed in [1,2,3,4,5,6,7,8,9,10]:\n",
    "        batchsize=2000\n",
    "        epochs=4\n",
    "        np.random.seed(seed)\n",
    "        tf.set_random_seed(seed)\n",
    "\n",
    "        model = keras_mercari_model(seed, params)\n",
    "\n",
    "        train_idx, val_idx= cvlist[seed]\n",
    "\n",
    "\n",
    "        X_tr = [x[train_idx] for x in X]\n",
    "        X_val = [x[val_idx] for x in X]\n",
    "\n",
    "        #val_store = TestCallback(X_val, X_test)\n",
    "        gc.collect()\n",
    "        model.fit(X_tr, y[train_idx], batch_size=batchsize, epochs=epochs,\n",
    "                                       verbose=1,\n",
    "                                      validation_data=(X_val, y[val_idx]), shuffle=True,\n",
    "                                      #callbacks= [val_store]\n",
    "                 )\n",
    "        y_val = y[val_idx,0]\n",
    "        y_pred = model.predict(X_val)[:, 0]\n",
    "        print(np.sqrt(metrics.mean_squared_error(y_val, y_pred)))\n",
    "        \n",
    "        y_trues.extend(y_val)\n",
    "        y_preds.extend(y_pred)\n",
    "        K.clear_session()\n",
    "    score = np.sqrt(metrics.mean_squared_error(y_trues, y_preds))\n",
    "    print(\"Score with params {} is {}\".format(params, score))\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5250"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import skopt\n",
    "from skopt import gp_minimize, gbrt_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 7, 60)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 22s 16us/step - loss: 0.9105 - val_loss: 0.1981\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1806 - val_loss: 0.1837\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1649 - val_loss: 0.1807\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1563 - val_loss: 0.1756\n",
      "0.4191008970561012\n",
      "(?, 7, 60)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.9064 - val_loss: 0.2268\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1814 - val_loss: 0.1864\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1656 - val_loss: 0.1807\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1574 - val_loss: 0.1795\n",
      "0.4236479394773567\n",
      "(?, 7, 60)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 1.0119 - val_loss: 0.2094\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1853 - val_loss: 0.1949\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1659 - val_loss: 0.1804\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1565 - val_loss: 0.1774\n",
      "0.4211672948165041\n",
      "(?, 7, 60)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.9019 - val_loss: 0.2082\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1842 - val_loss: 0.1862\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1666 - val_loss: 0.1787\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1583 - val_loss: 0.1793\n",
      "0.4233970360318168\n",
      "(?, 7, 60)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.9398 - val_loss: 0.1948\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1811 - val_loss: 0.1852\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1658 - val_loss: 0.1803\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1574 - val_loss: 0.1772\n",
      "0.42100617110804645\n",
      "(?, 7, 60)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.9717 - val_loss: 0.2155\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1841 - val_loss: 0.1882\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1683 - val_loss: 0.1832\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1599 - val_loss: 0.1793\n",
      "0.4234407136938914\n",
      "(?, 7, 60)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.7765 - val_loss: 0.2013\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1763 - val_loss: 0.1833\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1610 - val_loss: 0.1767\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1526 - val_loss: 0.1746\n",
      "0.4179086571091989\n",
      "(?, 7, 60)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.9327 - val_loss: 0.1993\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1822 - val_loss: 0.1839\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1646 - val_loss: 0.1822\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1558 - val_loss: 0.1752\n",
      "0.41858852302708816\n",
      "(?, 7, 60)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.9335 - val_loss: 0.2053\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1806 - val_loss: 0.1839\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1639 - val_loss: 0.1809\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1550 - val_loss: 0.1805\n",
      "0.4248839548639186\n",
      "(?, 7, 60)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.8453 - val_loss: 0.2049\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1769 - val_loss: 0.1837\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1617 - val_loss: 0.1814\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1535 - val_loss: 0.1841\n",
      "0.42901589220766373\n",
      "Score with params [60, 63, 80, 83, 83, 11, 13, 23, 7, 58, 44, 31, 0.178354600156416, 0.48183138025051475, 0.1917207594128889, 0.15834500761653295, 0.10577898395058091, 0.11360891221878648, 0.18511932765853223, 0.014207211639577392, 0.008712929970154073, 0.010109198720162861, 0.3330479382191753, 0.38907837547492535, 495, 0.013311134358659674, 0.007630512633831947] is 0.4222278201598307\n",
      "(?, 7, 65)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.9238 - val_loss: 0.2113\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.1832 - val_loss: 0.1799\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.1613 - val_loss: 0.1788\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.1496 - val_loss: 0.1750\n",
      "0.4183230435473312\n",
      "(?, 7, 65)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.7196 - val_loss: 0.2519\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.1796 - val_loss: 0.1801\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.1581 - val_loss: 0.1778\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.1472 - val_loss: 0.1728\n",
      "0.4157015439556793\n",
      "(?, 7, 65)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.7662 - val_loss: 0.2499\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.1805 - val_loss: 0.1858\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.1589 - val_loss: 0.1767\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.1477 - val_loss: 0.1739\n",
      "0.4170191138994934\n",
      "(?, 7, 65)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 17s 11us/step - loss: 0.8582 - val_loss: 0.2014\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.1798 - val_loss: 0.1803\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.1576 - val_loss: 0.1741\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.1462 - val_loss: 0.1721\n",
      "0.4148335445863016\n",
      "(?, 7, 65)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.7780 - val_loss: 0.2051\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.1801 - val_loss: 0.1834\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.1588 - val_loss: 0.1756\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.1475 - val_loss: 0.1718\n",
      "0.4144977441459289\n",
      "(?, 7, 65)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.8170 - val_loss: 0.2130\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.1785 - val_loss: 0.1859\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.1581 - val_loss: 0.1815\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.1473 - val_loss: 0.1720\n",
      "0.4147146527269324\n",
      "(?, 7, 65)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 17s 11us/step - loss: 0.8188 - val_loss: 0.1965\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.1797 - val_loss: 0.1817\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.1587 - val_loss: 0.1759\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.1475 - val_loss: 0.1751\n",
      "0.4184811985828743\n",
      "(?, 7, 65)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.7149 - val_loss: 0.1895\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.1772 - val_loss: 0.1910\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.1571 - val_loss: 0.1732\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.1459 - val_loss: 0.1743\n",
      "0.41748993892761455\n",
      "(?, 7, 65)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.8301 - val_loss: 0.1906\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.1771 - val_loss: 0.1778\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.1561 - val_loss: 0.1749\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.1450 - val_loss: 0.1753\n",
      "0.4186995376146819\n",
      "(?, 7, 65)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.8344 - val_loss: 0.2125\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.1794 - val_loss: 0.1823\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.1574 - val_loss: 0.1777\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.1462 - val_loss: 0.1813\n",
      "0.4257992825877752\n",
      "Score with params [65, 45, 35, 35, 30, 11, 36, 5, 3, 40, 39, 18, 0.08293238799810473, 0.1322778060523135, 0.38711684471710844, 0.09123006644330972, 0.11368678977372973, 0.0037579600872710292, 0.12352709941517544, 0.12241914454448431, 0.061693399687475704, 0.4718740392573122, 0.27272811964139343, 0.17975395028689303, 440, 0.013730531581261796, 0.002389924905025634] is 0.4175678473127487\n",
      "(?, 7, 73)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.5440 - val_loss: 0.1980\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 17s 12us/step - loss: 0.1789 - val_loss: 0.1867\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1661 - val_loss: 0.1789\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 17s 12us/step - loss: 0.1590 - val_loss: 0.1768\n",
      "0.42043553803336403\n",
      "(?, 7, 73)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.5607 - val_loss: 0.2067\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 17s 12us/step - loss: 0.1803 - val_loss: 0.1906\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1679 - val_loss: 0.1856\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1613 - val_loss: 0.1801\n",
      "0.42443244727733476\n",
      "(?, 7, 73)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.5534 - val_loss: 0.1949\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 17s 12us/step - loss: 0.1780 - val_loss: 0.1852\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1653 - val_loss: 0.1801\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1585 - val_loss: 0.1778\n",
      "0.4216854579062629\n",
      "(?, 7, 73)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.5604 - val_loss: 0.2012\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 17s 12us/step - loss: 0.1782 - val_loss: 0.1837\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1660 - val_loss: 0.1798\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 17s 12us/step - loss: 0.1593 - val_loss: 0.1779\n",
      "0.4217564857599147\n",
      "(?, 7, 73)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.5409 - val_loss: 0.1905\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1748 - val_loss: 0.1827\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 17s 12us/step - loss: 0.1624 - val_loss: 0.1770\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 17s 12us/step - loss: 0.1556 - val_loss: 0.1746\n",
      "0.4178580135937406\n",
      "(?, 7, 73)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.5678 - val_loss: 0.2009\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1824 - val_loss: 0.1902\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 17s 12us/step - loss: 0.1693 - val_loss: 0.1868\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1626 - val_loss: 0.1829\n",
      "0.42761167354664253\n",
      "(?, 7, 73)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.5462 - val_loss: 0.1975\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 17s 12us/step - loss: 0.1746 - val_loss: 0.1822\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 17s 12us/step - loss: 0.1619 - val_loss: 0.1788\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 17s 12us/step - loss: 0.1550 - val_loss: 0.1760\n",
      "0.4194900141478513\n",
      "(?, 7, 73)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.5652 - val_loss: 0.1915\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 17s 12us/step - loss: 0.1788 - val_loss: 0.1838\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 17s 12us/step - loss: 0.1654 - val_loss: 0.1844\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 17s 12us/step - loss: 0.1586 - val_loss: 0.1770\n",
      "0.4207148568439205\n",
      "(?, 7, 73)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.5612 - val_loss: 0.1991\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 17s 12us/step - loss: 0.1758 - val_loss: 0.1855\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1632 - val_loss: 0.1796\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1561 - val_loss: 0.1776\n",
      "0.4214352347451247\n",
      "(?, 7, 73)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.5442 - val_loss: 0.1983\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 17s 12us/step - loss: 0.1757 - val_loss: 0.1844\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1444619/1444619 [==============================] - 17s 12us/step - loss: 0.1632 - val_loss: 0.1810\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 17s 12us/step - loss: 0.1564 - val_loss: 0.1805\n",
      "0.4248748024401624\n",
      "Score with params [73, 51, 27, 62, 98, 31, 4, 18, 5, 61, 20, 50, 0.08772030269246409, 0.4941869190296132, 0.05102240537401405, 0.041775351218966944, 0.032261903576999255, 0.1306216650930797, 0.050658320507956434, 0.09326215457126127, 0.02444255920016028, 0.07948479182275987, 0.044150056465722065, 0.32816479473263677, 369, 0.012118491266336818, 0.01429399474831039] is 0.42203818347200256\n",
      "(?, 7, 111)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.4037 - val_loss: 0.1896\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1694 - val_loss: 0.1752\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1547 - val_loss: 0.1724\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1465 - val_loss: 0.1710\n",
      "0.41356434382061036\n",
      "(?, 7, 111)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.4055 - val_loss: 0.2018\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1704 - val_loss: 0.1807\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1559 - val_loss: 0.1753\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1477 - val_loss: 0.1727\n",
      "0.41558223417130946\n",
      "(?, 7, 111)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.4111 - val_loss: 0.1938\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1714 - val_loss: 0.1802\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1564 - val_loss: 0.1779\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1480 - val_loss: 0.1730\n",
      "0.4159609039474327\n",
      "(?, 7, 111)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.4054 - val_loss: 0.1914\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1703 - val_loss: 0.1786\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1557 - val_loss: 0.1751\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1474 - val_loss: 0.1733\n",
      "0.4163367196643656\n",
      "(?, 7, 111)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.4036 - val_loss: 0.1883\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1699 - val_loss: 0.1796\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1556 - val_loss: 0.1743\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1474 - val_loss: 0.1729\n",
      "0.41585115732506067\n",
      "(?, 7, 111)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.4125 - val_loss: 0.1951\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1712 - val_loss: 0.1812\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1564 - val_loss: 0.1765\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1481 - val_loss: 0.1746\n",
      "0.41790759466240573\n",
      "(?, 7, 111)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.4213 - val_loss: 0.1935\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1707 - val_loss: 0.1779\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1563 - val_loss: 0.1734\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1480 - val_loss: 0.1716\n",
      "0.41426004348819345\n",
      "(?, 7, 111)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.4066 - val_loss: 0.1935\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1705 - val_loss: 0.1771\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1559 - val_loss: 0.1758\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1476 - val_loss: 0.1739\n",
      "0.4170561818888353\n",
      "(?, 7, 111)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.4084 - val_loss: 0.1947\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1716 - val_loss: 0.1805\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1567 - val_loss: 0.1756\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1485 - val_loss: 0.1752\n",
      "0.418573501596919\n",
      "(?, 7, 111)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.4083 - val_loss: 0.1919\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1709 - val_loss: 0.1807\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1562 - val_loss: 0.1761\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1478 - val_loss: 0.1766\n",
      "0.42027747960580936\n",
      "Score with params [111, 85, 16, 66, 52, 6, 52, 33, 5, 42, 50, 21, 0.011142938740321264, 0.2255796072604641, 0.00999383270437937, 0.08834218424976908, 0.19591734576254574, 0.07188889279386432, 0.09617870616723258, 0.1377322365611541, 0.08804758892525956, 0.45911773318107246, 0.08672885505101717, 0.2825944333024377, 212, 0.005450063382212791, 0.008325472748533794] is 0.4165413447549714\n",
      "(?, 7, 57)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.3992 - val_loss: 0.1899\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1729 - val_loss: 0.1809\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1609 - val_loss: 0.1777\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1541 - val_loss: 0.1769\n",
      "0.4205461418105474\n",
      "(?, 7, 57)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.3881 - val_loss: 0.1970\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1728 - val_loss: 0.1831\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1608 - val_loss: 0.1799\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1540 - val_loss: 0.1786\n",
      "0.4225697233701061\n",
      "(?, 7, 57)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.3970 - val_loss: 0.1969\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1754 - val_loss: 0.1826\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1637 - val_loss: 0.1797\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1573 - val_loss: 0.1785\n",
      "0.4224434771677402\n",
      "(?, 7, 57)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.3898 - val_loss: 0.1904\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1742 - val_loss: 0.1833\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1621 - val_loss: 0.1804\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1554 - val_loss: 0.1793\n",
      "0.4234865021852877\n",
      "(?, 7, 57)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.3917 - val_loss: 0.1904\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1744 - val_loss: 0.1825\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1622 - val_loss: 0.1786\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1553 - val_loss: 0.1776\n",
      "0.42136743082061573\n",
      "(?, 7, 57)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.3891 - val_loss: 0.1927\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1738 - val_loss: 0.1834\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1617 - val_loss: 0.1811\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1548 - val_loss: 0.1806\n",
      "0.4249382134681367\n",
      "(?, 7, 57)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.3881 - val_loss: 0.1944\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1738 - val_loss: 0.1826\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1616 - val_loss: 0.1799\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1548 - val_loss: 0.1792\n",
      "0.4232904177455011\n",
      "(?, 7, 57)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.3914 - val_loss: 0.1898\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1726 - val_loss: 0.1800\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1605 - val_loss: 0.1794\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1538 - val_loss: 0.1767\n",
      "0.4204076363394694\n",
      "(?, 7, 57)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.3931 - val_loss: 0.1914\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1737 - val_loss: 0.1828\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1618 - val_loss: 0.1800\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1551 - val_loss: 0.1796\n",
      "0.42381862173326734\n",
      "(?, 7, 57)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.3904 - val_loss: 0.1914\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1734 - val_loss: 0.1842\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1613 - val_loss: 0.1819\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.1545 - val_loss: 0.1816\n",
      "0.4260900178437202\n",
      "Score with params [57, 74, 81, 52, 26, 26, 47, 44, 5, 10, 59, 24, 0.14326544082371315, 0.14470304647360058, 0.09159568100355843, 0.11730258696201665, 0.004021509237498711, 0.16578800584347264, 0.0009390952385094133, 0.13556330735924604, 0.02700079731921649, 0.3675970110612975, 0.3848754180469754, 0.12437657175997904, 83, 0.00714927758050371, 0.013544537425127078] is 0.42289937577856895\n",
      "(?, 7, 68)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 21s 14us/step - loss: 0.5516 - val_loss: 0.2026\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1711 - val_loss: 0.1776\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1538 - val_loss: 0.1752\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1439 - val_loss: 0.1748\n",
      "0.4181438029665993\n",
      "(?, 7, 68)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 21s 14us/step - loss: 0.5177 - val_loss: 0.2045\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 21s 14us/step - loss: 0.1716 - val_loss: 0.1843\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1540 - val_loss: 0.1761\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1442 - val_loss: 0.1737\n",
      "0.4168253584689968\n",
      "(?, 7, 68)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 21s 14us/step - loss: 0.5311 - val_loss: 0.1930\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1707 - val_loss: 0.1834\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1536 - val_loss: 0.1781\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 21s 14us/step - loss: 0.1440 - val_loss: 0.1742\n",
      "0.41738278570907694\n",
      "(?, 7, 68)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 21s 14us/step - loss: 0.5332 - val_loss: 0.1952\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1706 - val_loss: 0.1774\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 21s 14us/step - loss: 0.1534 - val_loss: 0.1754\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 21s 14us/step - loss: 0.1438 - val_loss: 0.1733\n",
      "0.4163147156936431\n",
      "(?, 7, 68)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 21s 14us/step - loss: 0.5439 - val_loss: 0.1977\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1760 - val_loss: 0.1880\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1587 - val_loss: 0.1767\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1489 - val_loss: 0.1750\n",
      "0.4183673202133811\n",
      "(?, 7, 68)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 21s 14us/step - loss: 0.5346 - val_loss: 0.2003\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1719 - val_loss: 0.1789\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1547 - val_loss: 0.1773\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1450 - val_loss: 0.1750\n",
      "0.41837235385991944\n",
      "(?, 7, 68)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 21s 14us/step - loss: 0.5498 - val_loss: 0.1959\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1714 - val_loss: 0.1807\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1543 - val_loss: 0.1760\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1446 - val_loss: 0.1744\n",
      "0.41758519237123165\n",
      "(?, 7, 68)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 21s 14us/step - loss: 0.5304 - val_loss: 0.1882\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1722 - val_loss: 0.1778\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1551 - val_loss: 0.1751\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1453 - val_loss: 0.1733\n",
      "0.41626738263477203\n",
      "(?, 7, 68)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 21s 14us/step - loss: 0.5519 - val_loss: 0.1927\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1723 - val_loss: 0.1803\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1547 - val_loss: 0.1784\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1448 - val_loss: 0.1751\n",
      "0.41844508545895726\n",
      "(?, 7, 68)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 21s 14us/step - loss: 0.5382 - val_loss: 0.1985\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1723 - val_loss: 0.1800\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1550 - val_loss: 0.1778\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1453 - val_loss: 0.1800\n",
      "0.42429643946011203\n",
      "Score with params [68, 83, 77, 30, 112, 8, 7, 49, 5, 30, 21, 53, 0.13508781624004723, 0.12489813782748907, 0.1566091659729755, 0.19308324410398545, 0.1176930170137499, 0.1319336824138826, 0.10664125078332966, 0.046106604701265985, 0.039486929345022924, 0.3094042822956786, 0.18994700616357685, 0.23506609462788264, 351, 0.010582255987404907, 0.005061086500946217] is 0.4182057242129699\n",
      "(?, 7, 95)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.4930 - val_loss: 0.1950\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1804 - val_loss: 0.1819\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1684 - val_loss: 0.1787\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1619 - val_loss: 0.1760\n",
      "0.4195254532884997\n",
      "(?, 7, 95)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.4857 - val_loss: 0.2052\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1804 - val_loss: 0.1863\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1681 - val_loss: 0.1822\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1617 - val_loss: 0.1785\n",
      "0.42251294990109933\n",
      "(?, 7, 95)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.5058 - val_loss: 0.1989\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1806 - val_loss: 0.1862\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1684 - val_loss: 0.1814\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1618 - val_loss: 0.1775\n",
      "0.4212524862483978\n",
      "(?, 7, 95)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.4989 - val_loss: 0.1951\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1805 - val_loss: 0.1855\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1685 - val_loss: 0.1797\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1619 - val_loss: 0.1775\n",
      "0.4213493745403769\n",
      "(?, 7, 95)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.4966 - val_loss: 0.1945\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1814 - val_loss: 0.1844\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1693 - val_loss: 0.1807\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1628 - val_loss: 0.1781\n",
      "0.42197340448440046\n",
      "(?, 7, 95)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.5057 - val_loss: 0.2001\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1807 - val_loss: 0.1840\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1687 - val_loss: 0.1811\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1621 - val_loss: 0.1788\n",
      "0.4228453912248032\n",
      "(?, 7, 95)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.4953 - val_loss: 0.1989\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1807 - val_loss: 0.1837\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1684 - val_loss: 0.1800\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1619 - val_loss: 0.1779\n",
      "0.42173075237770064\n",
      "(?, 7, 95)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.4886 - val_loss: 0.1961\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1798 - val_loss: 0.1812\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1678 - val_loss: 0.1786\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1613 - val_loss: 0.1765\n",
      "0.42009495739581176\n",
      "(?, 7, 95)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.5127 - val_loss: 0.2013\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1810 - val_loss: 0.1859\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1689 - val_loss: 0.1795\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1623 - val_loss: 0.1790\n",
      "0.42313621443156973\n",
      "(?, 7, 95)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 18s 13us/step - loss: 0.4928 - val_loss: 0.1980\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1809 - val_loss: 0.1867\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1689 - val_loss: 0.1822\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 18s 12us/step - loss: 0.1625 - val_loss: 0.1821\n",
      "0.426788180158252\n",
      "Score with params [95, 57, 34, 56, 70, 19, 15, 42, 7, 9, 39, 62, 0.13064016397142675, 0.3260516350008445, 0.21570921771698703, 0.17930931917021264, 0.07351237400957931, 0.08717298505312537, 0.17838467100313446, 0.16123879780921718, 0.07038885835403665, 0.05011344365615057, 0.3677930454978695, 0.35712064977455577, 485, 0.003942827960553919, 0.013153764803155] is 0.42212516363941255\n",
      "(?, 7, 18)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 15s 11us/step - loss: 0.3948 - val_loss: 0.1985\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 15s 10us/step - loss: 0.1757 - val_loss: 0.1811\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 15s 10us/step - loss: 0.1607 - val_loss: 0.1793\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 15s 10us/step - loss: 0.1519 - val_loss: 0.1755\n",
      "0.4189767745309324\n",
      "(?, 7, 18)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 15s 11us/step - loss: 0.3935 - val_loss: 0.2086\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 15s 10us/step - loss: 0.1758 - val_loss: 0.1846\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 15s 10us/step - loss: 0.1605 - val_loss: 0.1804\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 15s 10us/step - loss: 0.1519 - val_loss: 0.1761\n",
      "0.4196606958065555\n",
      "(?, 7, 18)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 15s 11us/step - loss: 0.3930 - val_loss: 0.1973\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1444619/1444619 [==============================] - 15s 10us/step - loss: 0.1758 - val_loss: 0.1837\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 15s 10us/step - loss: 0.1607 - val_loss: 0.1793\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 15s 10us/step - loss: 0.1521 - val_loss: 0.1764\n",
      "0.4200361295292199\n",
      "(?, 7, 18)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 15s 11us/step - loss: 0.3885 - val_loss: 0.1978\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 15s 10us/step - loss: 0.1751 - val_loss: 0.1808\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 15s 10us/step - loss: 0.1602 - val_loss: 0.1794\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 15s 10us/step - loss: 0.1518 - val_loss: 0.1764\n",
      "0.41995431627884244\n",
      "(?, 7, 18)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 15s 11us/step - loss: 0.3968 - val_loss: 0.1929\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 15s 10us/step - loss: 0.1765 - val_loss: 0.1878\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 15s 10us/step - loss: 0.1615 - val_loss: 0.1784\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 15s 11us/step - loss: 0.1526 - val_loss: 0.1785\n",
      "0.42245060623507136\n",
      "(?, 7, 18)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.3941 - val_loss: 0.2028\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 15s 11us/step - loss: 0.1764 - val_loss: 0.1831\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 15s 10us/step - loss: 0.1614 - val_loss: 0.1826\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 15s 11us/step - loss: 0.1529 - val_loss: 0.1773\n",
      "0.4210926884364594\n",
      "(?, 7, 18)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 15s 11us/step - loss: 0.3861 - val_loss: 0.2058\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 15s 10us/step - loss: 0.1761 - val_loss: 0.1836\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 15s 10us/step - loss: 0.1612 - val_loss: 0.1807\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 15s 10us/step - loss: 0.1527 - val_loss: 0.1773\n",
      "0.421101299955237\n",
      "(?, 7, 18)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.3858 - val_loss: 0.1980\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 15s 10us/step - loss: 0.1762 - val_loss: 0.1800\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 15s 10us/step - loss: 0.1612 - val_loss: 0.1772\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 15s 10us/step - loss: 0.1525 - val_loss: 0.1757\n",
      "0.4191376741267691\n",
      "(?, 7, 18)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 15s 11us/step - loss: 0.3908 - val_loss: 0.2000\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 15s 10us/step - loss: 0.1761 - val_loss: 0.1852\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 15s 11us/step - loss: 0.1612 - val_loss: 0.1791\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 15s 10us/step - loss: 0.1527 - val_loss: 0.1788\n",
      "0.4228066900827008\n",
      "(?, 7, 18)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 16s 11us/step - loss: 0.3916 - val_loss: 0.1966\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 15s 10us/step - loss: 0.1770 - val_loss: 0.1856\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 15s 10us/step - loss: 0.1618 - val_loss: 0.1826\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 15s 10us/step - loss: 0.1531 - val_loss: 0.1824\n",
      "0.4270956762474698\n",
      "Score with params [18, 43, 39, 69, 67, 18, 24, 57, 7, 11, 43, 47, 0.013833399091027612, 0.3487143865722819, 0.22677134133903448, 0.1444111198940696, 0.17327646518572587, 0.1951043010005772, 0.17116066847852224, 0.002342816837000395, 0.03599780644783639, 0.36499528121202907, 0.06865187090457622, 0.2605183031020647, 97, 0.0064068065181371175, 0.00351286365284998] is 0.42123759217093726\n",
      "(?, 7, 16)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.3669 - val_loss: 0.2035\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1761 - val_loss: 0.1812\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1634 - val_loss: 0.1792\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1561 - val_loss: 0.1786\n",
      "0.42258043302134196\n",
      "(?, 7, 16)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.3676 - val_loss: 0.2160\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1771 - val_loss: 0.1864\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1644 - val_loss: 0.1823\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1574 - val_loss: 0.1804\n",
      "0.424719339397301\n",
      "(?, 7, 16)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.3771 - val_loss: 0.1990\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1763 - val_loss: 0.1856\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1638 - val_loss: 0.1815\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1569 - val_loss: 0.1796\n",
      "0.42382765251915905\n",
      "(?, 7, 16)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.3580 - val_loss: 0.1931\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1764 - val_loss: 0.1852\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1641 - val_loss: 0.1804\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1572 - val_loss: 0.1790\n",
      "0.4231261627637663\n",
      "(?, 7, 16)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.3550 - val_loss: 0.1949\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1750 - val_loss: 0.1863\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1623 - val_loss: 0.1810\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1551 - val_loss: 0.1772\n",
      "0.42098397549673366\n",
      "(?, 7, 16)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.3588 - val_loss: 0.1961\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 20s 13us/step - loss: 0.1752 - val_loss: 0.1822\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1624 - val_loss: 0.1809\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1553 - val_loss: 0.1789\n",
      "0.4229681627256053\n",
      "(?, 7, 16)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.3651 - val_loss: 0.1993\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1755 - val_loss: 0.1848\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1625 - val_loss: 0.1808\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1553 - val_loss: 0.1796\n",
      "0.42377398943864003\n",
      "(?, 7, 16)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.3588 - val_loss: 0.1904\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1758 - val_loss: 0.1825\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1634 - val_loss: 0.1799\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1566 - val_loss: 0.1764\n",
      "0.4200400239841267\n",
      "(?, 7, 16)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.3564 - val_loss: 0.2020\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1743 - val_loss: 0.1834\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1614 - val_loss: 0.1807\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 19s 13us/step - loss: 0.1543 - val_loss: 0.1790\n",
      "0.42308461822082766\n",
      "(?, 7, 16)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.3818 - val_loss: 0.1974\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1764 - val_loss: 0.1877\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1643 - val_loss: 0.1830\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1574 - val_loss: 0.1851\n",
      "0.4302260673985404\n",
      "Score with params [16, 91, 50, 85, 69, 20, 12, 5, 3, 56, 43, 64, 0.010074011351508762, 0.34954903735925563, 0.4961981994443164, 0.05345250757629566, 0.1358181231254828, 0.17285628853724896, 0.15016884912568634, 0.19289795355788622, 0.055424243508444035, 0.10619524944515432, 0.08897728975472752, 0.10937468686838595, 108, 0.01161416033564074, 0.00786642332064594] is 0.4235409096778948\n",
      "(?, 7, 107)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.4239 - val_loss: 0.1902\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1742 - val_loss: 0.1812\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1621 - val_loss: 0.1782\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1554 - val_loss: 0.1766\n",
      "0.4202003688221998\n",
      "(?, 7, 107)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 21s 14us/step - loss: 0.4137 - val_loss: 0.1960\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1729 - val_loss: 0.1826\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1606 - val_loss: 0.1796\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1535 - val_loss: 0.1776\n",
      "0.42138760099897765\n",
      "(?, 7, 107)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1444619/1444619 [==============================] - 21s 14us/step - loss: 0.4114 - val_loss: 0.1946\n",
      "Epoch 2/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1732 - val_loss: 0.1810\n",
      "Epoch 3/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1604 - val_loss: 0.1797\n",
      "Epoch 4/4\n",
      "1444619/1444619 [==============================] - 20s 14us/step - loss: 0.1534 - val_loss: 0.1766\n",
      "0.42027825977267946\n",
      "(?, 7, 107)\n",
      "(?, ?)\n",
      "Train on 1444619 samples, validate on 37042 samples\n",
      "Epoch 1/4\n",
      "1076000/1444619 [=====================>........] - ETA: 5s - loss: 0.4869"
     ]
    }
   ],
   "source": [
    "space = [Integer(16, 128), #name='name_embed'),\n",
    "         Integer(16, 128),# name='desc_embed'),\n",
    "         Integer(16, 96),# name='desc2_embed'),\n",
    "         Integer(16, 128),# name='brand_embed'),\n",
    "         Integer(16, 128),# name='cat_embed'),\n",
    "         Integer(4, 32),# name='cat1_embed'),\n",
    "         Integer(4, 64),# name='cat2_embed'),\n",
    "         Integer(4, 64),# name='cat3_embed'),\n",
    "         Integer(2, 8),# name='cond_embed'),\n",
    "         Integer(8, 64),# name='brandcat_embed'),\n",
    "         Integer(8, 64),# name='catship_embed'),\n",
    "         Integer(8, 64),# name='itemname_embed'),\n",
    "         Real(0.00, 0.2, \"uniform\"), #name_drop'),\n",
    "         Real(0.00, 0.5, \"uniform\"), #desc_drop'),\n",
    "         Real(0.00, 0.5, \"uniform\"), #desc2_drop'),\n",
    "         Real(0.00, 0.2, \"uniform\"), #brand_drop),\n",
    "         Real(0.00, 0.2, \"uniform\"), #cat_drop),\n",
    "         Real(0.00, 0.2, \"uniform\"), #cat1_drop),\n",
    "         Real(0.00, 0.2, \"uniform\"), #cat2_drop),\n",
    "         Real(0.00, 0.2, \"uniform\"), #cat3_drop),\n",
    "         Real(0.00, 0.1, \"uniform\"), #cond_drop),\n",
    "         Real(0.00, 0.5, \"uniform\"), #brandcat_drop),\n",
    "         Real(0.00, 0.4, \"uniform\"), #catship_drop,\n",
    "         Real(0.00, 0.5, \"uniform\"), #itemname),\n",
    "         Integer(64, 512),# 'dense_dim'),\n",
    "         Real(0.002, 0.015, \"uniform\"), #lr_decay),\n",
    "         Real(0.001, 0.015, \"uniform\"), #lr_decay),\n",
    "        ]\n",
    "\n",
    "res_gp = gbrt_minimize(train_model, space, n_calls=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(seed):\n",
    "    config = tf.ConfigProto(intra_op_parallelism_threads=num_cores,\\\n",
    "            inter_op_parallelism_threads=num_cores, allow_soft_placement=True,\\\n",
    "            device_count = {'CPU' : num_CPU, 'GPU' : num_GPU})\n",
    "    session = tf.Session(config=config)\n",
    "    K.set_session(session)\n",
    "\n",
    "    #seed=1\n",
    "    batchsize=1000\n",
    "    epochs=3\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "\n",
    "\n",
    "    model = keras_mercari_model(seed)\n",
    "\n",
    "    train_idx, val_idx= cvlist[0]\n",
    "    X = [X_name, X_desc, X_desc2, X_itemname, X_cats[\"brand_name\"], X_cats[\"category_name\"],\n",
    "        X_cats[\"cat1\"], X_cats[\"cat2\"], X_cats[\"cat3\"], X_cats[\"brand_cat\"],\n",
    "        X_cats[\"category_shipping\"], X_cats[\"item_condition_id\"], X_conts]\n",
    "\n",
    "    X_test = [X_test_name, X_test_desc, X_test_desc2, X_test_itemname, X_test_cats[\"brand_name\"], X_test_cats[\"category_name\"],\n",
    "        X_test_cats[\"cat1\"], X_test_cats[\"cat2\"], X_test_cats[\"cat3\"], X_test_cats[\"brand_cat\"],\n",
    "        X_test_cats[\"category_shipping\"], X_test_cats[\"item_condition_id\"], X_test_conts]\n",
    "\n",
    "    X_tr = [x[train_idx] for x in X]\n",
    "    X_val = [x[val_idx] for x in X]\n",
    "\n",
    "    val_store = TestCallback(X_val, X_test)\n",
    "\n",
    "    model.fit(X_tr, y[train_idx], batch_size=batchsize, epochs=epochs,\n",
    "                                   verbose=1,\n",
    "                                  validation_data=(X_val, y[val_idx]), shuffle=True,\n",
    "                                  #callbacks= [val_store]\n",
    "             )\n",
    "    y_val1 = y[val_idx,0]\n",
    "    y_preds1 = model.predict(X_val)[:, 0]\n",
    "    print(np.sqrt(metrics.mean_squared_error(y_val1, y_preds1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2179"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 7, 50)\n",
      "(?, 7, 50)\n",
      "(?, 7, 50)\n",
      "(?, ?)\n",
      "(?, ?)\n",
      "(?, ?)\n",
      "Train on 1408408 samples, validate on 74127 samples\n",
      "Train on 1408408 samples, validate on 74127 samples\n",
      "Train on 1408408 samples, validate on 74127 samples\n",
      "Epoch 1/3\n",
      "Epoch 1/3\n",
      "Epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    pool = Pool(processes=4)\n",
    "    lr1s = [0.005, 0.0045, 0.0055, 0.005]\n",
    "    lr2s = [0.001, 0.001, 0.001, 0.0009]\n",
    "    results = [pool.apply_async(train_model, args=(np.random.randint(10000000),)) for i in range(3)]\n",
    "    output = [p.get() for p in results]\n",
    "    print(output[0][:5], output[1][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohsin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>brand_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>988663</th>\n",
       "      <td>Funko Pop White Lantern Batman NYCC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247109</th>\n",
       "      <td>VS 34C Bras</td>\n",
       "      <td>Victoria's Secret</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998327</th>\n",
       "      <td>Elta MD UV CLear Facial Sunscreen</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66939</th>\n",
       "      <td>VS Pink Bundle XSS 14 medium</td>\n",
       "      <td>PINK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095979</th>\n",
       "      <td>LuLaRoe TC Kaleidoscope Rainbow</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420643</th>\n",
       "      <td>Jessica Simpson Purse Clutch 4 AMY</td>\n",
       "      <td>Jessica Simpson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995161</th>\n",
       "      <td>EYESHADOW</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730406</th>\n",
       "      <td>LF Steve Madden limited edition combat</td>\n",
       "      <td>Steve Madden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123409</th>\n",
       "      <td>DORBZ Teen Titans Robin chase</td>\n",
       "      <td>Funko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735976</th>\n",
       "      <td>Lilly Pulitzer XL Dress</td>\n",
       "      <td>Lilly Pulitzer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           name         brand_name\n",
       "988663      Funko Pop White Lantern Batman NYCC                NaN\n",
       "1247109                             VS 34C Bras  Victoria's Secret\n",
       "998327        Elta MD UV CLear Facial Sunscreen                NaN\n",
       "66939              VS Pink Bundle XSS 14 medium               PINK\n",
       "1095979         LuLaRoe TC Kaleidoscope Rainbow                NaN\n",
       "420643       Jessica Simpson Purse Clutch 4 AMY    Jessica Simpson\n",
       "995161                                EYESHADOW                NaN\n",
       "730406   LF Steve Madden limited edition combat       Steve Madden\n",
       "1123409          DORBZ Teen Titans Robin chase               Funko\n",
       "735976                  Lilly Pulitzer XL Dress     Lilly Pulitzer"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.loc[train_data.name.str.contains('([A-Z]){2,}'), ['name', 'brand_name']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1401, 2)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UD - Urban decay\n",
    "#Eyeshadow\n",
    "#NYCC\n",
    "# MD - medium\n",
    "# FR SHP - same seller - same prices\n",
    "# hundreds of happy buyers\n",
    "# item description has price\n",
    "train_data.loc[train_data.item_description.str.contains('$ \\d'),  ['item_description','price']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.4858934169279"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.loc[train_data.name.str.contains(\"limited edition\"), 'price'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4134122927730147"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = val_store.val_preds\n",
    "y_preds = np.concatenate(y_preds, axis=1)\n",
    "y_preds_mean = y_preds[:, 0]*0.15 + y_preds[:, 1]*0.25 + y_preds[:, 2]*0.6\n",
    "rmse(y_val1, y_preds_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import skopt\n",
    "from skopt import gp_minimize, gbrt_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "space = [Integer(16, 512), #name='name_embed'),\n",
    "         Integer(16, 512),# name='desc_embed'),\n",
    "         Integer(16, 256),# name='brand_embed'),\n",
    "         Integer(16, 256),# name='cat_embed'),\n",
    "         Integer(64, 512),# name='dense1_dim'),\n",
    "         Integer(32, 256),# name='dense2_dim'),\n",
    "         Real(0.001, 0.01, \"uniform\"), #name='lr1'),\n",
    "         Real(1e-5, 0.005, \"uniform\"), #name='lr2'),\n",
    "         Real(1e-5, 1e-3, \"log-uniform\"), #name='lr3'),\n",
    "         Real(0.001, 0.4, \"uniform\"), #name='name_drop'),\n",
    "         Real(0.001, 0.5, \"uniform\"), #name='desc_drop'),\n",
    "         Real(0.0, 0.05, \"uniform\"), #name='dense_drop'),\n",
    "         Integer(10, 30), #name='name_filters3'),\n",
    "         Integer(10, 30), #name='name_filters2'),\n",
    "         Integer(10, 30), #name='desc_filters3'),\n",
    "         Integer(10, 30), #name='desc_filters2'),\n",
    "         Integer(256, 5000), #name='batch_size'),\n",
    "        ]\n",
    "\n",
    "res_gp = gbrt_minimize(train_tf, space, n_calls=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 7, 50)\n",
      "(?, ?)\n",
      "Train on 1407578 samples, validate on 74083 samples\n",
      "Epoch 1/4\n",
      "1407578/1407578 [==============================] - 27s 19us/step - loss: 0.5512 - val_loss: 0.4601\n",
      "Epoch 2/4\n",
      "1407578/1407578 [==============================] - 25s 18us/step - loss: 0.4263 - val_loss: 0.4424\n",
      "Epoch 3/4\n",
      "1407578/1407578 [==============================] - 25s 18us/step - loss: 0.4000 - val_loss: 0.4389\n",
      "Epoch 4/4\n",
      "1407578/1407578 [==============================] - 25s 18us/step - loss: 0.3844 - val_loss: 0.4378\n"
     ]
    }
   ],
   "source": [
    "normll = QuantileTransformer(output_distribution='normal')\n",
    "ynorm = normll.fit_transform(y)\n",
    "seed=1\n",
    "batchsize=1000\n",
    "epochs=4\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "callbacks= [ModelCheckpoint(\"embed_NN_\"+str(seed)+\".check\", save_best_only=True, verbose=1)]\n",
    "model = keras_mercari_model()\n",
    "\n",
    "train_idx, val_idx= cvlist[1]\n",
    "X = [X_name, X_desc, X_desc2, X_itemname, X_cats[\"brand_name\"], X_cats[\"category_name\"],\n",
    "    X_cats[\"cat1\"], X_cats[\"cat2\"], X_cats[\"cat3\"], X_cats[\"brand_cat\"],\n",
    "    X_cats[\"category_shipping\"], X_cats[\"item_condition_id\"], X_conts]\n",
    "\n",
    "X_tr = [x[train_idx] for x in X]\n",
    "X_val = [x[val_idx] for x in X]\n",
    "model.fit(X_tr, ynorm[train_idx], batch_size=batchsize, epochs=epochs,\n",
    "                               verbose=1,\n",
    "                              validation_data=(X_val, ynorm[val_idx]), shuffle=True,\n",
    "         )\n",
    "y_val2 = y[val_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42093463775873474\n"
     ]
    }
   ],
   "source": [
    "y_preds2 = normll.inverse_transform(model.predict(X_val))\n",
    "print(np.sqrt(metrics.mean_squared_error(y_val2, y_preds2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40684002989865675\n"
     ]
    }
   ],
   "source": [
    "ymean = y_preds1 *0.7 + y_preds2[:, 0]*0.3 \n",
    "print(np.sqrt(metrics.mean_squared_error(y_val2, ymean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 7, 50)\n",
      "(?, ?)\n",
      "Train on 1407578 samples, validate on 74083 samples\n",
      "Epoch 1/4\n",
      "1407578/1407578 [==============================] - 27s 19us/step - loss: 0.5149 - val_loss: 0.4240\n",
      "Epoch 2/4\n",
      "1407578/1407578 [==============================] - 25s 18us/step - loss: 0.3908 - val_loss: 0.4098\n",
      "Epoch 3/4\n",
      "1407578/1407578 [==============================] - 25s 18us/step - loss: 0.3647 - val_loss: 0.4033\n",
      "Epoch 4/4\n",
      "1407578/1407578 [==============================] - 25s 18us/step - loss: 0.3495 - val_loss: 0.4019\n"
     ]
    }
   ],
   "source": [
    "cat_means = train_data.groupby(\"category_name\")[\"price\"].mean()\n",
    "train_data['cat_price'] = np.log1p(train_data.category_name.map(cat_means).fillna(17))\n",
    "#yrel = ((train_data['price'] - train_data['cat_price'])/train_data['cat_price']).values.reshape(-1,1)\n",
    "yrel = ((y[:, 0] - train_data['cat_price'])/train_data['cat_price']).values.reshape(-1,1)\n",
    "normll = QuantileTransformer(output_distribution='normal')\n",
    "ynorm2 = normll.fit_transform(yrel)\n",
    "seed=1\n",
    "batchsize=1000\n",
    "epochs=4\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "callbacks= [ModelCheckpoint(\"embed_NN_\"+str(seed)+\".check\", save_best_only=True, verbose=1)]\n",
    "model = keras_mercari_model()\n",
    "\n",
    "train_idx, val_idx= cvlist[1]\n",
    "X = [X_name, X_desc, X_desc2, X_itemname, X_cats[\"brand_name\"], X_cats[\"category_name\"],\n",
    "    X_cats[\"cat1\"], X_cats[\"cat2\"], X_cats[\"cat3\"], X_cats[\"brand_cat\"],\n",
    "    X_cats[\"category_shipping\"], X_cats[\"item_condition_id\"], X_conts]\n",
    "\n",
    "X_tr = [x[train_idx] for x in X]\n",
    "X_val = [x[val_idx] for x in X]\n",
    "model.fit(X_tr, ynorm2[train_idx], batch_size=batchsize, epochs=epochs,\n",
    "                               verbose=1,\n",
    "                              validation_data=(X_val, ynorm2[val_idx]), shuffle=True,\n",
    "                              )\n",
    "y_val3 = y[val_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4123020256231108\n"
     ]
    }
   ],
   "source": [
    "#y_preds3 = np.log1p((normll.inverse_transform(model.predict(X_val))[:,0] + 1)* train_data['cat_price'].values[val_idx])\n",
    "y_preds3 = (normll.inverse_transform(model.predict(X_val))[:,0] + 1)* train_data['cat_price'].values[val_idx]\n",
    "\n",
    "print(np.sqrt(metrics.mean_squared_error(y_val3, y_preds3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40476890048691955\n"
     ]
    }
   ],
   "source": [
    "ymean = y_preds1 *0.4 + y_preds2[:, 0]*.2 + y_preds3 *0.4\n",
    "print(np.sqrt(metrics.mean_squared_error(y_val2, ymean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.96827273],\n",
       "       [0.96827273, 1.        ]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(y_preds1, y_preds2[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#obs:\n",
    "# desc2 and itemname contribute 0.001 - 0.002\n",
    "# drop rate on desc helps but not on desc2 and itemname\n",
    "#try out regularization in desc\n",
    "#PRelu giving 0.001 improvement\n",
    "#clipping improving score a bit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
