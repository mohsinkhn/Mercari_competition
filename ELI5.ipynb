{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook is based on the [kernel](https://www.kaggle.com/lopuhin/eli5-for-mercari/notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import re\n",
    "pd.set_option(\"max_colwidth\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_table('../input/train.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.log1p(df_train['price'])\n",
    "df_train['category_name'] = df_train['category_name'].fillna('Other').astype(str)\n",
    "df_train['brand_name'] = df_train['brand_name'].fillna('missing').astype(str)\n",
    "df_train['shipping'] = df_train['shipping'].astype(str)\n",
    "df_train['item_condition_id'] = df_train['item_condition_id'].astype(str)\n",
    "df_train['item_description'] = df_train['item_description'].fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_normalization(text_series):\n",
    "    text_series = text_series.str.lower()\n",
    "    text_series = text_series.str.replace(r'[\\W+]', \" \")\n",
    "    text_series =  text_series.str.replace(r't-shirt|t-shirts', \"tshirt\")\n",
    "    text_series =  text_series.str.replace(r'boot cut', \"bootcut\")\n",
    "    text_series =  text_series.str.replace(r'16 gb', \"16gb\")\n",
    "    text_series =  text_series.str.replace(r'32 gb', \"32gb\")\n",
    "    text_series =  text_series.str.replace(r'64 gb', \"64gb\")\n",
    "    text_series =  text_series.str.replace(r'128 gb', \"12gb\")\n",
    "    text_series =  text_series.str.replace(r'250 gb', \"250gb\")\n",
    "    text_series =  text_series.str.replace(r'500 gb', \"500gb\")\n",
    "    text_series = text_series.str.replace(r'new with tags|nwt|new with tag|bnwt', \"new_with_tags\")\n",
    "    text_series = text_series.str.replace(r'never wore|never used|neve used|never worn|never been worn', \"never_used\")\n",
    "    text_series = text_series.str.replace(r'nwot|new without tag|new without tags', \"new_without_tags\")\n",
    "    text_series = text_series.str.replace(r'your best offer|or best offer|orbestoffer|obo', \"or_best_offer\")\n",
    "    text_series = text_series.str.replace(r'brand new', \"brand_new\")\n",
    "    text_series = text_series.str.replace(r'hatchimals', \"hatchimal\")\n",
    "    text_series = text_series.str.replace(r'hover board', \"hoverboard\")\n",
    "    text_series = text_series.str.replace(r'weighs', \"weights\")\n",
    "    text_series = text_series.str.replace(r'mk purses', \"mk_purses\")\n",
    "    text_series = text_series.str.replace(r'pop stack', \"pop_stack\")\n",
    "    text_series = text_series.str.replace(r'alexis brittar', \"alexis_brittar\")\n",
    "    text_series = text_series.str.replace(r'14k|14kt|14 carat|14 k|14-karat|14-carat|14 gold', \"14_carat\")\n",
    "    text_series = text_series.str.replace(r'10k|10kt|10 carat|10 k|10-karat|10-carat|10 gold', \"10_carat\")\n",
    "    text_series = text_series.str.replace(r'24k|24kt|24 carat|24 k|24-karat|24-carat|24 gold', \"24_carat\")\n",
    "    text_series = text_series.str.replace(r'18k|18kt|18 carat|18 k|18-karat|18-carat|18 gold', \"18_carat\")\n",
    "    text_series = text_series.str.replace(r'dock a tot', \"dockatot\")\n",
    "    text_series = text_series.str.replace(r'007 taz', \"007taz\")\n",
    "    text_series = text_series.str.replace(r'playstation vr', \"playstation_vr\")\n",
    "    text_series = text_series.str.replace(r'lil', \"playstation_vr\")\n",
    "    return text_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 47s, sys: 1.36 s, total: 1min 48s\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_train.item_description = text_normalization(df_train.item_description)\n",
    "df_train.name = text_normalization(df_train.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 57s, sys: 6.37 s, total: 3min 4s\n",
      "Wall time: 3min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "default_preprocessor = CountVectorizer().build_preprocessor()\n",
    "def build_preprocessor(field):\n",
    "    field_idx = list(df_train.columns).index(field)\n",
    "    return lambda x: default_preprocessor(x[field_idx])\n",
    "    \n",
    "vectorizer = FeatureUnion([\n",
    "    ('name', CountVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        min_df = 5,\n",
    "        preprocessor=build_preprocessor('name'))),\n",
    "    ('category_name', CountVectorizer(\n",
    "        token_pattern='.+',\n",
    "        min_df = 5,\n",
    "        preprocessor=build_preprocessor('category_name'))),\n",
    "    ('brand_name', CountVectorizer(\n",
    "        token_pattern='.+',\n",
    "        preprocessor=build_preprocessor('brand_name'))),\n",
    "    ('shipping', CountVectorizer(\n",
    "        token_pattern='\\d+',\n",
    "        min_df = 5,\n",
    "        preprocessor=build_preprocessor('shipping'))),\n",
    "    ('item_condition_id', CountVectorizer(\n",
    "        token_pattern='\\d+',\n",
    "        preprocessor=build_preprocessor('item_condition_id'))),\n",
    "    ('item_description', TfidfVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        min_df = 5,\n",
    "        preprocessor=build_preprocessor('item_description'))),\n",
    "])\n",
    "X_train = vectorizer.fit_transform(df_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1482535, 787838)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "# the following functions allow for a parallelized batch generator\n",
    "class threadsafe_iter(object):\n",
    "    \"\"\"\n",
    "    Takes an iterator/generator and makes it thread-safe by\n",
    "    serializing call to the `next` method of given iterator/generator.\n",
    "    \"\"\"\n",
    "    def __init__(self, it):\n",
    "        self.it = it\n",
    "        self.lock = threading.Lock()\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        with self.lock:\n",
    "            return next(self.it)\n",
    "\n",
    "def threadsafe_generator(f):\n",
    "    \"\"\"\n",
    "    A decorator that takes a generator function and makes it thread-safe.\n",
    "    \"\"\"\n",
    "    def g(*a, **kw):\n",
    "        return threadsafe_iter(f(*a, **kw))\n",
    "    return g\n",
    "\n",
    "@threadsafe_generator\n",
    "def batch_generator(X, y, batch_size):\n",
    "    samples_per_epoch = X.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    shuffle_index = np.arange(np.shape(y)[0])\n",
    "    np.random.shuffle(shuffle_index)\n",
    "    X =  X[shuffle_index, :]\n",
    "    y =  y[shuffle_index]\n",
    "    while 1:\n",
    "        index_batch = shuffle_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[index_batch,:].tocsr()\n",
    "        y_batch = y[index_batch]\n",
    "        counter += 1\n",
    "        yield(X_batch,y_batch)\n",
    "        if (counter < number_of_batches):\n",
    "            np.random.shuffle(shuffle_index)\n",
    "            counter=0\n",
    "# def batch_generator(X_data, y_data, batch_size):\n",
    "#     samples_per_epoch = X_data.shape[0]\n",
    "#     number_of_batches = samples_per_epoch/batch_size\n",
    "#     counter=0\n",
    "#     index = np.arange(np.shape(y_data)[0])\n",
    "#     while 1:\n",
    "#         index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "#         X_batch = X_data[index_batch,:].tocsr()\n",
    "#         y_batch = y_data[index_batch]\n",
    "#         counter += 1\n",
    "#         yield np.array(X_batch),y_batch\n",
    "#         if (counter > number_of_batches):\n",
    "#             counter=0\n",
    "            \n",
    "            \n",
    "@threadsafe_generator\n",
    "def batch_generator_x(X_data,batch_size):\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    index = np.arange(np.shape(X_data)[0])\n",
    "    while 1:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:].todense()\n",
    "        counter += 1\n",
    "        yield np.array(X_batch)\n",
    "        if (counter > number_of_batches):\n",
    "            counter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of workers.... 8\n",
      "Epoch 1/2\n",
      "152/723 [=====>........................] - ETA: 7:46 - loss: 0.9981"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation, Input, Flatten, concatenate, Embedding, advanced_activations\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "import multiprocessing\n",
    "\n",
    "np.random.seed(11)\n",
    "\n",
    "n_workers = multiprocessing.cpu_count()\n",
    "print(\"Number of workers....\", n_workers)\n",
    "            \n",
    "def get_model():\n",
    "    \n",
    "    #Input\n",
    "    inp_layer = Input(shape = [X_train.shape[1]], sparse= True)\n",
    "\n",
    "    #main_layer\n",
    "   # main_l = BatchNormalization()(inp_layer)\n",
    "    main_l = Dense(60, activation = \"relu\", kernel_initializer = \"he_normal\")(inp_layer)\n",
    "#    main_l = Dropout(dr_1)(Dense(50, activation = \"relu\", kernel_initializer = \"he_normal\")(main_l))\n",
    "    main_l = BatchNormalization()(main_l)\n",
    "    #main_l = Dropout(dr_1)(Dense(2048, activation = \"relu\")(inp_layer))\n",
    "    main_l = Dropout(dr_2)(Dense(10, activation = \"relu\", kernel_initializer = \"he_normal\")(main_l))\n",
    "#    main_l = Dropout(dr_3)(Dense(64, activation = \"relu\", kernel_initializer = \"he_normal\")(main_l))\n",
    "    \n",
    "    #output\n",
    "    output = Dense(1, activation = \"linear\")(main_l)\n",
    "    \n",
    "    #model\n",
    "    model = Model(inp_layer, output)\n",
    "    \n",
    "    opt = optimizers.Adam(clipnorm=1.)\n",
    "    model.compile(loss = \"mse\", optimizer = opt)\n",
    "    \n",
    "    return model\n",
    "\n",
    "epochs = 2\n",
    "dr_1 = dr_2 = dr_3 = 0.15\n",
    "batch_size = 512*4\n",
    "model = get_model()\n",
    "K.set_value(model.optimizer.lr, 0.005)\n",
    "K.set_value(model.optimizer.decay, 0.005)\n",
    "\n",
    "history = model.fit_generator(generator=batch_generator(X_train, y_train, batch_size),\n",
    "                    workers=n_workers, \n",
    "                    steps_per_epoch=X_train.shape[0]//batch_size, \n",
    "                    max_queue_size=128,\n",
    "                    epochs=epochs, \n",
    "                    verbose=1,\n",
    "                   )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
