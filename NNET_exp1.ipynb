{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1\n",
    "* Tokenizer - Keras\n",
    "* Embeddings training from uniform; separate for each one\n",
    "* Optimizers and learning rate - test to check local cv vs lb\n",
    "* ** Extra features - Counts and iphone strings corrected **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohsin/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/mohsin/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import threading\n",
    "import multiprocessing\n",
    "\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "\n",
    "# from __future__ import print_function\n",
    "np.random.seed(786)  # for reproducibility\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D, ZeroPadding1D, AveragePooling1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier,  KerasRegressor\n",
    "#Some classes\n",
    "#Functions we need - Feature Selector, Fasttext_Estimator, Preprocessing Transformer, Binary_Encoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "from pandas.api.types import is_numeric_dtype, is_string_dtype\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    print(np.min(y_pred), np.max(y_pred))\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "rmse_sklearn = make_scorer(rmse, greater_is_better=False)\n",
    "\n",
    "def get_obj_cols(df):\n",
    "    \"\"\"Return columns with object dtypes\"\"\"\n",
    "    obj_cols = []\n",
    "    for idx, dt in enumerate(df.dtypes):\n",
    "        if dt == 'object':\n",
    "            obj_cols.append(df.columns.values[idx])\n",
    "\n",
    "    return obj_cols\n",
    "\n",
    "\n",
    "def convert_input(X):\n",
    "    \"\"\"if input not a dataframe convert it to one\"\"\"\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        if isinstance(X, list):\n",
    "            X = pd.DataFrame(np.array(X))\n",
    "        elif isinstance(X, (np.generic, np.ndarray)):\n",
    "            X = pd.DataFrame(X)\n",
    "        elif isinstance(X, csr_matrix):\n",
    "            X = pd.SparseDataFrame(X)\n",
    "        else:\n",
    "            raise ValueError('Unexpected input type: %s' % (str(type(X))))\n",
    "\n",
    "        #X = X.apply(lambda x: pd.to_numeric(x, errors='ignore'))\n",
    "    return X\n",
    "\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Class to do subset of features in sklearn pipeline\"\"\"\n",
    "    def __init__(self, cols=None, return_df=True, verbose=0):\n",
    "        self.cols = cols\n",
    "        self.return_df = return_df\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        #Do nothing\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        #if the input dataset isn't already a dataframe, convert it to one\n",
    "        X = X.copy(deep=True)\n",
    "        X = convert_input(X)\n",
    "        X = X.loc[:, self.col]\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Selecting columns are {}\".format(self.col))\n",
    "        if self.return_df:\n",
    "            return X\n",
    "        else:\n",
    "            return X.values\n",
    "        \n",
    "class TargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols=None, thresh=0, func=np.mean, add_to_orig=False):\n",
    "        self.cols = cols\n",
    "        self.thresh = thresh\n",
    "        self.func = func\n",
    "        self.add_to_orig = add_to_orig\n",
    "    \n",
    "    #@numba.jit        \n",
    "    def fit(self, X, y):\n",
    "        self.prior = self.func(y)\n",
    "        self._dict = {}\n",
    "        for col in self.cols:\n",
    "            if isinstance(col, (list, tuple)):\n",
    "                print('here')\n",
    "                tmp_df = X.loc[: ,col]\n",
    "                col = tuple(col)\n",
    "            else:\n",
    "                tmp_df = X.loc[: ,[col]]\n",
    "            tmp_df['y'] = y\n",
    "            print(tmp_df.columns)\n",
    "            #tmp_df = pd.DataFrame({'eval_col':X[col].values, 'y':y})\n",
    "            if isinstance(col, (list, tuple)):\n",
    "                print('here')\n",
    "                col = tuple(col)\n",
    "            self._dict[col] = tmp_df.groupby(col)['y'].apply(lambda x: \n",
    "                                self.func(x) if len(x) >= self.thresh  else self.prior).to_dict()\n",
    "                                \n",
    "            del tmp_df\n",
    "        return self\n",
    "    #@numba.jit\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for col in self.cols:\n",
    "            \n",
    "            if isinstance(col, (list, tuple)):\n",
    "                tmp_df = X.loc[:, col]\n",
    "                enc = tmp_df[col].apply(lambda x: self._dict[tuple(col)][tuple(x)]\n",
    "                                                                     if tuple(x) in self._dict[tuple(col)]\n",
    "                                                                     else self.prior, axis=1).values\n",
    "            else:\n",
    "                tmp_df = X.loc[:, [col]]\n",
    "                enc = tmp_df[col].apply(lambda x: self._dict[col][x]\n",
    "                                                                     if x in self._dict[col]\n",
    "                                                                     else self.prior).values\n",
    "            del tmp_df\n",
    "            X_transformed.append(enc)\n",
    "        \n",
    "        X_transformed = np.vstack(X_transformed).T\n",
    "        \n",
    "        if self.add_to_orig:\n",
    "            return np.concatenate((X.values, X_transformed), axis=1)\n",
    "            \n",
    "        else:\n",
    "            return X_transformed\n",
    "        \n",
    "def isiphonecase(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                                (series.str.contains('case', flags=re.IGNORECASE)) )\n",
    "def isiphone6(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('6|six', flags=re.IGNORECASE) &\n",
    "                        ~(series.str.contains('plus|\\+', flags=re.IGNORECASE)) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) )\n",
    "\n",
    "def isiphone6p(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('6|six', flags=re.IGNORECASE) &\n",
    "                        series.str.contains('plus|\\+', flags=re.IGNORECASE) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) )\n",
    "\n",
    "def isiphone5(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('5|five', flags=re.IGNORECASE) &\n",
    "                        ~(series.str.contains('plus|\\+', flags=re.IGNORECASE)) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) )\n",
    "\n",
    "def isiphone5p(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('5|five', flags=re.IGNORECASE) &\n",
    "                        series.str.contains('plus|\\+', flags=re.IGNORECASE) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) )\n",
    "\n",
    "def isiphone7(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('7|seven', flags=re.IGNORECASE) &\n",
    "                        ~(series.str.contains('plus|\\+', flags=re.IGNORECASE)) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) )\n",
    "\n",
    "def isiphone7p(series): return (series.str.contains('iphone', flags=re.IGNORECASE) & \n",
    "                        series.str.contains('7|seven', flags=re.IGNORECASE) &\n",
    "                        series.str.contains('plus|\\+', flags=re.IGNORECASE) &\n",
    "                                ~(series.str.contains('case', flags=re.IGNORECASE)) )\n",
    "\n",
    "#Data reading function\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "#(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.)\n",
    "\n",
    "def read_data(in_path, out_path):\n",
    "    if False and os.path.exists(os.path.join(out_path, 'train_2.pkl')) and os.path.exists(os.path.join(out_path, 'test_2.pkl')):\n",
    "        train_data = pd.read_pickle(os.path.join(out_path, 'train_2.pkl'))\n",
    "        test_data  = pd.read_pickle(os.path.join(out_path, 'test_2.pkl'))\n",
    "        \n",
    "        return train_data, test_data\n",
    "    \n",
    "    else:\n",
    "        train_data = pd.read_table(os.path.join(in_path, 'train.tsv'))\n",
    "        test_data  = pd.read_table(os.path.join(in_path, 'test.tsv'))\n",
    "    \n",
    "        train_rows = len(train_data)\n",
    "        data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "    \n",
    "        data['cat1'] = data['category_name'].apply(lambda x: str(x).split('/')[0])\n",
    "        data['cat2'] = data['category_name'].apply(lambda x: str(x).split('/')[1] \n",
    "                                                   if len(str(x).split('/')) > 1 else -1)\n",
    "        data['cat3'] = data['category_name'].apply(lambda x: ' '.join(str(x).split('/')[2:]) \n",
    "                                                   if len(str(x).split('/')) > 2 else -1)\n",
    "        data.fillna(-1, inplace=True)\n",
    "        \n",
    "        print(\"Getting word/char len features\")\n",
    "        data['desc_words'] = data['item_description'].apply(lambda x: len(str(x).split()))\n",
    "        data['desc_chars'] = data['item_description'].apply(lambda x: len(str(x)))\n",
    "        data['name_words'] = data['name'].apply(lambda x: len(str(x).split()))\n",
    "        data['name_chars'] = data['name'].apply(len)\n",
    "        \n",
    "        for col in [\"desc_words\", \"desc_chars\", \"name_words\", \"name_chars\"]:\n",
    "            data[col]  = data[col]/ data[col].max()\n",
    "        \n",
    "        print(\"Get iphone features\")\n",
    "        data['iphone_case'] = isiphonecase(data['name'])\n",
    "        data['iphone6'] = isiphone6(data['name'])\n",
    "        data['iphone6p'] = isiphone6p(data['name'])\n",
    "        data['iphone5'] = isiphone5(data['name'])\n",
    "        data['iphone5p'] = isiphone5p(data['name'])\n",
    "        data['iphone7'] = isiphone7(data['name'])\n",
    "        data['iphone7p'] = isiphone7p(data['name'])\n",
    "        data['unlocked_phone'] = data.name.str.contains('unlocked', flags=re.IGNORECASE)\n",
    "        cat_cols = ['category_name', 'brand_name', 'cat1', 'cat2', 'cat3', 'item_condition_id']\n",
    "        for col in cat_cols:\n",
    "            data[col] = LabelEncoder().fit_transform(data[col].astype(str)) + 1\n",
    "            \n",
    "        print(\"Get count features\")\n",
    "        target_enc1 = TargetEncoder(cols=['brand_name'], func=len)\n",
    "        data['brand_counts'] = target_enc1.fit_transform(data[['brand_name']], data.price)\n",
    "        data['brand_counts'] = data['brand_counts']/data['brand_counts'].max()\n",
    "\n",
    "        target_enc2 = TargetEncoder(cols=['category_name'], func=len)\n",
    "        data['cat_counts'] = target_enc2.fit_transform(data[['category_name']], data.price)\n",
    "        data['cat_counts'] = data['cat_counts']/data['cat_counts'].max()\n",
    "        \n",
    "        target_enc3 = TargetEncoder(cols=['cat1'], func=len)\n",
    "        data['cat1_counts'] = target_enc3.fit_transform(data[['cat1']], data.price)\n",
    "        data['cat1_counts'] = data['cat1_counts']/data['cat1_counts'].max()\n",
    "        \n",
    "        target_enc4 = TargetEncoder(cols=['cat2'], func=len)\n",
    "        data['cat2_counts'] = target_enc4.fit_transform(data[['cat2']], data.price)\n",
    "        data['cat2_counts'] = data['cat2_counts']/data['cat2_counts'].max()\n",
    "        \n",
    "        target_enc5 = TargetEncoder(cols=['cat3'], func=len)\n",
    "        data['cat3_counts'] = target_enc5.fit_transform(data[['cat3']], data.price)\n",
    "        data['cat3_counts'] = data['cat3_counts']/data['cat3_counts'].max()\n",
    "        #tkn_desc = Tokenizer(50000)   \n",
    "        \n",
    "        data['item_desc2gram'] = data.item_description.apply(lambda x: add_ngrams(x, 2))\n",
    "        print(\"Tokenizing data\")\n",
    "        tok_name  = Tokenizer(20000)\n",
    "        tok_name.fit_on_texts(data['name'].astype(str))\n",
    "        \n",
    "        tok_desc= Tokenizer(80000)\n",
    "        tok_desc.fit_on_texts(data['item_description'].astype(str))\n",
    "\n",
    "        tok_desc2 = Tokenizer(20000)\n",
    "        tok_desc2.fit_on_texts(data['item_desc2gram'].astype(str))\n",
    "        \n",
    "        data[\"name\"] = list(zip(sequence.pad_sequences(tok_name.texts_to_sequences(data.name.astype(str)),\n",
    "                                         maxlen=7, padding='post', truncating='post')))\n",
    "        \n",
    "        data[\"item_description\"] = list(zip(sequence.pad_sequences(tok_desc.texts_to_sequences(data.item_description.astype(str)),\n",
    "                                         maxlen=70, padding='post', truncating='post')))\n",
    "        \n",
    "        data[\"item_desc2gram\"] = list(zip(sequence.pad_sequences(tok_desc2.texts_to_sequences(data.item_desc2gram.astype(str)),\n",
    "                                         maxlen=30, padding='post', truncating='post')))\n",
    "        #tkn_desc = Tokenizer(50000)\n",
    "        #tkn_desc.fit_on_texts(data.item_description.astype(str))\n",
    "        #data['desc_seq'] = pad_sequences(tkn_desc.texts_to_sequences(data.item_description.astype(str)),\n",
    "        #                                 maxlen=100, padding='post', truncating='post')\n",
    "        \n",
    "        #tkn_name = Tokenizer(4000)\n",
    "        #tkn_name.fit_on_texts(data.name.astype(str))\n",
    "        #data['name_seq'] = pad_sequences(tkn_name.texts_to_sequences(data.name.astype(str)),\n",
    "        #                                 maxlen=6, padding='post', truncating='post')\n",
    "        \n",
    "        \n",
    "        train_data = data.loc[: train_rows - 1, :].reset_index(drop=True)\n",
    "        train_data = train_data.loc[(train_data.price >= 1) & (train_data.price <= 2000), :].reset_index(drop=True)\n",
    "        test_data  = data.loc[train_rows: , :].reset_index(drop=True)\n",
    "        \n",
    "        del train_data['test_id']\n",
    "        del test_data['train_id']\n",
    "        del data \n",
    "        test_data['test_id'] = test_data['test_id'].astype(int)\n",
    "        #train_data.to_pickle(os.path.join(out_path, 'train_2.pkl'))\n",
    "        #test_data.to_pickle(os.path.join(out_path, 'test_2.pkl'))\n",
    "        \n",
    "        return train_data, test_data\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EM_NNRegressor(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, embed_cols=None, dense_cols=None, embed_dims=None, \n",
    "                 text_embed_cols=None, text_embed_seq_lens=None, \n",
    "                 text_embed_dims=None, \n",
    "                 #text_embed_tokenizers=None,\n",
    "                 num_layers=2, multiprocess=False,\n",
    "                layer_activations=None, layer_dims=None,layer_dropouts=None, epochs=20, batchsize=32,\n",
    "                optimizer_kwargs=None, val_size=0.1, verbose=1, seed=1,):\n",
    "        \n",
    "        self.embed_cols = embed_cols\n",
    "        self.dense_cols = dense_cols\n",
    "        self.embed_dims = embed_dims\n",
    "        self.text_embed_cols = text_embed_cols\n",
    "        self.text_embed_dims = text_embed_dims\n",
    "        #self.text_embed_tokenizers = text_embed_tokenizers\n",
    "        self.text_embed_seq_lens = text_embed_seq_lens\n",
    "        self.dense_dims = None\n",
    "        self.num_layers = num_layers\n",
    "        self.layer_dims = layer_dims\n",
    "        self.layer_activations = layer_activations\n",
    "        self.layer_dropouts = layer_dropouts\n",
    "        self.epochs = epochs\n",
    "        self.batchsize = batchsize\n",
    "        self.optimizer_kwargs = optimizer_kwargs\n",
    "        self.val_size = val_size\n",
    "        self.verbose = verbose\n",
    "        self.multiprocess = multiprocess\n",
    "        self.seed = seed\n",
    "        #self.optim = optim\n",
    "        self.model = None\n",
    "        if self.dense_cols:\n",
    "            self.dense_dims = len(self.dense_cols)\n",
    "            \n",
    "    def _splitX(self, X):\n",
    "        X_splits = []\n",
    "        \n",
    "        if self.embed_cols:\n",
    "            for col in self.embed_cols :\n",
    "                X_splits.append(X[col].values.reshape(X.shape[0], -1))\n",
    "                \n",
    "        if self.text_embed_cols:\n",
    "            for i, col in enumerate(self.text_embed_cols):\n",
    "                #max_features = self.text_embed_dims[i][0]\n",
    "                #max_len = self.text_embed_seq_lens[i]\n",
    "                #input_text = X[col].astype(str)\n",
    "                #x_train = tok.texts_to_sequences(input_text)\n",
    "                #print(np.mean([len(l) for l in x_train]))\n",
    "                #x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "                #X_splits.append(np.array(x_train).reshape(X.shape[0], -1))\n",
    "                X_splits.append(np.concatenate(X[col].values))\n",
    "                \n",
    "        if self.dense_cols:\n",
    "            X_splits.append(X[self.dense_cols].values.reshape(X.shape[0], -1))\n",
    "            \n",
    "        return X_splits\n",
    "    \n",
    "    \n",
    "    def _build_model(self):\n",
    "        model_inputs = []\n",
    "        model_layers = []\n",
    "        \n",
    "        if self.embed_cols:\n",
    "            for col, dim in zip(self.embed_cols, self.embed_dims):\n",
    "                x1 = Input( shape=(1,), name=col)\n",
    "                model_inputs.append(x1)\n",
    "                x1 = Embedding(input_dim=dim[0], output_dim=dim[1], )(x1)\n",
    "                #x1 = Dropout(0.1)(x1)\n",
    "                x1 = Reshape(target_shape=(dim[1],))(x1)\n",
    "                model_layers.append(x1)\n",
    "                \n",
    "        if self.text_embed_cols:\n",
    "            for col, dim, seq_len in zip(self.text_embed_cols, \n",
    "                                                self.text_embed_dims, \n",
    "                                                self.text_embed_seq_lens):\n",
    "                x3 = Input( shape=(seq_len,))\n",
    "                model_inputs.append(x3)\n",
    "                x3 = Embedding(input_dim=dim[0], output_dim=dim[1], input_length=seq_len)(x3)\n",
    "                x3 = GlobalAveragePooling1D()(x3)\n",
    "                x3 = Reshape(target_shape=(dim[1],))(x3)\n",
    "                model_layers.append(x3)\n",
    "                \n",
    "        if self.dense_cols:\n",
    "            x2 = Input( shape=(self.dense_dims, ), name='dense_cols')\n",
    "            model_inputs.append(x2)\n",
    "            model_layers.append(x2)\n",
    "        print(model_layers)\n",
    "        x = concatenate(model_layers)\n",
    "        \n",
    "        if self.num_layers > 0:\n",
    "            for dim, drops in zip(self.layer_dims, self.layer_dropouts):\n",
    "                x = BatchNormalization()(x)\n",
    "                x = Dropout(rate=drops)(x)\n",
    "                x = Dense(dim, kernel_initializer='he_normal')(x)\n",
    "                x = LeakyReLU()(x)\n",
    "        \n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.05)(x)\n",
    "        output = Dense(1, activation='linear', kernel_initializer='he_normal')(x)\n",
    "        \n",
    "        model = Model(inputs=model_inputs, outputs=output)\n",
    "        #print(model.summary())\n",
    "        #adam = Nadam(lr=0.002, schedule_decay=0.02)\n",
    "        adam = Adam(lr=0.007, decay=0.001)\n",
    "        #adam = SGD(lr=0.01, nesterov=True, momentum=0.9, decay=0.003)\n",
    "        #adam = RMSprop(lr=0.01, decay=0.006)\n",
    "        #adam = self.optim\n",
    "        model.compile(optimizer=adam, loss='mean_squared_error' )\n",
    "        \n",
    "        return model \n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model = self._build_model()\n",
    "        if self.val_size > 0:\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.val_size, random_state=self.seed)\n",
    "            print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "            \n",
    "            callbacks= [ModelCheckpoint(\"embed_NN_\"+str(self.seed)+\".check\", save_best_only=True, verbose=1)]\n",
    "            if self.multiprocess == False:\n",
    "                self.model.fit(self._splitX(X_train), y_train, batch_size=self.batchsize, epochs=self.epochs,\n",
    "                               verbose=self.verbose,\n",
    "                              validation_data=(self._splitX(X_val), y_val), shuffle=True,\n",
    "                              callbacks=callbacks)\n",
    "            else:\n",
    "                X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.val_size, random_state=1)\n",
    "\n",
    "        else:\n",
    "            self.model.fit(self._splitX(X), y, batch_size=self.batchsize, epochs=self.epochs,\n",
    "               verbose=self.verbose, shuffle=True)\n",
    "\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        \n",
    "        if self.model:\n",
    "            model = load_model(\"embed_NN_\"+str(self.seed)+\".check\")\n",
    "            y_hat = model.predict(self._splitX(X))\n",
    "        else:\n",
    "            raise ValueError(\"Model not fit yet\")\n",
    "            \n",
    "        return y_hat\n",
    "        \n",
    "def add_ngrams(text, ngram=2):\n",
    "    word_list = str(text).lower().split(' ')\n",
    "    out_list = [''.join(word_list[i:i+ngram]) for i in range(len(word_list))]\n",
    "    return ' '.join(out_list[:-1])\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 7192633448965888522]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting word/char len features\n",
      "Get iphone features\n",
      "Get count features\n",
      "Index(['brand_name', 'y'], dtype='object')\n",
      "Index(['category_name', 'y'], dtype='object')\n",
      "Index(['cat1', 'y'], dtype='object')\n",
      "Index(['cat2', 'y'], dtype='object')\n",
      "Index(['cat3', 'y'], dtype='object')\n",
      "Tokenizing data\n",
      "(1481658, 29) (693359, 29)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>item_description</th>\n",
       "      <th>name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>train_id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>...</th>\n",
       "      <th>iphone5p</th>\n",
       "      <th>iphone7</th>\n",
       "      <th>iphone7p</th>\n",
       "      <th>unlocked_phone</th>\n",
       "      <th>brand_counts</th>\n",
       "      <th>cat_counts</th>\n",
       "      <th>cat1_counts</th>\n",
       "      <th>cat2_counts</th>\n",
       "      <th>cat3_counts</th>\n",
       "      <th>item_desc2gram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>831</td>\n",
       "      <td>3</td>\n",
       "      <td>([12, 63, 68, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>([2643, 4717, 5063, 57, 15, 4, 56],)</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>104</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.252631</td>\n",
       "      <td>0.141360</td>\n",
       "      <td>0.154083</td>\n",
       "      <td>0.252631</td>\n",
       "      <td>([16, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3891</td>\n",
       "      <td>88</td>\n",
       "      <td>3</td>\n",
       "      <td>([24, 2960, 10, 5, 34, 17, 1, 193, 50, 19, 899...</td>\n",
       "      <td>([5719, 12979, 10265, 1571, 0, 0, 0],)</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.016111</td>\n",
       "      <td>0.185101</td>\n",
       "      <td>0.055355</td>\n",
       "      <td>0.016111</td>\n",
       "      <td>([199, 82, 31, 756, 2437, 10575, 2056, 3181, 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4590</td>\n",
       "      <td>1279</td>\n",
       "      <td>1</td>\n",
       "      <td>([532, 87, 8, 3, 4629, 11, 249, 1, 3, 957, 108...</td>\n",
       "      <td>([3865, 6135, 198, 0, 0, 0, 0],)</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10</td>\n",
       "      <td>105</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.002936</td>\n",
       "      <td>0.338477</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.798175</td>\n",
       "      <td>0.338567</td>\n",
       "      <td>([1321, 71, 11756, 9293, 6548, 192, 13158, 151...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>505</td>\n",
       "      <td>1</td>\n",
       "      <td>([6, 8, 59, 191, 6741, 186, 4, 20, 141, 1060, ...</td>\n",
       "      <td>([118, 1602, 11558, 0, 0, 0, 0],)</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.217378</td>\n",
       "      <td>0.102457</td>\n",
       "      <td>0.187833</td>\n",
       "      <td>0.217378</td>\n",
       "      <td>([36, 49, 35, 505, 128, 1902, 13, 1, 1, 138, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1206</td>\n",
       "      <td>1</td>\n",
       "      <td>([745, 8, 6197, 11, 1695, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>([3528, 45, 915, 139, 0, 0, 0],)</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.328417</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.458721</td>\n",
       "      <td>0.328417</td>\n",
       "      <td>([4310, 6766, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   brand_name  category_name  item_condition_id  \\\n",
       "0           3            831                  3   \n",
       "1        3891             88                  3   \n",
       "2        4590           1279                  1   \n",
       "3           3            505                  1   \n",
       "4           3           1206                  1   \n",
       "\n",
       "                                    item_description  \\\n",
       "0  ([12, 63, 68, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1  ([24, 2960, 10, 5, 34, 17, 1, 193, 50, 19, 899...   \n",
       "2  ([532, 87, 8, 3, 4629, 11, 249, 1, 3, 957, 108...   \n",
       "3  ([6, 8, 59, 191, 6741, 186, 4, 20, 141, 1060, ...   \n",
       "4  ([745, 8, 6197, 11, 1695, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "\n",
       "                                     name  price  shipping  train_id  cat1  \\\n",
       "0    ([2643, 4717, 5063, 57, 15, 4, 56],)   10.0         1       0.0     6   \n",
       "1  ([5719, 12979, 10265, 1571, 0, 0, 0],)   52.0         0       1.0     2   \n",
       "2        ([3865, 6135, 198, 0, 0, 0, 0],)   10.0         1       2.0    10   \n",
       "3       ([118, 1602, 11558, 0, 0, 0, 0],)   35.0         1       3.0     4   \n",
       "4        ([3528, 45, 915, 139, 0, 0, 0],)   44.0         0       4.0    10   \n",
       "\n",
       "   cat2                        ...                          iphone5p  iphone7  \\\n",
       "0   104                        ...                             False    False   \n",
       "1    32                        ...                             False    False   \n",
       "2   105                        ...                             False    False   \n",
       "3    57                        ...                             False    False   \n",
       "4    60                        ...                             False    False   \n",
       "\n",
       "   iphone7p  unlocked_phone  brand_counts  cat_counts  cat1_counts  \\\n",
       "0     False           False      1.000000    0.252631     0.141360   \n",
       "1     False           False      0.000139    0.016111     0.185101   \n",
       "2     False           False      0.002936    0.338477     1.000000   \n",
       "3     False           False      1.000000    0.217378     0.102457   \n",
       "4     False           False      1.000000    0.328417     1.000000   \n",
       "\n",
       "   cat2_counts  cat3_counts                                     item_desc2gram  \n",
       "0     0.154083     0.252631  ([16, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1     0.055355     0.016111  ([199, 82, 31, 756, 2437, 10575, 2056, 3181, 8...  \n",
       "2     0.798175     0.338567  ([1321, 71, 11756, 9293, 6548, 192, 13158, 151...  \n",
       "3     0.187833     0.217378  ([36, 49, 35, 505, 128, 1902, 13, 1, 1, 138, 1...  \n",
       "4     0.458721     0.328417  ([4310, 6766, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read data\n",
    "train_data, test_data = read_data(\"../input\", \"./\")\n",
    "print(train_data.shape, test_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data\n",
    "y = np.log1p(train_data.price)\n",
    "\n",
    "cvlist= list(KFold(5, random_state=786).split(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_1/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_2/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_3/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_4/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_5/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_6/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_7/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_8/Reshape:0' shape=(?, 100) dtype=float32>, <tf.Tensor 'reshape_9/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols:0' shape=(?, 17) dtype=float32>]\n",
      "(1155692, 29) (29634, 29) (1155692,) (29634,)\n",
      "Train on 1155692 samples, validate on 29634 samples\n",
      "Epoch 1/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.6310Epoch 00001: val_loss improved from inf to 0.25458, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 113s 98us/step - loss: 0.6308 - val_loss: 0.2546\n",
      "Epoch 2/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.2116Epoch 00002: val_loss improved from 0.25458 to 0.19024, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 112s 97us/step - loss: 0.2116 - val_loss: 0.1902\n",
      "Epoch 3/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.1837Epoch 00003: val_loss improved from 0.19024 to 0.18546, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 113s 97us/step - loss: 0.1837 - val_loss: 0.1855\n",
      "Epoch 4/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.1686Epoch 00004: val_loss improved from 0.18546 to 0.18354, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 113s 98us/step - loss: 0.1686 - val_loss: 0.1835\n",
      "Epoch 5/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.1580Epoch 00005: val_loss did not improve\n",
      "1155692/1155692 [==============================] - 112s 97us/step - loss: 0.1580 - val_loss: 0.1838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  9.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_10/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_11/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_12/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_13/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_14/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_15/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_16/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_17/Reshape:0' shape=(?, 100) dtype=float32>, <tf.Tensor 'reshape_18/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_2:0' shape=(?, 17) dtype=float32>]\n",
      "(1155692, 29) (29634, 29) (1155692,) (29634,)\n",
      "Train on 1155692 samples, validate on 29634 samples\n",
      "Epoch 1/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.6142Epoch 00001: val_loss improved from inf to 0.25594, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 111s 96us/step - loss: 0.6140 - val_loss: 0.2559\n",
      "Epoch 2/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.2084Epoch 00002: val_loss improved from 0.25594 to 0.18997, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 112s 97us/step - loss: 0.2084 - val_loss: 0.1900\n",
      "Epoch 3/5\n",
      " 997376/1155692 [========================>.....] - ETA: 14s - loss: 0.1826"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-721b43c78a5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                  )\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0moof_preds1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnnet1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcvlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moof_preds1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_predict\u001b[0;34m(estimator, X, y, groups, cv, n_jobs, verbose, fit_params, pre_dispatch, method)\u001b[0m\n\u001b[1;32m    678\u001b[0m     prediction_blocks = parallel(delayed(_fit_and_predict)(\n\u001b[1;32m    679\u001b[0m         clone(estimator), X, y, train, test, verbose, fit_params, method)\n\u001b[0;32m--> 680\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;31m# Concatenate the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_predict\u001b[0;34m(estimator, X, y, train, test, verbose, fit_params, method)\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-342c7e6d831d>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    122\u001b[0m                                \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_splitX\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nnet1 = EM_NNRegressor(embed_cols=['brand_name','category_name','item_condition_id', 'cat1', 'cat2', 'cat3'], \n",
    "                  embed_dims=[(6000, 40),(1500, 30), (6,4), (16,4), (121, 10), (900, 20)],\n",
    "                  text_embed_cols=['name', 'item_description', 'item_desc2gram'],\n",
    "                  text_embed_dims=[(20000, 50), (100000, 100), (20000, 50)],\n",
    "                  text_embed_seq_lens =[7, 70, 30],\n",
    "                  #text_embed_tokenizers = [tok_name, tok_desc, tok_desc2],\n",
    "                  dense_cols=['shipping', 'desc_words', 'desc_chars', 'name_chars',\n",
    "                                'iphone_case', 'iphone6', 'iphone6p',\n",
    "                                'iphone5', 'iphone5p', 'iphone7', 'iphone7p', 'unlocked_phone',\n",
    "                              'brand_counts', 'cat_counts',\n",
    "                                   'cat1_counts', 'cat2_counts', 'cat3_counts'\n",
    "                                  ],\n",
    "                  epochs=5,\n",
    "                  batchsize=2048 ,\n",
    "                  num_layers = 1,\n",
    "                  layer_dropouts=[0.22],\n",
    "                  layer_dims=[200],\n",
    "                  seed=1,\n",
    "                  val_size=0.025,\n",
    "                 )\n",
    "\n",
    "oof_preds1 = cross_val_predict(nnet1, X, y, verbose=10, cv=cvlist)\n",
    "score = rmse(y, oof_preds1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19063"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_298/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_299/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_300/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_301/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_302/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_303/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_304/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_305/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_306/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_59:0' shape=(?, 17) dtype=float32>]\n",
      "(1155692, 29) (29634, 29) (1155692,) (29634,)\n",
      "Train on 1155692 samples, validate on 29634 samples\n",
      "Epoch 1/5\n",
      "1150976/1155692 [============================>.] - ETA: 0s - loss: 0.5649Epoch 00001: val_loss improved from inf to 0.26376, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 28s 24us/step - loss: 0.5636 - val_loss: 0.2638\n",
      "Epoch 2/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.2023Epoch 00002: val_loss improved from 0.26376 to 0.18651, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.2023 - val_loss: 0.1865\n",
      "Epoch 3/5\n",
      "1153024/1155692 [============================>.] - ETA: 0s - loss: 0.1777Epoch 00003: val_loss improved from 0.18651 to 0.18236, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1777 - val_loss: 0.1824\n",
      "Epoch 4/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.1635Epoch 00004: val_loss improved from 0.18236 to 0.18088, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1635 - val_loss: 0.1809\n",
      "Epoch 5/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.1538Epoch 00005: val_loss improved from 0.18088 to 0.17902, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1539 - val_loss: 0.1790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_307/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_308/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_309/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_310/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_311/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_312/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_313/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_314/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_315/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_61:0' shape=(?, 17) dtype=float32>]\n",
      "(1155692, 29) (29634, 29) (1155692,) (29634,)\n",
      "Train on 1155692 samples, validate on 29634 samples\n",
      "Epoch 1/5\n",
      "1150976/1155692 [============================>.] - ETA: 0s - loss: 0.5806Epoch 00001: val_loss improved from inf to 0.24525, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 28s 25us/step - loss: 0.5792 - val_loss: 0.2453\n",
      "Epoch 2/5\n",
      "1153024/1155692 [============================>.] - ETA: 0s - loss: 0.2021Epoch 00002: val_loss improved from 0.24525 to 0.18385, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.2021 - val_loss: 0.1839\n",
      "Epoch 3/5\n",
      "1153024/1155692 [============================>.] - ETA: 0s - loss: 0.1781Epoch 00003: val_loss improved from 0.18385 to 0.18114, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1781 - val_loss: 0.1811\n",
      "Epoch 4/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.1645Epoch 00004: val_loss improved from 0.18114 to 0.17804, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1645 - val_loss: 0.1780\n",
      "Epoch 5/5\n",
      "1153024/1155692 [============================>.] - ETA: 0s - loss: 0.1548Epoch 00005: val_loss did not improve\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1548 - val_loss: 0.1789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  4.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_316/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_317/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_318/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_319/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_320/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_321/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_322/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_323/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_324/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_63:0' shape=(?, 17) dtype=float32>]\n",
      "(1155692, 29) (29634, 29) (1155692,) (29634,)\n",
      "Train on 1155692 samples, validate on 29634 samples\n",
      "Epoch 1/5\n",
      "1150976/1155692 [============================>.] - ETA: 0s - loss: 0.5647Epoch 00001: val_loss improved from inf to 0.25191, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 29s 25us/step - loss: 0.5633 - val_loss: 0.2519\n",
      "Epoch 2/5\n",
      "1153024/1155692 [============================>.] - ETA: 0s - loss: 0.2011Epoch 00002: val_loss improved from 0.25191 to 0.18897, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.2011 - val_loss: 0.1890\n",
      "Epoch 3/5\n",
      "1150976/1155692 [============================>.] - ETA: 0s - loss: 0.1775Epoch 00003: val_loss improved from 0.18897 to 0.18056, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1775 - val_loss: 0.1806\n",
      "Epoch 4/5\n",
      "1150976/1155692 [============================>.] - ETA: 0s - loss: 0.1633Epoch 00004: val_loss improved from 0.18056 to 0.17862, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1633 - val_loss: 0.1786\n",
      "Epoch 5/5\n",
      "1153024/1155692 [============================>.] - ETA: 0s - loss: 0.1534Epoch 00005: val_loss did not improve\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1534 - val_loss: 0.1786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  7.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_325/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_326/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_327/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_328/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_329/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_330/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_331/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_332/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_333/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_65:0' shape=(?, 17) dtype=float32>]\n",
      "(1155693, 29) (29634, 29) (1155693,) (29634,)\n",
      "Train on 1155693 samples, validate on 29634 samples\n",
      "Epoch 1/5\n",
      "1155072/1155693 [============================>.] - ETA: 0s - loss: 0.5760Epoch 00001: val_loss improved from inf to 0.25223, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 30s 26us/step - loss: 0.5758 - val_loss: 0.2522\n",
      "Epoch 2/5\n",
      "1155072/1155693 [============================>.] - ETA: 0s - loss: 0.2019Epoch 00002: val_loss improved from 0.25223 to 0.18721, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.2018 - val_loss: 0.1872\n",
      "Epoch 3/5\n",
      "1155072/1155693 [============================>.] - ETA: 0s - loss: 0.1779Epoch 00003: val_loss improved from 0.18721 to 0.18023, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.1779 - val_loss: 0.1802\n",
      "Epoch 4/5\n",
      "1155072/1155693 [============================>.] - ETA: 0s - loss: 0.1637Epoch 00004: val_loss improved from 0.18023 to 0.17916, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.1637 - val_loss: 0.1792\n",
      "Epoch 5/5\n",
      "1153024/1155693 [============================>.] - ETA: 0s - loss: 0.1537Epoch 00005: val_loss improved from 0.17916 to 0.17838, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.1537 - val_loss: 0.1784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  9.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_334/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_335/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_336/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_337/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_338/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_339/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_340/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_341/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_342/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_67:0' shape=(?, 17) dtype=float32>]\n",
      "(1155693, 29) (29634, 29) (1155693,) (29634,)\n",
      "Train on 1155693 samples, validate on 29634 samples\n",
      "Epoch 1/5\n",
      "1153024/1155693 [============================>.] - ETA: 0s - loss: 0.5633Epoch 00001: val_loss improved from inf to 0.24940, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 30s 26us/step - loss: 0.5626 - val_loss: 0.2494\n",
      "Epoch 2/5\n",
      "1155072/1155693 [============================>.] - ETA: 0s - loss: 0.2015Epoch 00002: val_loss improved from 0.24940 to 0.18373, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.2015 - val_loss: 0.1837\n",
      "Epoch 3/5\n",
      "1150976/1155693 [============================>.] - ETA: 0s - loss: 0.1779Epoch 00003: val_loss improved from 0.18373 to 0.17886, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.1779 - val_loss: 0.1789\n",
      "Epoch 4/5\n",
      "1153024/1155693 [============================>.] - ETA: 0s - loss: 0.1640Epoch 00004: val_loss improved from 0.17886 to 0.17707, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.1640 - val_loss: 0.1771\n",
      "Epoch 5/5\n",
      "1155072/1155693 [============================>.] - ETA: 0s - loss: 0.1540Epoch 00005: val_loss improved from 0.17707 to 0.17607, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.1540 - val_loss: 0.1761\n",
      "0.47740754 9.648284\n",
      "0.4204833150009789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 12.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 12.3min finished\n"
     ]
    }
   ],
   "source": [
    "nnet2 = EM_NNRegressor(embed_cols=['brand_name','category_name','item_condition_id', 'cat1', 'cat2', 'cat3'], \n",
    "                  embed_dims=[(6000, 30),(1500, 25), (5,4), (15,4), (120, 10), (900, 20)],\n",
    "                  text_embed_cols=['name', 'item_description', 'item_desc2gram'],\n",
    "                  text_embed_dims=[(20000, 30), (50000, 30), (20000, 30)],\n",
    "                  text_embed_seq_lens =[7, 70, 30],\n",
    "                  #text_embed_tokenizers = [tok_name, tok_desc, tok_desc2],\n",
    "                  dense_cols=['shipping', 'desc_words', 'desc_chars', 'name_chars',\n",
    "                                'iphone_case', 'iphone6', 'iphone6p',\n",
    "                                'iphone5', 'iphone5p', 'iphone7', 'iphone7p', 'unlocked_phone',\n",
    "                              'brand_counts', 'cat_counts',\n",
    "                                   'cat1_counts', 'cat2_counts', 'cat3_counts'],\n",
    "                  epochs=4,\n",
    "                  batchsize=2048 ,\n",
    "                  num_layers = 1,\n",
    "                  layer_dropouts=[0.2],\n",
    "                  layer_dims=[100],\n",
    "                  seed=2,\n",
    "                  val_size=0.02\n",
    "                 )\n",
    "oof_preds2 = cross_val_predict(nnet1, X, y, verbose=10, cv=cvlist)\n",
    "score = rmse(y, oof_preds2)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_343/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_344/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_345/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_346/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_347/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_348/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_349/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_350/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_351/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_69:0' shape=(?, 17) dtype=float32>]\n",
      "(1155692, 29) (29634, 29) (1155692,) (29634,)\n",
      "Train on 1155692 samples, validate on 29634 samples\n",
      "Epoch 1/5\n",
      "1153024/1155692 [============================>.] - ETA: 0s - loss: 0.5652Epoch 00001: val_loss improved from inf to 0.25011, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 31s 27us/step - loss: 0.5644 - val_loss: 0.2501\n",
      "Epoch 2/5\n",
      "1153024/1155692 [============================>.] - ETA: 0s - loss: 0.2023Epoch 00002: val_loss improved from 0.25011 to 0.18530, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.2023 - val_loss: 0.1853\n",
      "Epoch 3/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.1781Epoch 00003: val_loss improved from 0.18530 to 0.18093, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1781 - val_loss: 0.1809\n",
      "Epoch 4/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.1643Epoch 00004: val_loss improved from 0.18093 to 0.17861, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1643 - val_loss: 0.1786\n",
      "Epoch 5/5\n",
      "1153024/1155692 [============================>.] - ETA: 0s - loss: 0.1540Epoch 00005: val_loss did not improve\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1540 - val_loss: 0.1789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_352/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_353/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_354/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_355/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_356/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_357/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_358/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_359/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_360/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_71:0' shape=(?, 17) dtype=float32>]\n",
      "(1155692, 29) (29634, 29) (1155692,) (29634,)\n",
      "Train on 1155692 samples, validate on 29634 samples\n",
      "Epoch 1/5\n",
      "1153024/1155692 [============================>.] - ETA: 0s - loss: 0.5729Epoch 00001: val_loss improved from inf to 0.24662, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 31s 27us/step - loss: 0.5721 - val_loss: 0.2466\n",
      "Epoch 2/5\n",
      "1150976/1155692 [============================>.] - ETA: 0s - loss: 0.2020Epoch 00002: val_loss improved from 0.24662 to 0.18736, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.2020 - val_loss: 0.1874\n",
      "Epoch 3/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.1785Epoch 00003: val_loss improved from 0.18736 to 0.18088, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1785 - val_loss: 0.1809\n",
      "Epoch 4/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.1646Epoch 00004: val_loss improved from 0.18088 to 0.17897, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1646 - val_loss: 0.1790\n",
      "Epoch 5/5\n",
      "1153024/1155692 [============================>.] - ETA: 0s - loss: 0.1546Epoch 00005: val_loss did not improve\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1546 - val_loss: 0.1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  5.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_361/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_362/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_363/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_364/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_365/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_366/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_367/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_368/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_369/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_73:0' shape=(?, 17) dtype=float32>]\n",
      "(1155692, 29) (29634, 29) (1155692,) (29634,)\n",
      "Train on 1155692 samples, validate on 29634 samples\n",
      "Epoch 1/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.5759Epoch 00001: val_loss improved from inf to 0.24836, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 31s 27us/step - loss: 0.5757 - val_loss: 0.2484\n",
      "Epoch 2/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.2010Epoch 00002: val_loss improved from 0.24836 to 0.18542, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.2010 - val_loss: 0.1854\n",
      "Epoch 3/5\n",
      "1153024/1155692 [============================>.] - ETA: 0s - loss: 0.1778Epoch 00003: val_loss improved from 0.18542 to 0.18066, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1778 - val_loss: 0.1807\n",
      "Epoch 4/5\n",
      "1155072/1155692 [============================>.] - ETA: 0s - loss: 0.1640Epoch 00004: val_loss improved from 0.18066 to 0.17752, saving model to embed_NN_1.check\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1640 - val_loss: 0.1775\n",
      "Epoch 5/5\n",
      "1153024/1155692 [============================>.] - ETA: 0s - loss: 0.1542Epoch 00005: val_loss did not improve\n",
      "1155692/1155692 [==============================] - 14s 12us/step - loss: 0.1542 - val_loss: 0.1776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  7.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_370/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_371/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_372/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_373/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_374/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_375/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_376/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_377/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_378/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_75:0' shape=(?, 17) dtype=float32>]\n",
      "(1155693, 29) (29634, 29) (1155693,) (29634,)\n",
      "Train on 1155693 samples, validate on 29634 samples\n",
      "Epoch 1/5\n",
      "1153024/1155693 [============================>.] - ETA: 0s - loss: 0.5573Epoch 00001: val_loss improved from inf to 0.26231, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 32s 27us/step - loss: 0.5566 - val_loss: 0.2623\n",
      "Epoch 2/5\n",
      "1155072/1155693 [============================>.] - ETA: 0s - loss: 0.2072Epoch 00002: val_loss improved from 0.26231 to 0.18558, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.2072 - val_loss: 0.1856\n",
      "Epoch 3/5\n",
      "1153024/1155693 [============================>.] - ETA: 0s - loss: 0.1818Epoch 00003: val_loss improved from 0.18558 to 0.18380, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.1818 - val_loss: 0.1838\n",
      "Epoch 4/5\n",
      "1153024/1155693 [============================>.] - ETA: 0s - loss: 0.1667Epoch 00004: val_loss improved from 0.18380 to 0.17916, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.1667 - val_loss: 0.1792\n",
      "Epoch 5/5\n",
      "1153024/1155693 [============================>.] - ETA: 0s - loss: 0.1566Epoch 00005: val_loss did not improve\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.1566 - val_loss: 0.1802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 10.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'reshape_379/Reshape:0' shape=(?, 40) dtype=float32>, <tf.Tensor 'reshape_380/Reshape:0' shape=(?, 30) dtype=float32>, <tf.Tensor 'reshape_381/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_382/Reshape:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'reshape_383/Reshape:0' shape=(?, 10) dtype=float32>, <tf.Tensor 'reshape_384/Reshape:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'reshape_385/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_386/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'reshape_387/Reshape:0' shape=(?, 50) dtype=float32>, <tf.Tensor 'dense_cols_77:0' shape=(?, 17) dtype=float32>]\n",
      "(1155693, 29) (29634, 29) (1155693,) (29634,)\n",
      "Train on 1155693 samples, validate on 29634 samples\n",
      "Epoch 1/5\n",
      "1155072/1155693 [============================>.] - ETA: 0s - loss: 0.5663Epoch 00001: val_loss improved from inf to 0.26489, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 32s 28us/step - loss: 0.5661 - val_loss: 0.2649\n",
      "Epoch 2/5\n",
      "1155072/1155693 [============================>.] - ETA: 0s - loss: 0.2042Epoch 00002: val_loss improved from 0.26489 to 0.18569, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.2042 - val_loss: 0.1857\n",
      "Epoch 3/5\n",
      "1153024/1155693 [============================>.] - ETA: 0s - loss: 0.1798Epoch 00003: val_loss improved from 0.18569 to 0.18255, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.1798 - val_loss: 0.1826\n",
      "Epoch 4/5\n",
      "1155072/1155693 [============================>.] - ETA: 0s - loss: 0.1657Epoch 00004: val_loss improved from 0.18255 to 0.18069, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.1657 - val_loss: 0.1807\n",
      "Epoch 5/5\n",
      "1153024/1155693 [============================>.] - ETA: 0s - loss: 0.1555Epoch 00005: val_loss improved from 0.18069 to 0.17996, saving model to embed_NN_1.check\n",
      "1155693/1155693 [==============================] - 14s 12us/step - loss: 0.1555 - val_loss: 0.1800\n",
      "0.5372175 8.981724\n",
      "0.42130786201521814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 12.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 12.7min finished\n"
     ]
    }
   ],
   "source": [
    "nnet3 = EM_NNRegressor(embed_cols=['brand_name','category_name','item_condition_id', 'cat1', 'cat2', 'cat3'], \n",
    "                  embed_dims=[(6000, 30),(1500, 20), (5,4), (15,4), (120, 10), (900, 20)],\n",
    "                  text_embed_cols=['name', 'item_description', 'item_desc2gram'],\n",
    "                  text_embed_dims=[(20000, 50), (50000, 50), (20000, 50)],\n",
    "                  text_embed_seq_lens =[7, 70, 30],\n",
    "                  #text_embed_tokenizers = [tok_name, tok_desc, tok_desc2],\n",
    "                  dense_cols=['shipping', 'desc_words', 'desc_chars', 'name_chars',\n",
    "                                'iphone_case', 'iphone6', 'iphone6p',\n",
    "                                'iphone5', 'iphone5p', 'iphone7', 'iphone7p', 'unlocked_phone',\n",
    "                              'brand_counts', 'cat_counts',\n",
    "                                   'cat1_counts', 'cat2_counts', 'cat3_counts'],\n",
    "                  epochs=4,\n",
    "                  batchsize=2048 ,\n",
    "                  num_layers = 1,\n",
    "                  layer_dropouts=[0.2],\n",
    "                  layer_dims=[200],\n",
    "                  seed=3,\n",
    "                  val_size=0.02,\n",
    "                 )\n",
    "oof_preds3 = cross_val_predict(nnet1, X, y, verbose=10, cv=cvlist)\n",
    "score = rmse(y, oof_preds3)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import hmean, gmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1481658,)\n",
      "0.4934621 9.394966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4137029815802479"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_preds = np.mean(np.hstack((oof_preds1, oof_preds2, oof_preds3)), axis=1)\n",
    "print(oof_preds.shape)\n",
    "rmse(y, oof_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With Nadam lr=0.015, decay=0.005 - mean of 3 models is 0.4185 (0.429, 0.429, 0.428)\n",
    "#RMSProp lr=0.003, decay=0.005 - mean 0.420 (0.425, 0.426, 0.424)\n",
    "#RMSProp lr=0.01, decay=0.005 - mean 0.425 (0.434, 0.431, 0.430)\n",
    "#RMSProp lr=0.004, decay=0.004 - mean 0.421 (0.428, 0.429, 0.427) \n",
    "#Adam lr=0.003, decay=0.003 - mean -- (0.4)\n",
    "#With Nadam lr=0.012, decay=0.01 - mean of 3 models is 0.4175 (0.427, 0.428, 0.429)\n",
    "\n",
    "#Adam and features lr=0.005, decay=0.001, 0.420 (0.4207, 0.4204, 0.4213)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet1.fit(train_data, np.log1p(train_data.price) )\n",
    "print(\"Predicting on test data\")\n",
    "test_preds1 = nnet1.predict(test_data)\n",
    "\n",
    "nnet2.fit(train_data, np.log1p(train_data.price) )\n",
    "print(\"Predicting on test data\")\n",
    "test_preds2 = nnet2.predict(test_data)\n",
    "\n",
    "nnet3.fit(train_data, np.log1p(train_data.price) )\n",
    "print(\"Predicting on test data\")\n",
    "test_preds3 = nnet3.predict(test_data)\n",
    "\n",
    "test_preds = (1/3)*(test_preds1 + test_preds2 + test_preds3)\n",
    "print(\"Write out submission\")\n",
    "submission: pd.DataFrame = test_data[['test_id']]\n",
    "submission['price'] = np.expm1(test_preds)\n",
    "submission.price = submission.price.clip(3, 2000)\n",
    "submission.to_csv(\"embedding_nn_v2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
